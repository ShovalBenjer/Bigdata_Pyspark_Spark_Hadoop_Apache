{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShovalBenjer/Bigdata_Pyspark_Spark_Hadoop_Apache/blob/main/CFPB_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kafka-python transformers torch autoviz mlflow pyspark==3.5.5 findspark pyarrow pandas pyyaml ipython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLBNwBnHb2NC",
        "outputId": "e7f90cd7-df0b-47cd-d92f-6c683da6047f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kafka-python\n",
            "  Downloading kafka_python-2.1.3-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting autoviz\n",
            "  Downloading autoviz-0.1.905-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting mlflow\n",
            "  Downloading mlflow-2.21.2-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: pyspark==3.5.5 in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (18.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (7.34.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark==3.5.5) (0.10.9.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.11/dist-packages (from autoviz) (2.0.1)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (from autoviz) (1.9.4)\n",
            "Collecting emoji (from autoviz)\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting pyamg (from autoviz)\n",
            "  Downloading pyamg-5.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from autoviz) (1.6.1)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from autoviz) (0.14.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from autoviz) (3.9.1)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (from autoviz) (0.19.0)\n",
            "Collecting xgboost<1.7,>=0.82 (from autoviz)\n",
            "  Downloading xgboost-1.6.2-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting pandas-dq>=1.29 (from autoviz)\n",
            "  Downloading pandas_dq-1.29-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting hvplot>=0.9.2 (from autoviz)\n",
            "  Downloading hvplot-0.11.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: holoviews>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from autoviz) (1.20.2)\n",
            "Requirement already satisfied: panel>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from autoviz) (1.6.1)\n",
            "Requirement already satisfied: matplotlib>3.7.4 in /usr/local/lib/python3.11/dist-packages (from autoviz) (3.10.0)\n",
            "Requirement already satisfied: seaborn>0.12.2 in /usr/local/lib/python3.11/dist-packages (from autoviz) (0.13.2)\n",
            "Collecting mlflow-skinny==2.21.2 (from mlflow)\n",
            "  Downloading mlflow_skinny-2.21.2-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.0)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
            "  Downloading alembic-1.15.1-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.7)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.14.1)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.39)\n",
            "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (8.1.8)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (3.1.1)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.21.2->mlflow)\n",
            "  Downloading databricks_sdk-0.48.0-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting fastapi<1 (from mlflow-skinny==2.21.2->mlflow)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (3.1.44)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (8.6.1)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (1.31.1)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (1.31.1)\n",
            "Requirement already satisfied: protobuf<6,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (5.29.4)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (2.10.6)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (0.5.3)\n",
            "Collecting uvicorn<1 (from mlflow-skinny==2.21.2->mlflow)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython) (4.9.0)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.1.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.3.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: bokeh>=3.1 in /usr/local/lib/python3.11/dist-packages (from holoviews>=1.16.0->autoviz) (3.6.3)\n",
            "Requirement already satisfied: colorcet in /usr/local/lib/python3.11/dist-packages (from holoviews>=1.16.0->autoviz) (3.1.0)\n",
            "Requirement already satisfied: param<3.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from holoviews>=1.16.0->autoviz) (2.2.0)\n",
            "Requirement already satisfied: pyviz-comms>=2.1 in /usr/local/lib/python3.11/dist-packages (from holoviews>=1.16.0->autoviz) (3.0.4)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython) (0.8.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>3.7.4->autoviz) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>3.7.4->autoviz) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>3.7.4->autoviz) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>3.7.4->autoviz) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>3.7.4->autoviz) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>3.7.4->autoviz) (3.2.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from panel>=1.4.0->autoviz) (6.2.0)\n",
            "Requirement already satisfied: linkify-it-py in /usr/local/lib/python3.11/dist-packages (from panel>=1.4.0->autoviz) (2.0.3)\n",
            "Requirement already satisfied: markdown-it-py in /usr/local/lib/python3.11/dist-packages (from panel>=1.4.0->autoviz) (3.0.0)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.11/dist-packages (from panel>=1.4.0->autoviz) (0.4.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->autoviz) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->autoviz) (3.6.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->autoviz) (1.0.1)\n",
            "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.1->holoviews>=1.16.0->autoviz) (6.4.2)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.1->holoviews>=1.16.0->autoviz) (2025.1.0)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.2->mlflow) (2.38.0)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi<1->mlflow-skinny==2.21.2->mlflow)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.21.2->mlflow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.21.2->mlflow) (3.21.0)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.21.2->mlflow) (1.2.18)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.21.2->mlflow) (0.52b1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.21.2->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.21.2->mlflow) (2.27.2)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn<1->mlflow-skinny==2.21.2->mlflow) (0.14.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->panel>=1.4.0->autoviz) (0.5.1)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.11/dist-packages (from linkify-it-py->panel>=1.4.0->autoviz) (1.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py->panel>=1.4.0->autoviz) (0.1.2)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.21.2->mlflow) (1.17.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.21.2->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.2->mlflow) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.2->mlflow) (4.9)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==2.21.2->mlflow) (4.9.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==2.21.2->mlflow) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.2->mlflow) (0.6.1)\n",
            "Downloading kafka_python-2.1.3-py2.py3-none-any.whl (276 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.1/276.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoviz-0.1.905-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.5/67.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow-2.21.2-py3-none-any.whl (28.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.2/28.2 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-2.21.2-py3-none-any.whl (6.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Downloading alembic-1.15.1-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.8/231.8 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hvplot-0.11.2-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.9/161.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas_dq-1.29-py3-none-any.whl (29 kB)\n",
            "Downloading xgboost-1.6.2-py3-none-manylinux2014_x86_64.whl (255.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.9/255.9 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyamg-5.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.48.0-py3-none-any.whl (677 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m677.6/677.6 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# Comprehensive Big Data Pipeline with Spark Structured Streaming, Kafka\n",
        "# Designed for Google Colab Pro with External Kafka Setup\n",
        "# VERSION incorporating fixes for Indentation, Task 2 Filter, Task 4 GPU/Except, Task 5 TypeError, AutoViz Inline\n",
        "\n",
        "print(\"--- Initializing Pipeline Script ---\")\n",
        "print(\"Ensure Kafka/Zookeeper are running externally and topics are created.\")\n",
        "print(\"Ensure Consumer_Complaints.csv is available at /content/Consumer_Complaints.csv\")\n",
        "print(\"Ensure required packages are installed (run pip install cell).\")\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "import mlflow\n",
        "import findspark\n",
        "import contextlib\n",
        "from typing import Iterator\n",
        "\n",
        "# --- Spark Configuration ---\n",
        "# Set Spark environment variables\n",
        "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
        "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
        "findspark.init() # Finds Spark installation\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "from pyspark.ml.feature import (\n",
        "    StringIndexer, OneHotEncoder, VectorAssembler, SQLTransformer # Import SQLTransformer\n",
        ")\n",
        "from pyspark.ml.base import Transformer\n",
        "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
        "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable, MLWriter, MLReader, DefaultParamsWriter, DefaultParamsReader\n",
        "from pyspark.ml.torch.distributor import TorchDistributor\n",
        "from pyspark.sql.streaming import StreamingQueryListener\n",
        "from kafka import KafkaProducer, errors as kafka_errors\n",
        "\n",
        "# --- Transformers & PyTorch ---\n",
        "from transformers import DistilBertTokenizer, DistilBertModel, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, DistributedSampler\n",
        "import torch.nn as nn\n",
        "import torch.distributed as dist\n",
        "from collections import OrderedDict\n",
        "\n",
        "# --- Visualization ---\n",
        "try:\n",
        "    from autoviz import AutoViz_Class\n",
        "    import matplotlib.pyplot as plt\n",
        "    AUTOVIZ_AVAILABLE = True\n",
        "except ImportError:\n",
        "    AUTOVIZ_AVAILABLE = False\n",
        "    print(\"WARN: autoviz or matplotlib not found. Visualization will be skipped.\")\n",
        "\n",
        "# --- Configuration Variables ---\n",
        "BASE_DIR = \"/content/consumer_complaints\"\n",
        "CSV_FILE_PATH = \"/content/Consumer_Complaints.csv\"\n",
        "TEST_DATA_PERSISTENCE_PATH = f\"{BASE_DIR}/data/test_data_source.parquet\"\n",
        "TRAINING_PIPELINE_SAVE_PATH = f\"{BASE_DIR}/models/training_pipeline\"\n",
        "EMBEDDING_MODEL_SAVE_PATH = f\"{BASE_DIR}/models/embedding_model\"\n",
        "STREAMING_CHECKPOINT_LOCATION = f\"{BASE_DIR}/checkpoints\"\n",
        "MLFLOW_TRACKING_URI = f\"file://{BASE_DIR}/mlflow\"\n",
        "VISUALIZATION_DIR = f\"{BASE_DIR}/visualizations\"\n",
        "TRAIN_PARQUET_PATH = f\"{BASE_DIR}/data/train_data.parquet\"\n",
        "VAL_PARQUET_PATH = f\"{BASE_DIR}/data/val_data.parquet\"\n",
        "\n",
        "# Kafka configuration (assuming external setup on localhost)\n",
        "KAFKA_BROKERS = \"localhost:9092\"\n",
        "KAFKA_TOPIC_RAW = \"complaints-raw\"\n",
        "KAFKA_TOPIC_TRAINING = \"complaints-training-data\"\n",
        "KAFKA_TOPIC_TESTING_STREAM = \"complaints-testing-stream\"\n",
        "KAFKA_TOPIC_PREDICTIONS = \"complaint-predictions\"\n",
        "KAFKA_TOPIC_METRICS = \"streaming-metrics\"\n",
        "\n",
        "# Simulation parameters\n",
        "MESSAGES_PER_MINUTE = 100\n",
        "BATCH_SIZE = 20\n",
        "\n",
        "# Training parameters\n",
        "TRAIN_SAMPLE_LIMIT = 20000 # Adjust based on Colab RAM\n",
        "VAL_SAMPLE_LIMIT = 2000   # Adjust based on Colab RAM\n",
        "BERT_MAX_LENGTH = 128\n",
        "BERT_BATCH_SIZE = 16 # Per GPU/Process\n",
        "NUM_EPOCHS = 3       # Reduced for faster demo\n",
        "\n",
        "# --- Create Directories ---\n",
        "for path in [\n",
        "    f\"{BASE_DIR}/data\", f\"{BASE_DIR}/models\", f\"{BASE_DIR}/checkpoints\",\n",
        "    MLFLOW_TRACKING_URI.replace(\"file://\", \"\"), VISUALIZATION_DIR\n",
        "]:\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "# --- Initialize Spark Session ---\n",
        "print(\"Initializing Spark Session...\")\n",
        "# --- Initialize Spark Session ---\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Consumer Complaints ML Pipeline\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.5\") \\\n",
        "    .config(\"spark.sql.streaming.checkpointLocation\", STREAMING_CHECKPOINT_LOCATION) \\\n",
        "    .config(\"spark.executor.memory\", \"6g\") \\\n",
        "    .config(\"spark.driver.memory\", \"6g\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "print(\"\\n--- Spark Configuration ---\")\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"Application ID: {spark.sparkContext.applicationId}\")\n",
        "print(f\"Using PySpark: {spark.sparkContext.pythonVer}\")\n",
        "\n",
        "# --- MLflow Setup ---\n",
        "def setup_mlflow_tracking():\n",
        "    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
        "    mlflow.set_experiment(\"complaint-classification\")\n",
        "    print(f\"MLflow tracking configured. URI: {MLFLOW_TRACKING_URI}\")\n",
        "    return mlflow\n",
        "\n",
        "mlflow = setup_mlflow_tracking()"
      ],
      "metadata": {
        "id": "Q54MkIDvcAeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Schema Definitions ---\n",
        "def get_full_schema():\n",
        "    return StructType([\n",
        "        StructField(\"Date received\", StringType(), True), StructField(\"Product\", StringType(), True),\n",
        "        StructField(\"Sub-product\", StringType(), True), StructField(\"Issue\", StringType(), True),\n",
        "        StructField(\"Sub-issue\", StringType(), True), StructField(\"Consumer complaint narrative\", StringType(), True),\n",
        "        StructField(\"Company public response\", StringType(), True), StructField(\"Company\", StringType(), True),\n",
        "        StructField(\"State\", StringType(), True), StructField(\"ZIP code\", StringType(), True),\n",
        "        StructField(\"Tags\", StringType(), True), StructField(\"Consumer consent provided?\", StringType(), True),\n",
        "        StructField(\"Submitted via\", StringType(), True), StructField(\"Date sent to company\", StringType(), True),\n",
        "        StructField(\"Company response to consumer\", StringType(), True), StructField(\"Timely response?\", StringType(), True),\n",
        "        StructField(\"Consumer disputed?\", StringType(), True), StructField(\"Complaint ID\", StringType(), True)\n",
        "    ])\n",
        "\n",
        "def get_streaming_schema():\n",
        "    return StructType([\n",
        "        StructField(\"Date received\", StringType(), True), StructField(\"Complaint ID\", StringType(), True),\n",
        "        StructField(\"Company\", StringType(), True), StructField(\"State\", StringType(), True),\n",
        "        StructField(\"ZIP code\", StringType(), True), StructField(\"Submitted via\", StringType(), True),\n",
        "        StructField(\"Consumer complaint narrative\", StringType(), True)\n",
        "    ])\n",
        "\n",
        "# --- Custom BERT Embedding Transformer ---\n",
        "class BERTEmbeddingTransformer(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
        "    def __init__(self, inputCol=None, outputCol=None, modelPath=None):\n",
        "        super().__init__()\n",
        "        self.inputCol = Param(self, \"inputCol\", \"\")\n",
        "        self.outputCol = Param(self, \"outputCol\", \"\")\n",
        "        self.modelPath = Param(self, \"modelPath\", \"\")\n",
        "        self._setDefault(inputCol=inputCol, outputCol=outputCol, modelPath=modelPath)\n",
        "        self.setModelPath(modelPath) # Set model path on init\n",
        "\n",
        "    def setInputCol(self, value): return self._set(inputCol=value)\n",
        "    def getInputCol(self): return self.getOrDefault(self.inputCol)\n",
        "    def setOutputCol(self, value): return self._set(outputCol=value)\n",
        "    def getOutputCol(self): return self.getOrDefault(self.outputCol)\n",
        "    def setModelPath(self, value): return self._set(modelPath=value)\n",
        "    def getModelPath(self): return self.getOrDefault(self.modelPath)\n",
        "\n",
        "    def _transform(self, dataset):\n",
        "        schema = dataset.schema # Get schema once\n",
        "        input_col_name = self.getInputCol()\n",
        "        output_col_name = self.getOutputCol()\n",
        "        _model_path = self.getModelPath() # Get path for closure\n",
        "\n",
        "        # Ensure input column exists\n",
        "        if input_col_name not in schema.fieldNames():\n",
        "            raise ValueError(f\"Input column '{input_col_name}' does not exist in DataFrame.\")\n",
        "\n",
        "        # UDF definition remains the same, ensure _model_path is used\n",
        "        @F.pandas_udf(ArrayType(FloatType()))\n",
        "        def bert_embed_batch(texts_series: pd.Series) -> pd.Series:\n",
        "            # Load model once per executor process\n",
        "            if not hasattr(bert_embed_batch, 'model') or not hasattr(bert_embed_batch, 'tokenizer'):\n",
        "                model_dir = _model_path # Use variable from closure\n",
        "                try:\n",
        "                    if not model_dir or not os.path.exists(model_dir):\n",
        "                         raise ValueError(f\"Model path '{model_dir}' not found or not specified.\")\n",
        "                    bert_embed_batch.tokenizer = DistilBertTokenizer.from_pretrained(model_dir)\n",
        "                    bert_embed_batch.model = DistilBertModel.from_pretrained(model_dir)\n",
        "                    bert_embed_batch.model.to(\"cpu\").eval() # Use CPU in UDF\n",
        "                    bert_embed_batch.embedding_dim = bert_embed_batch.model.config.dim\n",
        "                    print(f\"Worker loaded BERT embedder from {model_dir}.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Worker ERROR loading BERT embedder from '{model_dir}': {e}. Using zero embeddings.\")\n",
        "                    bert_embed_batch.tokenizer = None\n",
        "                    bert_embed_batch.model = None\n",
        "                    bert_embed_batch.embedding_dim = 768 # Default DistilBERT\n",
        "\n",
        "            results = []\n",
        "            if bert_embed_batch.model is None or bert_embed_batch.tokenizer is None:\n",
        "                # Return zeros if model failed to load\n",
        "                return pd.Series([[0.0] * bert_embed_batch.embedding_dim] * len(texts_series))\n",
        "\n",
        "            # Process texts\n",
        "            for text in texts_series:\n",
        "                try:\n",
        "                    clean_text = str(text) if text is not None else \"\"\n",
        "                    if len(clean_text.strip()) == 0:\n",
        "                        results.append([0.0] * bert_embed_batch.embedding_dim)\n",
        "                        continue\n",
        "\n",
        "                    inputs = bert_embed_batch.tokenizer(\n",
        "                        clean_text, return_tensors=\"pt\", truncation=True,\n",
        "                        max_length=BERT_MAX_LENGTH, padding=\"max_length\"\n",
        "                    )\n",
        "                    with torch.no_grad():\n",
        "                        outputs = bert_embed_batch.model(**inputs)\n",
        "                    # Use CLS token embedding [:, 0, :]\n",
        "                    results.append(outputs.last_hidden_state[:, 0, :].squeeze().tolist())\n",
        "                except Exception as e:\n",
        "                    # Log less frequently or sample errors if too noisy\n",
        "                    # print(f\"Worker ERROR embedding text snippet '{clean_text[:50]}...': {e}\")\n",
        "                    results.append([0.0] * bert_embed_batch.embedding_dim)\n",
        "            return pd.Series(results)\n",
        "\n",
        "        # Apply UDF\n",
        "        return dataset.withColumn(output_col_name, bert_embed_batch(F.col(input_col_name)))\n",
        "\n",
        "    # write/read methods remain the same as previous correct version\n",
        "    def write(self):\n",
        "        writer = DefaultParamsWriter(self)\n",
        "        original_save = writer.save\n",
        "        def custom_save(path):\n",
        "            original_save(path)\n",
        "            extra_metadata = {\"modelPath\": self.getModelPath()}\n",
        "            extra_metadata_path = os.path.join(path, \"bert_model_metadata.json\")\n",
        "            with open(extra_metadata_path, \"w\") as f: json.dump(extra_metadata, f)\n",
        "        writer.save = custom_save\n",
        "        return writer\n",
        "    @classmethod\n",
        "    def read(cls):\n",
        "        reader = DefaultParamsReader(cls)\n",
        "        original_load = reader.load\n",
        "        def custom_load(path):\n",
        "            instance = original_load(path)\n",
        "            extra_metadata_path = os.path.join(path, \"bert_model_metadata.json\")\n",
        "            if os.path.exists(extra_metadata_path):\n",
        "                with open(extra_metadata_path, \"r\") as f:\n",
        "                    extra_metadata = json.load(f)\n",
        "                    instance.setModelPath(extra_metadata.get(\"modelPath\"))\n",
        "            return instance\n",
        "        reader.load = custom_load\n",
        "        return reader\n",
        "\n",
        "# --- PyTorch Classes ---\n",
        "class ComplaintDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx] if self.texts[idx] else \"\" # Handle None\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text, add_special_tokens=True, max_length=self.max_length,\n",
        "            padding='max_length', truncation=True, return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "class EnhancedDistilBERTClassifier(torch.nn.Module):\n",
        "    def __init__(self, bert_model, dropout_rate=0.3):\n",
        "        super().__init__()\n",
        "        self.bert = bert_model\n",
        "        self.dropout1 = torch.nn.Dropout(dropout_rate)\n",
        "        hidden_size = self.bert.config.dim\n",
        "        self.dense1 = torch.nn.Linear(hidden_size, 256)\n",
        "        self.batch_norm1 = torch.nn.BatchNorm1d(256)\n",
        "        self.relu1 = torch.nn.ReLU()\n",
        "        self.dropout2 = torch.nn.Dropout(dropout_rate)\n",
        "        self.dense2 = torch.nn.Linear(256, 64)\n",
        "        self.batch_norm2 = torch.nn.BatchNorm1d(64)\n",
        "        self.relu2 = torch.nn.ReLU()\n",
        "        self.dropout3 = torch.nn.Dropout(dropout_rate)\n",
        "        self.classifier = torch.nn.Linear(64, 2) # Binary classification\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, embeddings=None):\n",
        "        if embeddings is None:\n",
        "            if input_ids is None: raise ValueError(\"Must provide input_ids or embeddings\")\n",
        "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            sequence_output = outputs.last_hidden_state[:, 0, :] # CLS token\n",
        "        else:\n",
        "            sequence_output = embeddings\n",
        "\n",
        "        x = self.dropout1(sequence_output)\n",
        "        x = self.dense1(x)\n",
        "        if x.shape[0] > 1 or not self.training: # Apply BN if batch>1 OR if evaluating (use running stats)\n",
        "             try:\n",
        "                 x = self.batch_norm1(x)\n",
        "             except ValueError as e:\n",
        "                 print(f\"WARN: BatchNorm1d error (input shape {x.shape}): {e}. Skipping BN.\")\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.dense2(x)\n",
        "        if x.shape[0] > 1 or not self.training:\n",
        "             try:\n",
        "                 x = self.batch_norm2(x)\n",
        "             except ValueError as e:\n",
        "                 print(f\"WARN: BatchNorm1d error (input shape {x.shape}): {e}. Skipping BN.\")\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout3(x)\n",
        "        logits = self.classifier(x)\n",
        "        return logits\n",
        "\n",
        "# --- Kafka Metrics Listener ---\n",
        "class MetricsListener(StreamingQueryListener):\n",
        "    def __init__(self, kafka_brokers, topic):\n",
        "        self.topic = topic\n",
        "        self.producer = None # Initialize as None\n",
        "        try:\n",
        "            self.producer = KafkaProducer(\n",
        "                bootstrap_servers=kafka_brokers.split(','),\n",
        "                value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
        "                retries=3, linger_ms=5, request_timeout_ms=10000 # Shorter timeout\n",
        "            )\n",
        "            print(\"MetricsListener Kafka Producer initialized.\")\n",
        "        except Exception as e: # Catch broader exceptions during init\n",
        "            print(f\"ERROR: MetricsListener failed to initialize Kafka Producer: {e}\")\n",
        "\n",
        "    def send_metric(self, metrics):\n",
        "        if self.producer:\n",
        "            try:\n",
        "                future = self.producer.send(self.topic, value=metrics)\n",
        "                # Optional: Add timeout for blocking flush if needed, but generally avoid blocking here\n",
        "                # future.get(timeout=1) # Example: wait 1 sec max\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR sending metric to Kafka topic '{self.topic}': {e}\")\n",
        "        # else: print(\"WARN: Metrics Kafka producer not available. Metric not sent.\") # Can be noisy\n",
        "\n",
        "    def onQueryStarted(self, event): self.send_metric({\n",
        "        \"queryName\": event.name, \"id\": str(event.id), \"runId\": str(event.runId),\n",
        "        \"timestamp\": event.timestamp, \"event\": \"started\" })\n",
        "    def onQueryProgress(self, event): self.send_metric({\n",
        "        \"queryName\": event.progress.name, \"id\": str(event.progress.id), \"runId\": str(event.progress.runId),\n",
        "        \"timestamp\": event.progress.timestamp, \"event\": \"progress\", \"numInputRows\": event.progress.numInputRows,\n",
        "        \"inputRowsPerSecond\": event.progress.inputRowsPerSecond, \"processedRowsPerSecond\": event.progress.processedRowsPerSecond,\n",
        "        \"batchId\": event.progress.batchId })\n",
        "    def onQueryTerminated(self, event): self.send_metric({\n",
        "        \"queryName\": getattr(event, 'name', None), \"id\": str(event.id), \"runId\": str(event.runId),\n",
        "        \"timestamp\": time.time() * 1000, \"event\": \"terminated\", \"exception\": str(event.exception) if event.exception else None })\n",
        "\n",
        "    def close_producer(self): # Add explicit close method\n",
        "        if self.producer:\n",
        "            try:\n",
        "                print(\"Flushing and closing MetricsListener Kafka Producer...\")\n",
        "                self.producer.flush(timeout=5) # 5 sec timeout\n",
        "                self.producer.close(timeout=5)\n",
        "                self.producer = None\n",
        "                print(\"MetricsListener Kafka Producer closed.\")\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR closing MetricsListener Kafka Producer: {e}\")\n",
        "\n",
        "    def __del__(self): # Keep __del__ as fallback\n",
        "         self.close_producer()\n",
        "\n",
        "# --- Task 1: Load Data to Kafka ---\n",
        "# Use the version that filters narrative BEFORE sampling\n",
        "def load_data_to_kafka():\n",
        "    print(f\"\\n--- Task 1: Loading Data to Kafka ---\")\n",
        "    print(f\"Reading CSV: {CSV_FILE_PATH}\")\n",
        "    if not os.path.exists(CSV_FILE_PATH):\n",
        "        print(f\"ERROR: CSV file not found at {CSV_FILE_PATH}. Download it first.\")\n",
        "        return None\n",
        "\n",
        "    raw_df_unfiltered = spark.read.format(\"csv\") \\\n",
        "                   .option(\"header\", \"true\") \\\n",
        "                   .schema(get_full_schema()) \\\n",
        "                   .option(\"escape\", \"\\\"\") \\\n",
        "                   .option(\"multiLine\", \"true\") \\\n",
        "                   .load(CSV_FILE_PATH)\n",
        "\n",
        "    initial_count = raw_df_unfiltered.count()\n",
        "    print(f\"Total records loaded initially from CSV: {initial_count}\")\n",
        "    if initial_count == 0: return None\n",
        "\n",
        "    print(\"Filtering for non-empty 'Consumer complaint narrative'...\")\n",
        "    raw_df = raw_df_unfiltered.filter(\n",
        "        (F.col(\"Consumer complaint narrative\").isNotNull()) &\n",
        "        (F.length(F.trim(F.col(\"Consumer complaint narrative\"))) > 0)\n",
        "    )\n",
        "    filtered_count = raw_df.cache().count()\n",
        "    print(f\"Records after filtering for non-empty narrative: {filtered_count}\")\n",
        "    raw_df_unfiltered.unpersist()\n",
        "\n",
        "    if filtered_count == 0:\n",
        "        print(\"ERROR: No records found with non-empty narratives.\")\n",
        "        raw_df.unpersist()\n",
        "        return None\n",
        "\n",
        "    MAX_KAFKA_LOAD_RECORDS = 50000\n",
        "    df_to_write = raw_df\n",
        "    if filtered_count > MAX_KAFKA_LOAD_RECORDS:\n",
        "        sample_fraction = MAX_KAFKA_LOAD_RECORDS / filtered_count\n",
        "        print(f\"Sampling {sample_fraction:.2%} of filtered records ({MAX_KAFKA_LOAD_RECORDS}) for Kafka loading...\")\n",
        "        df_to_write = raw_df.sample(False, sample_fraction, seed=42)\n",
        "        print(f\"Sample size for Kafka: {df_to_write.cache().count()} records\") # Cache sample\n",
        "\n",
        "    try:\n",
        "        print(f\"Writing data to Kafka topic: {KAFKA_TOPIC_RAW} at {KAFKA_BROKERS}\")\n",
        "        kafka_df = df_to_write.selectExpr(\"`Complaint ID` AS key\", \"to_json(struct(*)) AS value\")\n",
        "        kafka_df.write \\\n",
        "            .format(\"kafka\") \\\n",
        "            .option(\"kafka.bootstrap.servers\", KAFKA_BROKERS) \\\n",
        "            .option(\"topic\", KAFKA_TOPIC_RAW) \\\n",
        "            .option(\"kafka.request.timeout.ms\", \"120000\") \\\n",
        "            .option(\"kafka.delivery.timeout.ms\", \"180000\") \\\n",
        "            .save()\n",
        "        print(f\"Data successfully written to Kafka topic: {KAFKA_TOPIC_RAW}\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR writing to Kafka: {e}\")\n",
        "        print(\"Consider checking Kafka broker status and topic existence.\")\n",
        "        return None # Indicate failure if write fails\n",
        "    finally:\n",
        "         if df_to_write.is_cached: df_to_write.unpersist() # Unpersist sample if created\n",
        "         if raw_df.is_cached: raw_df.unpersist() # Unpersist original filtered\n",
        "\n",
        "    return df_to_write # Return the dataframe that was written\n",
        "\n",
        "# --- Task 2: Preprocess, Filter, Visualize ---\n",
        "# Use the version with inline AutoViz and no date filter\n",
        "def visualize_with_autoviz(df, max_rows=5000, save_dir=VISUALIZATION_DIR):\n",
        "    if not AUTOVIZ_AVAILABLE:\n",
        "        print(\"AutoViz not available. Skipping visualization.\")\n",
        "        return\n",
        "    try:\n",
        "        print(\"\\nStarting AutoViz data visualization...\")\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        df = df.cache() # Cache before counting\n",
        "        df_count = df.count()\n",
        "        if df_count == 0:\n",
        "            print(\"No data to visualize.\")\n",
        "            df.unpersist()\n",
        "            return\n",
        "\n",
        "        if df_count > max_rows:\n",
        "            print(f\"Sampling {max_rows} rows for AutoViz.\")\n",
        "            fraction = max_rows / df_count\n",
        "            sample_df = df.sample(False, fraction, seed=42)\n",
        "        else:\n",
        "            sample_df = df\n",
        "\n",
        "        pandas_df = sample_df.limit(max_rows).toPandas() # Limit again just in case + convert\n",
        "        df.unpersist() # Unpersist original after sampling/conversion\n",
        "\n",
        "        if not pandas_df.empty:\n",
        "            AV = AutoViz_Class()\n",
        "            print(\"Setting backend for inline display...\")\n",
        "            try:\n",
        "                 # Attempt to run the magic command\n",
        "                 from IPython import get_ipython\n",
        "                 ipython = get_ipython()\n",
        "                 if ipython: ipython.run_line_magic('matplotlib', 'inline')\n",
        "            except Exception as magic_e:\n",
        "                 print(f\"WARN: Could not set matplotlib inline: {magic_e}\")\n",
        "\n",
        "            print(\"Generating AutoViz charts (verbose=1)...\")\n",
        "            viz_df = AV.AutoViz(\n",
        "                \"\", dfte=pandas_df, depVar=\"\", header=0, verbose=1,\n",
        "                lowess=False, chart_format=\"html\",\n",
        "                max_rows_analyzed=max_rows,\n",
        "                save_plot_dir=save_dir\n",
        "            )\n",
        "            print(f\"\\nAutoViz visualizations saved to: {save_dir}\")\n",
        "        else:\n",
        "             print(\"Sampled DataFrame is empty, skipping AutoViz.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during AutoViz: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    finally:\n",
        "        if 'df' in locals() and df.is_cached: df.unpersist() # Ensure unpersist\n",
        "\n",
        "def preprocess_filter_and_visualize():\n",
        "    print(f\"\\n--- Task 2: Preprocessing, Filtering & Visualization ---\")\n",
        "    print(f\"Reading data from Kafka topic: {KAFKA_TOPIC_RAW}\")\n",
        "    full_schema = get_full_schema()\n",
        "    print(\"Waiting 5s before reading from Kafka...\")\n",
        "    time.sleep(5)\n",
        "    try:\n",
        "        kafka_raw_df = spark.read \\\n",
        "            .format(\"kafka\") \\\n",
        "            .option(\"kafka.bootstrap.servers\", KAFKA_BROKERS) \\\n",
        "            .option(\"subscribe\", KAFKA_TOPIC_RAW) \\\n",
        "            .option(\"startingOffsets\", \"earliest\") \\\n",
        "            .option(\"failOnDataLoss\", \"false\") \\\n",
        "            .load()\n",
        "        kafka_read_count = kafka_raw_df.count()\n",
        "        print(f\"Read {kafka_read_count} raw messages from Kafka.\")\n",
        "        if kafka_read_count == 0: return None\n",
        "\n",
        "        parsed_df = kafka_raw_df.select(\n",
        "            F.from_json(F.col(\"value\").cast(\"string\"), full_schema).alias(\"data\")\n",
        "        ).select(\"data.*\").na.drop(subset=[\"Complaint ID\"])\n",
        "        dedup_df = parsed_df.dropDuplicates([\"Complaint ID\"]).cache() # Cache after deduplication\n",
        "        print(f\"Records after JSON parsing and deduplication: {dedup_df.count()}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR reading or parsing from Kafka topic {KAFKA_TOPIC_RAW}: {e}\")\n",
        "        import traceback; traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "    # Apply filters (narrative already filtered in Task 1, keep response filter)\n",
        "    # No date filter needed here now\n",
        "    filtered_df = dedup_df.filter(F.col(\"Company response to consumer\") != \"In progress\")\n",
        "    # We don't strictly need the narrative filter again, but it ensures consistency\n",
        "    filtered_df = filtered_df.filter(\n",
        "         (F.col(\"Consumer complaint narrative\").isNotNull()) &\n",
        "         (F.length(F.trim(F.col(\"Consumer complaint narrative\"))) > 0)\n",
        "        ).cache()\n",
        "\n",
        "    filtered_count = filtered_df.count()\n",
        "    print(f\"Records after filtering (response not 'In progress'): {filtered_count}\")\n",
        "    dedup_df.unpersist() # Unpersist previous step\n",
        "\n",
        "    if filtered_count == 0:\n",
        "        print(\"WARNING: No records left after filtering.\")\n",
        "        filtered_df.unpersist()\n",
        "        return None\n",
        "\n",
        "    visualize_with_autoviz(filtered_df) # Will unpersist df inside\n",
        "    return filtered_df # Return the cached df\n",
        "\n",
        "# --- Task 3: Split, Label, Prepare Data ---\n",
        "# Keep this function as is\n",
        "def split_label_and_prepare_data(filtered_df, seed_value=42):\n",
        "    print(\"\\n--- Task 3: Data Splitting & Labeling ---\")\n",
        "    if filtered_df is None: return None, None\n",
        "    df_for_split = filtered_df # Use the potentially cached DF\n",
        "\n",
        "    training_base_df, test_base_df = df_for_split.randomSplit([0.8, 0.2], seed=seed_value)\n",
        "    training_labeled_df = training_base_df.withColumn(\n",
        "        \"is_target_complaint\",\n",
        "        F.when(\n",
        "            (F.col(\"Consumer disputed?\") == 'No') &\n",
        "            (F.col(\"Timely response?\") == 'Yes') &\n",
        "            (F.col(\"Company response to consumer\").isin(\n",
        "                'Closed with explanation', 'Closed',\n",
        "                'Closed with monetary relief', 'Closed with non-monetary relief'\n",
        "            )), 1\n",
        "        ).otherwise(0)\n",
        "    ).cache() # Cache labeled training data\n",
        "\n",
        "    print(\"Target distribution in training data:\")\n",
        "    training_labeled_df.groupBy(\"is_target_complaint\").count().show()\n",
        "\n",
        "    try:\n",
        "        print(f\"Writing training data to Kafka topic: {KAFKA_TOPIC_TRAINING}\")\n",
        "        training_kafka_df = training_labeled_df.selectExpr(\"`Complaint ID` AS key\", \"to_json(struct(*)) AS value\")\n",
        "        training_kafka_df.write \\\n",
        "            .format(\"kafka\") \\\n",
        "            .option(\"kafka.bootstrap.servers\", KAFKA_BROKERS) \\\n",
        "            .option(\"topic\", KAFKA_TOPIC_TRAINING) \\\n",
        "            .save()\n",
        "        print(f\"Training data sent to Kafka topic: {KAFKA_TOPIC_TRAINING}\")\n",
        "    except Exception as e: print(f\"ERROR writing training data to Kafka: {e}\")\n",
        "\n",
        "    try:\n",
        "        print(f\"Saving test data to Parquet: {TEST_DATA_PERSISTENCE_PATH}\")\n",
        "        # Select only essential columns needed for inference stream\n",
        "        test_cols = [\"Date received\", \"Complaint ID\", \"Company\", \"State\", \"ZIP code\", \"Submitted via\", \"Consumer complaint narrative\"]\n",
        "        test_base_df.select(*test_cols).write.format(\"parquet\").mode(\"overwrite\").save(TEST_DATA_PERSISTENCE_PATH)\n",
        "        print(f\"Test data saved to: {TEST_DATA_PERSISTENCE_PATH}\")\n",
        "    except Exception as e: print(f\"ERROR saving test data to Parquet: {e}\")\n",
        "\n",
        "    # Unpersist the input DF if it was cached\n",
        "    if df_for_split.is_cached: df_for_split.unpersist()\n",
        "    # Keep training_labeled_df cached for Task 5 fitting\n",
        "    return training_labeled_df, test_base_df\n",
        "\n",
        "\n",
        "# --- Task 4: Distributed BERT Training ---\n",
        "# Use the version with the outer try/except and TorchDistributor fallback\n",
        "def train_bert_model_distributed():\n",
        "    print(\"\\n--- Task 4: Distributed BERT Model Training ---\")\n",
        "    def train_function():\n",
        "        # Imports needed on worker\n",
        "        import pandas as pd\n",
        "        import mlflow\n",
        "        from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, DistributedSampler\n",
        "        from transformers import DistilBertTokenizer, DistilBertModel, get_linear_schedule_with_warmup\n",
        "        from torch.optim import AdamW\n",
        "        import torch.distributed as dist\n",
        "        import torch.nn as nn\n",
        "        from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "        import os, torch\n",
        "        from collections import OrderedDict\n",
        "        import numpy as np # Needed for metrics\n",
        "\n",
        "        use_gpu = torch.cuda.is_available()\n",
        "        is_distributed = dist.is_available() and dist.is_initialized()\n",
        "        rank = dist.get_rank() if is_distributed else 0\n",
        "        world_size = dist.get_world_size() if is_distributed else 1\n",
        "        local_rank = rank % torch.cuda.device_count() if use_gpu else 0\n",
        "        device = torch.device(f\"cuda:{local_rank}\" if use_gpu else \"cpu\")\n",
        "        if use_gpu: torch.cuda.set_device(device)\n",
        "        is_rank_0 = (rank == 0)\n",
        "        if is_rank_0: print(f\"Worker {rank}/{world_size}: Starting train_function. Device: {device}\")\n",
        "\n",
        "        if is_rank_0: print(f\"Worker {rank}: Reading data from Parquet...\")\n",
        "        try:\n",
        "            train_pd = pd.read_parquet(TRAIN_PARQUET_PATH)\n",
        "            val_pd = pd.read_parquet(VAL_PARQUET_PATH)\n",
        "            if is_rank_0: print(f\"Worker {rank}: Loaded {len(train_pd)} train, {len(val_pd)} val records.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Worker {rank} ERROR reading Parquet: {e}\")\n",
        "            if is_distributed: dist.barrier()\n",
        "            return None\n",
        "\n",
        "        train_texts = train_pd[\"Consumer complaint narrative\"].fillna(\"\").astype(str).tolist()\n",
        "        train_labels = train_pd[\"is_target_complaint\"].tolist()\n",
        "        val_texts = val_pd[\"Consumer complaint narrative\"].fillna(\"\").astype(str).tolist()\n",
        "        val_labels = val_pd[\"is_target_complaint\"].tolist()\n",
        "\n",
        "        if is_rank_0: print(f\"Worker {rank}: Initializing model and tokenizer...\")\n",
        "        try:\n",
        "            if is_distributed and not is_rank_0: dist.barrier()\n",
        "            tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "            base_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "            if is_distributed and is_rank_0: dist.barrier()\n",
        "            classifier = EnhancedDistilBERTClassifier(base_model, dropout_rate=0.3)\n",
        "            classifier.to(device)\n",
        "        except Exception as e:\n",
        "            print(f\"Worker {rank}: ERROR initializing model/tokenizer: {e}\")\n",
        "            if is_distributed: dist.barrier()\n",
        "            return None\n",
        "\n",
        "        train_dataset = ComplaintDataset(train_texts, train_labels, tokenizer, BERT_MAX_LENGTH)\n",
        "        val_dataset = ComplaintDataset(val_texts, val_labels, tokenizer, BERT_MAX_LENGTH)\n",
        "        train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True) if is_distributed else RandomSampler(train_dataset)\n",
        "        val_sampler = SequentialSampler(val_dataset)\n",
        "        train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BERT_BATCH_SIZE, num_workers=2, pin_memory=True if use_gpu else False)\n",
        "        val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=BERT_BATCH_SIZE * 2, num_workers=2, pin_memory=True if use_gpu else False)\n",
        "\n",
        "        if is_distributed:\n",
        "            classifier = torch.nn.parallel.DistributedDataParallel(classifier, device_ids=[local_rank] if use_gpu else None, find_unused_parameters=False)\n",
        "\n",
        "        optimizer_params = classifier.parameters()\n",
        "        optimizer = AdamW(optimizer_params, lr=3e-5)\n",
        "        num_training_steps = len(train_dataloader) * NUM_EPOCHS\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * num_training_steps), num_training_steps=num_training_steps)\n",
        "\n",
        "        class_weights_tensor = torch.tensor([1.0, 1.0], dtype=torch.float)\n",
        "        if is_rank_0:\n",
        "            train_labels_tensor = torch.tensor(train_labels)\n",
        "            total = len(train_labels_tensor); pos_count = torch.sum(train_labels_tensor == 1).item(); neg_count = total - pos_count\n",
        "            if pos_count > 0 and neg_count > 0:\n",
        "                weight_for_0 = total / (2.0 * neg_count); weight_for_1 = total / (2.0 * pos_count)\n",
        "                class_weights_tensor = torch.tensor([weight_for_0, weight_for_1], dtype=torch.float)\n",
        "            print(f\"Rank 0 calculated class weights: {class_weights_tensor}\")\n",
        "\n",
        "        if is_distributed:\n",
        "            class_weights_tensor = class_weights_tensor.to(device); dist.broadcast(class_weights_tensor, src=0)\n",
        "        class_weights = class_weights_tensor.to(device)\n",
        "        if is_rank_0 or not is_distributed: print(f\"Effective class weights on device {device}: {class_weights}\")\n",
        "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "        best_val_f1 = 0.0; mlflow_run_id = None\n",
        "        with mlflow.start_run(run_name=\"bert_classifier_dist\", nested=True) if is_rank_0 else contextlib.nullcontext() as run:\n",
        "            if is_rank_0 and run:\n",
        "                mlflow_run_id = run.info.run_id\n",
        "                mlflow.log_params({ # Log multiple params\n",
        "                    \"learning_rate\": 3e-5, \"batch_size_per_worker\": BERT_BATCH_SIZE, \"world_size\": world_size,\n",
        "                    \"total_batch_size\": BERT_BATCH_SIZE * world_size, \"epochs\": NUM_EPOCHS, \"bert_model\": \"distilbert-base-uncased\",\n",
        "                    \"data_loading\": \"Parquet_Worker_Read\", \"num_train_samples_in_parquet\": len(train_texts),\n",
        "                    \"num_val_samples_in_parquet\": len(val_texts), \"train_sample_limit\": TRAIN_SAMPLE_LIMIT,\n",
        "                    \"val_sample_limit\": VAL_SAMPLE_LIMIT\n",
        "                })\n",
        "            for epoch in range(NUM_EPOCHS):\n",
        "                if is_rank_0: print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
        "                if isinstance(train_sampler, DistributedSampler): train_sampler.set_epoch(epoch)\n",
        "                classifier.train(); total_train_loss = 0.0; train_steps = 0\n",
        "                for step, batch in enumerate(train_dataloader):\n",
        "                    input_ids = batch['input_ids'].to(device); attention_mask = batch['attention_mask'].to(device); labels = batch['labels'].to(device)\n",
        "                    optimizer.zero_grad(); logits = classifier(input_ids=input_ids, attention_mask=attention_mask); loss = criterion(logits, labels)\n",
        "                    if torch.isnan(loss): print(f\"Worker {rank}: NaN loss at step {step}! Skipping.\"); continue\n",
        "                    loss.backward(); optimizer.step(); scheduler.step()\n",
        "                    total_train_loss += loss.item(); train_steps += 1\n",
        "                    if is_rank_0 and step % 50 == 0 and step > 0: print(f\"  Epoch {epoch+1}, Step {step}/{len(train_dataloader)}, Batch Loss: {loss.item():.4f}\")\n",
        "\n",
        "                avg_train_loss = total_train_loss / train_steps if train_steps > 0 else 0.0\n",
        "                if is_distributed:\n",
        "                    loss_tensor = torch.tensor(avg_train_loss, device=device); dist.all_reduce(loss_tensor, op=dist.ReduceOp.AVG); avg_train_loss = loss_tensor.item()\n",
        "                if is_rank_0:\n",
        "                    print(f\"Epoch {epoch+1} Avg Training Loss: {avg_train_loss:.4f}\")\n",
        "                    if mlflow_run_id: mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
        "\n",
        "                classifier.eval(); all_val_preds = []; all_val_labels = []; total_val_loss = 0.0; val_steps = 0\n",
        "                with torch.no_grad():\n",
        "                    for batch in val_dataloader:\n",
        "                        input_ids = batch['input_ids'].to(device); attention_mask = batch['attention_mask'].to(device); labels = batch['labels'].to(device)\n",
        "                        logits = classifier(input_ids=input_ids, attention_mask=attention_mask); loss = criterion(logits, labels)\n",
        "                        if not torch.isnan(loss): total_val_loss += loss.item(); val_steps += 1\n",
        "                        else: print(f\"Worker {rank}: NaN validation loss! Skipping.\")\n",
        "                        preds = torch.argmax(logits, dim=1)\n",
        "                        all_val_preds.extend(preds.cpu().numpy()); all_val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "                avg_val_loss = 0.0\n",
        "                if is_distributed:\n",
        "                    val_loss_sum_tensor = torch.tensor(total_val_loss, device=device); val_steps_tensor = torch.tensor(val_steps, device=device)\n",
        "                    dist.all_reduce(val_loss_sum_tensor, op=dist.ReduceOp.SUM); dist.all_reduce(val_steps_tensor, op=dist.ReduceOp.SUM)\n",
        "                    total_val_loss_agg = val_loss_sum_tensor.item(); total_val_steps_agg = val_steps_tensor.item()\n",
        "                    avg_val_loss = total_val_loss_agg / total_val_steps_agg if total_val_steps_agg > 0 else 0.0\n",
        "                else: avg_val_loss = total_val_loss / val_steps if val_steps > 0 else 0.0\n",
        "\n",
        "                if is_rank_0:\n",
        "                    np_val_labels = np.array(all_val_labels); np_val_preds = np.array(all_val_preds)\n",
        "                    val_accuracy = accuracy_score(np_val_labels, np_val_preds); val_precision = precision_score(np_val_labels, np_val_preds, average='binary', zero_division=0)\n",
        "                    val_recall = recall_score(np_val_labels, np_val_preds, average='binary', zero_division=0); val_f1 = f1_score(np_val_labels, np_val_preds, average='binary', zero_division=0)\n",
        "                    print(f\"Epoch {epoch+1} Avg Validation Loss: {avg_val_loss:.4f}\")\n",
        "                    print(f\"Validation Metrics (Rank 0): Acc: {val_accuracy:.4f}, P: {val_precision:.4f}, R: {val_recall:.4f}, F1: {val_f1:.4f}\")\n",
        "                    if mlflow_run_id: mlflow.log_metrics({\"val_loss\": avg_val_loss, \"val_accuracy\": val_accuracy, \"val_precision\": val_precision, \"val_recall\": val_recall, \"val_f1\": val_f1}, step=epoch)\n",
        "\n",
        "                    if val_f1 > best_val_f1:\n",
        "                        best_val_f1 = val_f1\n",
        "                        print(f\"  >>> New best F1: {val_f1:.4f}. Saving model to {EMBEDDING_MODEL_SAVE_PATH}...\")\n",
        "                        os.makedirs(EMBEDDING_MODEL_SAVE_PATH, exist_ok=True)\n",
        "                        model_to_save = classifier.module if is_distributed else classifier\n",
        "                        try:\n",
        "                             torch.save(model_to_save.state_dict(), f\"{EMBEDDING_MODEL_SAVE_PATH}/classifier.pt\")\n",
        "                             model_to_save.bert.save_pretrained(EMBEDDING_MODEL_SAVE_PATH)\n",
        "                             tokenizer.save_pretrained(EMBEDDING_MODEL_SAVE_PATH)\n",
        "                             print(f\"  >>> Model components saved successfully.\")\n",
        "                             if mlflow_run_id: mlflow.log_metric(\"best_val_f1\", best_val_f1, step=epoch)\n",
        "                        except Exception as save_e: print(f\"  >>> ERROR saving model: {save_e}\")\n",
        "                if is_distributed: dist.barrier()\n",
        "            if is_rank_0:\n",
        "                 print(f\"\\nTraining complete. Best Validation F1 across epochs: {best_val_f1:.4f}\")\n",
        "                 if mlflow_run_id: mlflow.log_metric(\"final_best_f1\", best_val_f1)\n",
        "        return EMBEDDING_MODEL_SAVE_PATH\n",
        "\n",
        "    # --- Driver-side setup ---\n",
        "    try:\n",
        "        print(\"Reading training data from Kafka for preprocessing...\")\n",
        "        full_schema_with_target = get_full_schema().add(StructField(\"is_target_complaint\", IntegerType(), True))\n",
        "        kafka_df = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", KAFKA_BROKERS).option(\"subscribe\", KAFKA_TOPIC_TRAINING).option(\"startingOffsets\", \"earliest\").option(\"failOnDataLoss\", \"false\").load()\n",
        "        kafka_read_count = kafka_df.count(); print(f\"Read {kafka_read_count} raw training messages from Kafka.\")\n",
        "        if kafka_read_count == 0: return None, None\n",
        "        training_df = kafka_df.select(F.from_json(F.col(\"value\").cast(\"string\"), full_schema_with_target).alias(\"data\")).select(\"data.*\").na.drop(subset=[\"Complaint ID\"]).dropDuplicates([\"Complaint ID\"])\n",
        "        print(f\"Parsed {training_df.count()} unique training records.\")\n",
        "\n",
        "        pos_df = training_df.filter(F.col(\"is_target_complaint\") == 1); neg_df = training_df.filter(F.col(\"is_target_complaint\") == 0)\n",
        "        pos_count = pos_df.count(); neg_count = neg_df.count(); print(f\"Training data counts: Positive={pos_count}, Negative={neg_count}\")\n",
        "        ratio = pos_count / (pos_count + neg_count) if (pos_count + neg_count) > 0 else 0.5; print(f\"Positive class ratio: {ratio:.2f}\")\n",
        "        if ratio > 0 and ratio < 0.1 and neg_count > pos_count:\n",
        "            print(\"Balancing dataset by undersampling negative class...\")\n",
        "            target_neg_fraction = min(1.0, (pos_count * 7.0) / neg_count) if neg_count > 0 else 1.0; print(f\"Target negative fraction: {target_neg_fraction:.2f}\")\n",
        "            neg_sample_df = neg_df.sample(False, target_neg_fraction, seed=42); balanced_df = pos_df.unionByName(neg_sample_df)\n",
        "            print(f\"Balanced dataset size: {balanced_df.count()} (+ve={pos_df.count()}, -ve={neg_sample_df.count()})\")\n",
        "        elif ratio > 0.9 and pos_count > neg_count:\n",
        "             print(\"Balancing dataset by undersampling positive class...\"); target_pos_fraction = min(1.0, (neg_count * 7.0) / pos_count) if pos_count > 0 else 1.0; print(f\"Target positive fraction: {target_pos_fraction:.2f}\")\n",
        "             pos_sample_df = pos_df.sample(False, target_pos_fraction, seed=42); balanced_df = neg_df.unionByName(pos_sample_df)\n",
        "             print(f\"Balanced dataset size: {balanced_df.count()} (+ve={pos_sample_df.count()}, -ve={neg_df.count()})\")\n",
        "        else: print(\"Dataset already reasonably balanced or cannot balance further.\"); balanced_df = training_df\n",
        "        balanced_df = balanced_df.cache(); balanced_count = balanced_df.count()\n",
        "        if balanced_count == 0: print(\"ERROR: Balanced DataFrame empty.\"); return None, None\n",
        "\n",
        "        train_spark_df, val_spark_df = balanced_df.randomSplit([0.9, 0.1], seed=42); print(f\"Split sizes: Train={train_spark_df.count()}, Val={val_spark_df.count()}\")\n",
        "        final_train_df = train_spark_df.limit(TRAIN_SAMPLE_LIMIT).cache(); final_val_df = val_spark_df.limit(VAL_SAMPLE_LIMIT).cache()\n",
        "        final_train_count = final_train_df.count(); final_val_count = final_val_df.count(); print(f\"Final sampled sizes for Parquet: Train={final_train_count}, Val={final_val_count}\")\n",
        "        if final_train_count == 0 or final_val_count == 0: print(\"ERROR: Not enough data after sampling.\"); return None, None\n",
        "\n",
        "        print(f\"Saving sampled training data to Parquet: {TRAIN_PARQUET_PATH}\"); final_train_df.select(\"Consumer complaint narrative\", \"is_target_complaint\").write.mode(\"overwrite\").parquet(TRAIN_PARQUET_PATH)\n",
        "        print(f\"Saving sampled validation data to Parquet: {VAL_PARQUET_PATH}\"); final_val_df.select(\"Consumer complaint narrative\", \"is_target_complaint\").write.mode(\"overwrite\").parquet(VAL_PARQUET_PATH)\n",
        "        balanced_df.unpersist(); final_train_df.unpersist(); final_val_df.unpersist()\n",
        "\n",
        "        print(\"Starting TorchDistributor...\")\n",
        "        if torch.cuda.is_available(): num_processes = torch.cuda.device_count(); use_gpu_dist_flag = True; print(f\"Torch reports GPU available. num_processes={num_processes}, use_gpu=True.\")\n",
        "        else: num_processes = 1; use_gpu_dist_flag = False; print(\"Torch reports NO GPU. num_processes=1, use_gpu=False.\")\n",
        "        print(\"NOTE: Each worker process loads full Parquet subset into memory.\")\n",
        "        distributor = None\n",
        "        try:\n",
        "            distributor = TorchDistributor(num_processes=num_processes, local_mode=True, use_gpu=use_gpu_dist_flag, _ssl_conf=None); print(\"TorchDistributor initialized.\")\n",
        "        except RuntimeError as e:\n",
        "            if \"GPUs were unable to be found on the driver\" in str(e):\n",
        "                print(\"WARN: TorchDistributor driver GPU check failed. Forcing CPU mode.\"); num_processes = 1; use_gpu_dist_flag = False\n",
        "                distributor = TorchDistributor(num_processes=num_processes, local_mode=True, use_gpu=use_gpu_dist_flag, _ssl_conf=None); print(\"TorchDistributor initialized in CPU fallback.\")\n",
        "            else: print(f\"ERROR initializing TorchDistributor: {e}\"); raise e\n",
        "        except Exception as e_init: print(f\"UNEXPECTED ERROR initializing TorchDistributor: {e_init}\"); raise e_init\n",
        "\n",
        "        saved_model_path = distributor.run(train_function)\n",
        "        if saved_model_path is None or not os.path.exists(os.path.join(saved_model_path, \"config.json\")): raise RuntimeError(\"Model training execution failed or model not saved.\")\n",
        "        print(f\"Training finished. Model components expected in: {saved_model_path}\")\n",
        "        tokenizer = DistilBertTokenizer.from_pretrained(saved_model_path)\n",
        "        return saved_model_path, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during training setup or distribution: {e}\")\n",
        "        import traceback; traceback.print_exc(); return None, None\n",
        "\n",
        "# --- Task 5: Unified Feature Pipeline ---\n",
        "def create_unified_pipeline():\n",
        "    print(\"\\n--- Task 5: Creating Unified Feature Pipeline ---\")\n",
        "    categorical_columns = [\"Company\", \"State\", \"Submitted via\"]\n",
        "    zip_col = \"ZIP code\"\n",
        "    stages = []\n",
        "    imputed_cols_map = {} # To map original names to imputed names\n",
        "\n",
        "    # Impute and Index Categorical/Zip columns first using SQLTransformer\n",
        "    impute_select_exprs = [\"*\"] # Start with all existing columns\n",
        "    for col_name in categorical_columns + [zip_col]:\n",
        "        imputed_col_name = f\"{col_name}_imputed\"\n",
        "        impute_select_exprs.append(f\"COALESCE(CAST({col_name} AS STRING), 'Unknown') AS {imputed_col_name}\")\n",
        "        imputed_cols_map[col_name] = imputed_col_name\n",
        "\n",
        "    impute_sql = f\"SELECT {', '.join(impute_select_exprs)} FROM __THIS__\"\n",
        "    stages.append(SQLTransformer(statement=impute_sql))\n",
        "\n",
        "    # BERT embedding transformer\n",
        "    stages.append(BERTEmbeddingTransformer(\n",
        "        inputCol=\"Consumer complaint narrative\", outputCol=\"narrative_features\", modelPath=EMBEDDING_MODEL_SAVE_PATH\n",
        "    ))\n",
        "\n",
        "    # Index and Encode the imputed columns\n",
        "    encoded_columns = []\n",
        "    for col_name in categorical_columns + [zip_col]:\n",
        "        imputed_col_name = imputed_cols_map[col_name] # Get the imputed column name\n",
        "        indexer_output = f\"{col_name}_indexed\"\n",
        "        encoder_output = f\"{col_name}_encoded\"\n",
        "        stages.append(StringIndexer(inputCol=imputed_col_name, outputCol=indexer_output, handleInvalid=\"keep\"))\n",
        "        stages.append(OneHotEncoder(inputCol=indexer_output, outputCol=encoder_output, dropLast=False))\n",
        "        encoded_columns.append(encoder_output)\n",
        "\n",
        "    # Feature assembly (Ensure narrative_features exists and is first)\n",
        "    feature_columns = [\"narrative_features\"] + encoded_columns\n",
        "    stages.append(VectorAssembler(inputCols=feature_columns, outputCol=\"features\", handleInvalid=\"keep\"))\n",
        "\n",
        "    pipeline = Pipeline(stages=stages)\n",
        "    print(f\"Pipeline created with stages: {[type(s).__name__ for s in stages]}\")\n",
        "    return pipeline\n",
        "\n",
        "\n",
        "# --- Task 6: Simulation Script ---\n",
        "def simulate_test_data_to_kafka():\n",
        "    print(\"\\n--- Task 6: Test Data Simulation ---\")\n",
        "    if not os.path.exists(TEST_DATA_PERSISTENCE_PATH): return 0\n",
        "    print(f\"Loading test data from: {TEST_DATA_PERSISTENCE_PATH}\")\n",
        "    try: test_pd = pd.read_parquet(TEST_DATA_PERSISTENCE_PATH); print(f\"Loaded {len(test_pd)} test records.\")\n",
        "    except Exception as e: print(f\"ERROR loading test data: {e}\"); return 0\n",
        "    messages = test_pd.to_dict('records'); total_messages = len(messages)\n",
        "    if total_messages == 0: print(\"No test messages.\"); return 0\n",
        "    producer = None\n",
        "    try:\n",
        "        producer = KafkaProducer(bootstrap_servers=KAFKA_BROKERS.split(','), value_serializer=lambda v: json.dumps(v).encode('utf-8'), key_serializer=lambda k: str(k).encode('utf-8'), batch_size=16384, linger_ms=10, retries=3)\n",
        "        print(f\"Kafka Producer connected for simulation.\")\n",
        "    except kafka_errors.NoBrokersAvailable: print(f\"ERROR: Simulation Kafka Producer failed connect.\"); return 0\n",
        "    delay = (60.0 / MESSAGES_PER_MINUTE) * BATCH_SIZE if MESSAGES_PER_MINUTE > 0 else 0; print(f\"Starting simulation: {total_messages} msgs @ ~{MESSAGES_PER_MINUTE}/min (Batch: {BATCH_SIZE}, Delay: {delay:.2f}s)\")\n",
        "    start_time = time.time(); messages_sent = 0\n",
        "    try:\n",
        "        for i in range(0, total_messages, BATCH_SIZE):\n",
        "            batch_start = time.time(); batch_end = min(i + BATCH_SIZE, total_messages); batch = messages[i:batch_end]\n",
        "            for msg in batch: producer.send(KAFKA_TOPIC_TESTING_STREAM, key=msg.get(\"Complaint ID\", str(messages_sent)), value=msg); messages_sent += 1\n",
        "            producer.flush(); batch_elapsed = time.time() - batch_start; sleep_time = max(0, delay - batch_elapsed)\n",
        "            if sleep_time > 0: time.sleep(sleep_time)\n",
        "            if messages_sent % (BATCH_SIZE * 10) == 0 or messages_sent == total_messages: # Log less often\n",
        "                elapsed = time.time() - start_time; rate = (messages_sent / elapsed * 60) if elapsed > 0 else 0\n",
        "                print(f\"  Sim Progress: {messages_sent}/{total_messages} ({messages_sent/total_messages*100:.1f}%) @ {rate:.1f} msgs/min\")\n",
        "    except KeyboardInterrupt: print(\"\\nSim interrupted.\")\n",
        "    except Exception as e: print(f\"\\nERROR during sim: {e}\")\n",
        "    finally:\n",
        "        if producer: producer.close()\n",
        "        elapsed = time.time() - start_time; rate = (messages_sent / elapsed * 60) if elapsed > 0 else 0\n",
        "        print(\"\\nSim Summary:\"); print(f\"- Sent: {messages_sent}/{total_messages}\"); print(f\"- Time: {elapsed:.2f}s\"); print(f\"- Rate: {rate:.1f} msg/min\")\n",
        "    return messages_sent\n",
        "\n",
        "# --- Task 7: Streaming Inference Job ---\n",
        "@F.pandas_udf(DoubleType())\n",
        "def predict_udf(features_iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
        "    import pandas as pd; import torch; import numpy as np\n",
        "    from transformers import DistilBertModel; import os; from collections import OrderedDict\n",
        "    # Need EnhancedDistilBERTClassifier definition available globally or redefine here\n",
        "    # Assuming global definition is accessible\n",
        "\n",
        "    model_path = EMBEDDING_MODEL_SAVE_PATH; classifier_state_path = os.path.join(model_path, \"classifier.pt\")\n",
        "    device = torch.device(\"cpu\"); classifier_head = None; embedding_dim = 768\n",
        "    try:\n",
        "        if os.path.exists(classifier_state_path) and os.path.exists(model_path):\n",
        "            bert_config = DistilBertModel.from_pretrained(model_path).config; embedding_dim = bert_config.dim\n",
        "            temp_bert = DistilBertModel(bert_config); classifier_head = EnhancedDistilBERTClassifier(temp_bert, dropout_rate=0.3)\n",
        "            state_dict = torch.load(classifier_state_path, map_location=device)\n",
        "            new_state_dict = OrderedDict(); is_ddp = any(k.startswith('module.') for k in state_dict.keys())\n",
        "            for k, v in state_dict.items(): name = k[7:] if is_ddp else k; new_state_dict[name] = v\n",
        "            classifier_head.load_state_dict(new_state_dict); classifier_head.to(device); classifier_head.eval()\n",
        "            # print(f\"Worker loaded classifier head. Embedding dim: {embedding_dim}\") # Reduce noise\n",
        "        else: print(f\"Worker WARN: Model/State not found ('{model_path}', '{classifier_state_path}'). Default preds.\")\n",
        "    except Exception as e: print(f\"Worker ERROR loading model: {e}. Default preds.\"); classifier_head = None\n",
        "\n",
        "    for features_series in features_iterator:\n",
        "        if features_series.empty: yield pd.Series([], dtype=float); continue\n",
        "        results = []\n",
        "        if classifier_head is not None:\n",
        "            try:\n",
        "                feature_list = []; expected_len = -1\n",
        "                valid_indices = [] # Track indices that are valid\n",
        "                for i, f in enumerate(features_series.tolist()):\n",
        "                    if f is not None:\n",
        "                        vec = np.array(f)\n",
        "                        current_len = len(vec)\n",
        "                        if expected_len == -1: expected_len = current_len\n",
        "                        if current_len == expected_len: feature_list.append(vec); valid_indices.append(i)\n",
        "                        else: print(f\"WARN: Skip vector len {current_len} != exp {expected_len}\")\n",
        "                    # else: print(\"WARN: Skip None vector\") # Can be noisy\n",
        "                if not feature_list: # If no valid vectors in batch\n",
        "                    yield pd.Series([0.0] * len(features_series), dtype=float); continue\n",
        "\n",
        "                feature_vectors = np.stack(feature_list); batch_tensor = torch.tensor(feature_vectors, dtype=torch.float32).to(device)\n",
        "                bert_embeddings = batch_tensor[:, :embedding_dim] # Assumes correct order/dim\n",
        "                with torch.no_grad():\n",
        "                    logits = classifier_head(embeddings=bert_embeddings)\n",
        "                    predictions = torch.argmax(logits, dim=1).cpu().numpy().astype(float)\n",
        "\n",
        "                # Map predictions back to original series positions\n",
        "                results_array = np.full(len(features_series), 0.0) # Default 0.0\n",
        "                for i, pred_idx in enumerate(valid_indices):\n",
        "                     results_array[pred_idx] = predictions[i]\n",
        "                results = results_array.tolist()\n",
        "\n",
        "            except IndexError as slice_e: print(f\"Worker ERROR slicing embed (dim {embedding_dim}): {slice_e}\"); results = [0.0] * len(features_series)\n",
        "            except ValueError as stack_e: print(f\"Worker ERROR stacking vectors: {stack_e}\"); results = [0.0] * len(features_series)\n",
        "            except Exception as e: print(f\"Worker ERROR during pred batch: {e}\"); results = [0.0] * len(features_series)\n",
        "        else: results = np.random.binomial(1, 0.3, len(features_series)).astype(float).tolist() # Fallback\n",
        "        yield pd.Series(results, dtype=float)\n",
        "\n",
        "def run_streaming_inference_job(pipeline_model_path):\n",
        "    print(\"\\n--- Task 7: Streaming Inference Job ---\")\n",
        "    try:\n",
        "        print(f\"Loading fitted pipeline model from: {pipeline_model_path}\")\n",
        "        pipeline_model = PipelineModel.load(pipeline_model_path); print(\"Pipeline model loaded.\")\n",
        "    except Exception as e: print(f\"ERROR loading pipeline model: {e}\"); import traceback; traceback.print_exc(); return None, None\n",
        "\n",
        "    metrics_listener = MetricsListener(KAFKA_BROKERS, KAFKA_TOPIC_METRICS)\n",
        "    spark.streams.addListener(metrics_listener)\n",
        "    stream_schema = get_streaming_schema()\n",
        "    print(f\"Setting up streaming source from Kafka topic: {KAFKA_TOPIC_TESTING_STREAM}\")\n",
        "    kafka_stream = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", KAFKA_BROKERS).option(\"subscribe\", KAFKA_TOPIC_TESTING_STREAM).option(\"startingOffsets\", \"latest\").option(\"failOnDataLoss\", \"false\").load()\n",
        "    parsed_stream = kafka_stream.select(F.from_json(F.col(\"value\").cast(\"string\"), stream_schema).alias(\"data\")).select(\"data.*\").na.drop(subset=[\"Complaint ID\"])\n",
        "    processed_stream = pipeline_model.transform(parsed_stream)\n",
        "    prediction_stream = processed_stream.filter(F.col(\"features\").isNotNull()).withColumn(\"prediction\", predict_udf(F.col(\"features\")))\n",
        "    final_stream = prediction_stream.select(\n",
        "        F.col(\"Complaint ID\").alias(\"complaint_id\"), F.col(\"prediction\"), F.col(\"State\").alias(\"state\"),\n",
        "        F.col(\"ZIP code\").alias(\"zip_code\"), F.col(\"Submitted via\").alias(\"submitted_via\"), F.current_timestamp().alias(\"inference_time\")\n",
        "    ).withColumn(\"inference_time_str\", F.date_format(\"inference_time\", \"yyyy-MM-dd HH:mm:ss\"))\n",
        "\n",
        "    kafka_output = final_stream.selectExpr(\"complaint_id AS key\", \"to_json(struct(*)) AS value\")\n",
        "    print(f\"Starting streaming query to write predictions to Kafka topic: {KAFKA_TOPIC_PREDICTIONS}\")\n",
        "    query = kafka_output.writeStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", KAFKA_BROKERS).option(\"topic\", KAFKA_TOPIC_PREDICTIONS).option(\"checkpointLocation\", f\"{STREAMING_CHECKPOINT_LOCATION}/predictions\").outputMode(\"append\").trigger(processingTime=\"10 seconds\").start()\n",
        "    console_query = final_stream.writeStream.format(\"console\").option(\"truncate\", \"false\").option(\"numRows\", 5).trigger(processingTime=\"15 seconds\").outputMode(\"append\").start()\n",
        "    print(\"Streaming queries started.\")\n",
        "    return query, console_query\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "def main():\n",
        "    print(\"\\n--- Starting Main Pipeline Execution ---\")\n",
        "    start_pipeline_time = time.time()\n",
        "    metrics_listener_main = None # To close explicitly later\n",
        "    pipeline_failed = False\n",
        "    active_queries = [] # Track active queries\n",
        "\n",
        "    try:\n",
        "        print(\"Checking Kafka connection...\")\n",
        "        try:\n",
        "             temp_producer = KafkaProducer(bootstrap_servers=KAFKA_BROKERS.split(','), request_timeout_ms=5000)\n",
        "             temp_producer.close(); print(\"Kafka connection test successful.\")\n",
        "        except kafka_errors.NoBrokersAvailable:\n",
        "             print(f\"FATAL ERROR: Cannot connect to Kafka at {KAFKA_BROKERS}. Check ZK/Kafka server status.\"); return\n",
        "\n",
        "        # Task 1: Load data\n",
        "        load_data_to_kafka() # Just need data in Kafka\n",
        "\n",
        "        # Task 2: Preprocess (reads from Kafka)\n",
        "        filtered_df = preprocess_filter_and_visualize()\n",
        "        if filtered_df is None: raise ValueError(\"Preprocessing failed or yielded no data.\")\n",
        "\n",
        "        # Task 3: Split & Label (writes train to Kafka, test to Parquet)\n",
        "        training_df_cached, _ = split_label_and_prepare_data(filtered_df) # Keep training df cached for fitting pipeline\n",
        "        if training_df_cached is None: raise ValueError(\"Data splitting/labeling failed.\")\n",
        "\n",
        "        # Task 4: Train Model\n",
        "        model_path, tokenizer = train_bert_model_distributed()\n",
        "        if model_path is None or tokenizer is None: raise RuntimeError(\"Model training failed.\")\n",
        "        print(f\"Model training completed. Model saved in: {model_path}\")\n",
        "\n",
        "        # Task 5: Create and Save Feature Pipeline\n",
        "        print(\"Fitting the unified feature pipeline...\")\n",
        "        pipeline = create_unified_pipeline()\n",
        "        fit_sample_df = training_df_cached.limit(100) # Use cached labeled data for fitting\n",
        "        pipeline_model = pipeline.fit(fit_sample_df)\n",
        "        pipeline_model.write().overwrite().save(TRAINING_PIPELINE_SAVE_PATH)\n",
        "        print(f\"Fitted pipeline saved to: {TRAINING_PIPELINE_SAVE_PATH}\")\n",
        "        if training_df_cached.is_cached: training_df_cached.unpersist() # Unpersist after fitting\n",
        "\n",
        "        # Task 6: Simulate Test Data Stream\n",
        "        messages_simulated = simulate_test_data_to_kafka()\n",
        "        if messages_simulated == 0: print(\"WARNING: No test data simulated.\")\n",
        "\n",
        "        # Task 7: Start Streaming Inference Job\n",
        "        # Pass the correct path where the fitted pipeline was saved\n",
        "        query, console_query = run_streaming_inference_job(TRAINING_PIPELINE_SAVE_PATH)\n",
        "        if query is None: raise RuntimeError(\"Failed to start streaming inference job.\")\n",
        "        active_queries.extend([query, console_query]) # Track queries\n",
        "\n",
        "        print(\"\\n--- Pipeline Running ---\"); print(\"Streaming inference started.\"); print(\">>> Press Ctrl+C in Colab cell to stop. <<<\")\n",
        "        # Keep main thread alive while streams run (alternative to awaitTermination)\n",
        "        while any(q.isActive for q in active_queries): time.sleep(5)\n",
        "        print(\"Streaming queries seem to have stopped.\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nKeyboardInterrupt received. Stopping pipeline...\")\n",
        "        pipeline_failed = True\n",
        "    except Exception as e:\n",
        "        print(f\"\\nFATAL ERROR in pipeline execution: {e}\")\n",
        "        import traceback; traceback.print_exc(); pipeline_failed = True\n",
        "    finally:\n",
        "        print(\"\\n--- Cleaning up ---\")\n",
        "        stopped_count = 0\n",
        "        for q in active_queries:\n",
        "            if q and q.isActive:\n",
        "                print(f\"Stopping query '{q.name}' (id: {q.id})...\")\n",
        "                try: q.stop(); q.awaitTermination(timeout=10); stopped_count += 1; print(\"Stopped.\")\n",
        "                except Exception as stop_e: print(f\"Error stopping query '{q.name}': {stop_e}\")\n",
        "        print(f\"Stopped {stopped_count} active streaming queries.\")\n",
        "\n",
        "        # Explicitly close metrics listener producer\n",
        "        # Find listener instance (assuming only one was added)\n",
        "        for listener in spark.streams.listListeners():\n",
        "             if isinstance(listener, MetricsListener):\n",
        "                 listener.close_producer()\n",
        "                 break # Assume only one\n",
        "\n",
        "        # Optional: Stop Spark session\n",
        "        # print(\"Stopping Spark session...\")\n",
        "        # spark.stop()\n",
        "\n",
        "        end_pipeline_time = time.time()\n",
        "        print(f\"\\nTotal Pipeline Execution Time: {end_pipeline_time - start_pipeline_time:.2f} seconds\")\n",
        "        status = \"finished with errors or was interrupted\" if pipeline_failed else \"finished successfully (streaming stopped)\"\n",
        "        print(f\"Pipeline {status}.\")\n",
        "\n",
        "# --- Run Main ---\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "5HoknUAWaUc5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}