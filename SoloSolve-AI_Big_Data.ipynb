{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShovalBenjer/Bigdata_Pyspark_Spark_Hadoop_Apache/blob/main/SoloSolve-AI_Big_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **‚ö° Spark & MLflow Setup**\n",
        "\n",
        "## üìå **Import Dependencies**\n",
        "The following libraries are imported:\n",
        "- **PySpark**: `SparkSession`, `SparkConf`\n",
        "- **MLflow**: `mlflow`, `mlflow.spark`\n",
        "\n",
        "---\n",
        "\n",
        "## üîß **Define Spark Session Creator (`create_spark_session`)**\n",
        "A function is implemented to **manage the SparkSession lifecycle**, ensuring:\n",
        "- **Reusing an existing session** when available.\n",
        "- **Applying essential configurations**, such as:\n",
        "  - Memory allocation\n",
        "  - Package dependencies\n",
        "  - Checkpointing settings\n",
        "  - Performance optimizations\n",
        "- **Robust error handling** to catch and log session creation failures.  \n",
        "\n",
        "üìå **Refer to the function docstring for configuration details.**\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ **Initialize Spark**\n",
        "- Calls `create_spark_session` to **initialize Spark**.\n",
        "- Assigns the resulting **SparkSession** instance to the `spark` variable.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç **Log Spark Info**\n",
        "- Prints the **Spark version** and **active configuration settings** to the console for verification.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä **Setup MLflow**\n",
        "1. **Configure Tracking**:\n",
        "   - Sets **MLflow Tracking URI** to a local directory (`MLFLOW_DIR`).\n",
        "\n",
        "2. **Manage Experiment**:\n",
        "   - Attempts to retrieve an **MLflow experiment** matching `SPARK_APP_NAME`.\n",
        "   - If the experiment **does not exist**, it is **created**.\n",
        "\n",
        "3. **Log Experiment Details**:\n",
        "   - Displays the **MLflow tracking URI** and the **active experiment name**.\n",
        "\n",
        "4. **Error Handling**:\n",
        "   - Catches potential **MLflow setup issues**.\n",
        "   - Prints a **warning message** if an error occurs, but **allows execution to continue**.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ **Final Verification**\n",
        "- Ensures **SparkSession** is running properly.\n",
        "- Confirms **MLflow setup** is successful (or logs any issues).\n"
      ],
      "metadata": {
        "id": "xFd34P_MPh7d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIYdn1woOS1n"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "import mlflow\n",
        "import mlflow.spark\n",
        "# Assuming logger and constants (CHECKPOINT_DIR, SPARK_APP_NAME, etc.) are defined elsewhere\n",
        "\n",
        "def create_spark_session():\n",
        "    \"\"\"\n",
        "    Creates or retrieves and configures a PySpark SparkSession.\n",
        "\n",
        "    What:\n",
        "        This function initializes a SparkSession, which is the entry point\n",
        "        for programming Spark with the Dataset and DataFrame API. It ensures\n",
        "        that only one session is active and configured correctly.\n",
        "\n",
        "    Why:\n",
        "        - To provide a centralized and standardized way to obtain a SparkSession.\n",
        "        - To reuse existing sessions if available, preventing resource overhead.\n",
        "        - To apply consistent configurations (memory, packages, performance tuning,\n",
        "          checkpointing) required for the application's Spark jobs.\n",
        "        - To manage Spark's logging verbosity.\n",
        "\n",
        "    How:\n",
        "        1.  **Check Existing Session:** It first attempts to retrieve an active\n",
        "            SparkSession using `SparkSession.getActiveSession()`.\n",
        "        2.  **Configure Existing:** If a session exists, it updates its configuration\n",
        "            with necessary settings like checkpoint location, adaptive query\n",
        "            execution, and shuffle partitions. The log level is set to 'WARN'.\n",
        "            The existing, re-configured session is then returned. A warning is\n",
        "            logged if checking fails.\n",
        "        3.  **Create New Session:** If no active session is found or an error\n",
        "            occurred during the check, it proceeds to create a new one.\n",
        "        4.  **Configure New:** A `SparkConf` object is instantiated and configured\n",
        "            with the application name, master URL, driver/executor memory,\n",
        "            required JAR packages, checkpoint location, adaptive execution, and\n",
        "            shuffle partitions using predefined constants.\n",
        "        5.  **Build Session:** `SparkSession.builder.config(conf=conf).getOrCreate()`\n",
        "            is used to build the session with the specified configuration.\n",
        "            `getOrCreate` ensures that if a session was created concurrently\n",
        "            elsewhere, that session is returned.\n",
        "        6.  **Set Log Level:** The SparkContext's log level is set to 'WARN' to\n",
        "            reduce console output noise.\n",
        "        7.  **Error Handling:** Both checking for existing sessions and creating\n",
        "            new ones are wrapped in try-except blocks to log potential errors\n",
        "            using the 'logger' object. Creation failure raises the exception.\n",
        "\n",
        "    Returns:\n",
        "        pyspark.sql.SparkSession: The active, configured SparkSession instance.\n",
        "\n",
        "    Raises:\n",
        "        Exception: Propagates exceptions encountered during SparkSession creation\n",
        "                   if it fails after logging the error.\n",
        "\n",
        "    Note:\n",
        "        This function relies on globally defined constants such as SPARK_APP_NAME,\n",
        "        SPARK_MASTER, SPARK_DRIVER_MEMORY, SPARK_EXECUTOR_MEMORY, SPARK_PACKAGES,\n",
        "        CHECKPOINT_DIR, and a pre-configured 'logger' object.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        existing_spark = SparkSession.getActiveSession()\n",
        "        if existing_spark:\n",
        "            # logger.info(\"Using existing Spark session\") # Assuming logger is available\n",
        "            existing_spark.conf.set(\"spark.sql.streaming.checkpointLocation\", CHECKPOINT_DIR)\n",
        "            existing_spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "            existing_spark.conf.set(\"spark.sql.shuffle.partitions\", \"10\")\n",
        "            existing_spark.sparkContext.setLogLevel(\"WARN\")\n",
        "            return existing_spark\n",
        "    except Exception as e:\n",
        "        # logger.warning(f\"Error checking for existing session: {str(e)}\") # Assuming logger is available\n",
        "        pass # Proceed to create a new session\n",
        "\n",
        "    try:\n",
        "        conf = SparkConf().setAppName(SPARK_APP_NAME).setMaster(SPARK_MASTER)\n",
        "        conf.set(\"spark.driver.memory\", SPARK_DRIVER_MEMORY)\n",
        "        conf.set(\"spark.executor.memory\", SPARK_EXECUTOR_MEMORY)\n",
        "        if SPARK_PACKAGES:\n",
        "            conf.set(\"spark.jars.packages\", \",\".join(SPARK_PACKAGES))\n",
        "        conf.set(\"spark.sql.streaming.checkpointLocation\", CHECKPOINT_DIR)\n",
        "        conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "        conf.set(\"spark.sql.shuffle.partitions\", \"10\")\n",
        "        builder = SparkSession.builder.config(conf=conf)\n",
        "        spark = builder.getOrCreate()\n",
        "        spark.sparkContext.setLogLevel(\"WARN\")\n",
        "        return spark\n",
        "    except Exception as e:\n",
        "        # logger.error(f\"Failed to create Spark session: {str(e)}\") # Assuming logger is available\n",
        "        raise\n",
        "\n",
        "spark = create_spark_session()\n",
        "\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"Spark Configuration:\")\n",
        "for item in sorted(spark.sparkContext.getConf().getAll()):\n",
        "    print(f\"  {item[0]}: {item[1]}\")\n",
        "\n",
        "try:\n",
        "    mlflow.set_tracking_uri(f\"file:{MLFLOW_DIR}\")\n",
        "    experiment = mlflow.get_experiment_by_name(SPARK_APP_NAME)\n",
        "    if experiment is None:\n",
        "        experiment_id = mlflow.create_experiment(SPARK_APP_NAME)\n",
        "        experiment = mlflow.get_experiment(experiment_id)\n",
        "        # logger.info(f\"Created new MLflow experiment: {SPARK_APP_NAME}\") # Assuming logger is available\n",
        "    else:\n",
        "        # logger.info(f\"Using existing MLflow experiment: {SPARK_APP_NAME}\") # Assuming logger is available\n",
        "        pass\n",
        "    print(f\"\\nMLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
        "    print(f\"MLflow experiment: {experiment.name}\")\n",
        "except Exception as e:\n",
        "    # logger.error(f\"Error setting up MLflow: {str(e)}\") # Assuming logger is available\n",
        "    print(f\"\\nWarning: MLflow setup failed. Continuing without experiment tracking.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **üîó Kafka Connection & Topic Management**\n",
        "\n",
        "## üìå **Import Dependencies**\n",
        "The following Python modules are imported:\n",
        "- **Kafka Interaction**: `kafka-python`\n",
        "- **System Commands & JSON Handling**: `subprocess`, `json`\n",
        "\n",
        "---\n",
        "\n",
        "## üîç **Define Kafka Connection Check (`check_kafka_connection`)**\n",
        "A function is defined to perform a **basic connectivity test** to the Kafka brokers by:\n",
        "1. Attempting to instantiate a **Kafka Producer**.\n",
        "2. Attempting to instantiate a **Kafka Consumer**.\n",
        "3. Closing both instances after the check.  \n",
        "\n",
        "üìå **For more details, refer to the function docstring.**\n",
        "\n",
        "---\n",
        "\n",
        "## üõ† **Define Topic Verification & Creation (`verify_create_kafka_topics`)**\n",
        "A function is defined to ensure that all **required Kafka topics** exist by:\n",
        "1. **Listing existing topics**:  \n",
        "   - Uses **Kafka Admin API** first.  \n",
        "   - Falls back to **CLI-based listing** if necessary.  \n",
        "2. **Creating missing topics**:  \n",
        "   - Uses **Admin API** as the first approach.  \n",
        "   - Falls back to **CLI commands** if API creation fails.  \n",
        "\n",
        "üìå **For more details, refer to the function docstring.**\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ **Execute Connection Check**\n",
        "- Calls the `check_kafka_connection` function to test **Kafka connectivity**.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è **Handle Connection Result**\n",
        "- **If the connection fails**:\n",
        "  - Displays a **warning message** with guidance on how to start Kafka.  \n",
        "- **If the connection succeeds**:\n",
        "  - Calls `verify_create_kafka_topics` to ensure required topics are available.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ **Summarize Results**\n",
        "- Prints a summary of the **Kafka connection status**.\n",
        "- Lists **newly created topics** (if any) during execution.\n"
      ],
      "metadata": {
        "id": "g24iyEdiP2FK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import json\n",
        "from kafka.admin import KafkaAdminClient, NewTopic\n",
        "from kafka import KafkaProducer, KafkaConsumer\n",
        "# Assuming logger and constants (KAFKA_BROKERS, KAFKA_TOPIC_*, KAFKA_HOME) are defined elsewhere\n",
        "\n",
        "def check_kafka_connection():\n",
        "    \"\"\"\n",
        "    Verifies connectivity to the configured Kafka brokers.\n",
        "\n",
        "    What:\n",
        "        This function attempts to establish a basic connection to the Kafka\n",
        "        cluster defined by the KAFKA_BROKERS constant.\n",
        "\n",
        "    Why:\n",
        "        To confirm that the Kafka cluster is accessible from the current\n",
        "        environment before proceeding with operations like topic creation\n",
        "        or data production/consumption. This prevents downstream errors\n",
        "        caused by network issues or Kafka service unavailability.\n",
        "\n",
        "    How:\n",
        "        1.  Instantiates a `KafkaProducer` targeting the specified `KAFKA_BROKERS`.\n",
        "            A basic JSON serializer is configured, though not strictly needed\n",
        "            for the connection check itself.\n",
        "        2.  Immediately closes the producer.\n",
        "        3.  Instantiates a `KafkaConsumer` targeting the same brokers. A short\n",
        "            timeout is set to avoid indefinite blocking if brokers are down.\n",
        "        4.  Immediately closes the consumer.\n",
        "        5.  If both instantiation and closing succeed without exceptions, it\n",
        "            logs a success message and returns `True`.\n",
        "        6.  If any exception occurs during this process (e.g., connection refused,\n",
        "            timeout), it logs an error message detailing the failure and\n",
        "            returns `False`.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if a connection could be established and resources closed,\n",
        "              False otherwise.\n",
        "\n",
        "    Note:\n",
        "        Relies on the globally defined `KAFKA_BROKERS` constant and a\n",
        "        pre-configured `logger` object.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        producer = KafkaProducer(\n",
        "            bootstrap_servers=KAFKA_BROKERS,\n",
        "            value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
        "        )\n",
        "        producer.close()\n",
        "        consumer = KafkaConsumer(\n",
        "            bootstrap_servers=KAFKA_BROKERS,\n",
        "            auto_offset_reset='earliest',\n",
        "            consumer_timeout_ms=5000\n",
        "        )\n",
        "        consumer.close()\n",
        "        # logger.info(\"‚úÖ Kafka connection successful\") # Assuming logger is available\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        # logger.error(f\"‚ùå Kafka connection failed: {str(e)}\") # Assuming logger is available\n",
        "        return False\n",
        "\n",
        "def verify_create_kafka_topics():\n",
        "    \"\"\"\n",
        "    Checks for the existence of required Kafka topics and creates any that are missing.\n",
        "\n",
        "    What:\n",
        "        Ensures that a predefined list of Kafka topics, necessary for the\n",
        "        application's workflow, exists within the connected Kafka cluster.\n",
        "\n",
        "    Why:\n",
        "        To automate the setup of essential Kafka infrastructure, preventing errors\n",
        "        that would occur if producers or consumers attempt to interact with\n",
        "        non-existent topics. It provides robustness by trying multiple methods\n",
        "        for listing and creating topics.\n",
        "\n",
        "    How:\n",
        "        1.  **Define Requirements:** A list of necessary topic names is defined using\n",
        "            globally available constants (e.g., `KAFKA_TOPIC_RAW`).\n",
        "        2.  **List Existing Topics (Attempt 1: API):** It first tries to retrieve\n",
        "            the list of all existing topics using `kafka-python`'s\n",
        "            `KafkaConsumer.topics()` method.\n",
        "        3.  **List Existing Topics (Attempt 2: CLI Fallback):** If the API method fails\n",
        "            or returns an empty set, it attempts to list topics by executing the\n",
        "            `kafka-topics.sh --list` command via `subprocess.run`, parsing the output.\n",
        "            This requires `KAFKA_HOME` to be set correctly. Errors during CLI execution\n",
        "            are logged.\n",
        "        4.  **Identify Missing Topics:** The required topics list is compared against\n",
        "            the retrieved list of existing topics to determine which ones need creation.\n",
        "        5.  **Create Missing Topics (Attempt 1: Admin API):** If missing topics are\n",
        "            found, it first tries to create them using `kafka-python`'s\n",
        "            `KafkaAdminClient`. It configures `NewTopic` objects with desired\n",
        "            partition counts and replication factors before calling `create_topics`.\n",
        "        6.  **Create Missing Topics (Attempt 2: CLI Fallback):** If the Admin API\n",
        "            creation fails (e.g., due to permissions or API issues), it falls back\n",
        "            to creating each missing topic individually by executing the\n",
        "            `kafka-topics.sh --create` command via `subprocess.run`. Errors during\n",
        "            CLI creation are logged per topic.\n",
        "        7.  **Logging:** Informational messages are logged (using `logger`) about\n",
        "            existing topics, topics being created, and the success or failure of\n",
        "            creation methods. Error messages detail any exceptions or CLI failures.\n",
        "        8.  **Return Value:** The function returns a list containing the names of\n",
        "            the topics that were successfully created during its execution. If no\n",
        "            topics needed creation or creation failed, an empty list may be returned.\n",
        "\n",
        "    Returns:\n",
        "        list[str]: A list of topic names that were newly created by this function.\n",
        "                   Returns an empty list if all topics already existed or if\n",
        "                   creation failed completely.\n",
        "\n",
        "    Note:\n",
        "        Relies on globally defined constants `KAFKA_BROKERS`, `KAFKA_HOME`,\n",
        "        `KAFKA_TOPIC_*`, and a pre-configured `logger` object. Assumes appropriate\n",
        "        permissions for listing and creating topics via API or CLI. The partition\n",
        "        count (3) and replication factor (1) are hardcoded but could be parameterized.\n",
        "    \"\"\"\n",
        "    required_topics = [\n",
        "        KAFKA_TOPIC_RAW,\n",
        "        KAFKA_TOPIC_TRAINING,\n",
        "        KAFKA_TOPIC_TESTING_STREAM,\n",
        "        KAFKA_TOPIC_PREDICTIONS,\n",
        "        KAFKA_TOPIC_METRICS\n",
        "    ]\n",
        "    existing_topics = set()\n",
        "    try:\n",
        "        # logger.info(\"Checking existing Kafka topics...\") # Assuming logger is available\n",
        "        try:\n",
        "            consumer = KafkaConsumer(bootstrap_servers=KAFKA_BROKERS, request_timeout_ms=6000)\n",
        "            existing_topics = consumer.topics()\n",
        "            consumer.close()\n",
        "            if existing_topics:\n",
        "                # logger.info(f\"Existing topics (via API): {existing_topics}\") # Assuming logger is available\n",
        "                pass\n",
        "        except Exception as api_e:\n",
        "             # logger.warning(f\"Failed to list topics via API ({api_e}), trying CLI...\") # Assuming logger is available\n",
        "             existing_topics = set() # Ensure it's empty if API failed\n",
        "\n",
        "        if not existing_topics and KAFKA_HOME: # Try CLI only if API failed/empty and KAFKA_HOME is set\n",
        "            cmd = f\"{KAFKA_HOME}/bin/kafka-topics.sh --list --bootstrap-server {KAFKA_BROKERS}\"\n",
        "            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=10)\n",
        "            if result.returncode == 0:\n",
        "                existing_topics = set(topic for topic in result.stdout.strip().split('\\n') if topic) # Filter empty lines\n",
        "                # logger.info(f\"Existing topics (via CLI): {existing_topics}\") # Assuming logger is available\n",
        "            else:\n",
        "                # logger.warning(f\"Error listing topics via CLI: {result.stderr or result.stdout}\") # Assuming logger is available\n",
        "                existing_topics = set() # Reset if CLI also failed\n",
        "\n",
        "        topics_to_create = [topic for topic in required_topics if topic not in existing_topics and topic and topic.strip()]\n",
        "\n",
        "        if topics_to_create:\n",
        "            # logger.info(f\"Creating missing topics: {topics_to_create}\") # Assuming logger is available\n",
        "            created_list = []\n",
        "            try:\n",
        "                admin_client = KafkaAdminClient(bootstrap_servers=KAFKA_BROKERS)\n",
        "                new_topics_obj = [NewTopic(name=topic, num_partitions=3, replication_factor=1) for topic in topics_to_create]\n",
        "                admin_client.create_topics(new_topics=new_topics_obj, validate_only=False)\n",
        "                admin_client.close()\n",
        "                # logger.info(\"Topics created successfully via Admin API\") # Assuming logger is available\n",
        "                created_list.extend(topics_to_create) # Assume all succeeded if no exception\n",
        "                return created_list # Return immediately if API worked\n",
        "            except Exception as e:\n",
        "                # logger.warning(f\"Error creating topics via Admin API: {str(e)}. Falling back to CLI.\") # Assuming logger is available\n",
        "                # Fallback to CLI\n",
        "                if not KAFKA_HOME:\n",
        "                     # logger.error(\"Cannot fallback to CLI: KAFKA_HOME not set.\") # Assuming logger is available\n",
        "                     return [] # Cannot proceed\n",
        "\n",
        "                successfully_created_cli = []\n",
        "                for topic in topics_to_create:\n",
        "                    cmd = f\"{KAFKA_HOME}/bin/kafka-topics.sh --create --topic {topic} \" \\\n",
        "                          f\"--bootstrap-server {KAFKA_BROKERS} --partitions 3 --replication-factor 1\"\n",
        "                    result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=10)\n",
        "                    if result.returncode == 0 or \"already exists\" in (result.stderr + result.stdout).lower():\n",
        "                        # logger.info(f\"Topic {topic} created or already exists (via CLI)\") # Assuming logger is available\n",
        "                        successfully_created_cli.append(topic)\n",
        "                    else:\n",
        "                         # logger.error(f\"Failed to create topic {topic} via CLI: {result.stderr or result.stdout}\") # Assuming logger is available\n",
        "                         pass # Continue trying others\n",
        "                return successfully_created_cli\n",
        "        else:\n",
        "            # logger.info(\"All required topics already exist\") # Assuming logger is available\n",
        "            return []\n",
        "    except Exception as e:\n",
        "        # logger.error(f\"General error verifying/creating Kafka topics: {str(e)}\") # Assuming logger is available\n",
        "        return [] # Return empty list on major failure\n",
        "\n",
        "kafka_connected = check_kafka_connection()\n",
        "created_topics = []\n",
        "if not kafka_connected:\n",
        "    print(\"\\n‚ö†Ô∏è WARNING: Kafka connection failed. Please check if Kafka is running.\")\n",
        "    print(\"  Consider checking Zookeeper: bin/zookeeper-server-start.sh config/zookeeper.properties\")\n",
        "    print(\"  Consider checking Kafka Broker: bin/kafka-server-start.sh config/server.properties\")\n",
        "else:\n",
        "    created_topics = verify_create_kafka_topics()\n",
        "\n",
        "print(\"\\n=== Kafka Environment Verification Results ===\")\n",
        "print(f\"Kafka Connection: {'‚úÖ Success' if kafka_connected else '‚ùå Failed'}\")\n",
        "print(f\"Topics Created Now: {', '.join(created_topics) if created_topics else 'None (all existed or creation failed)'}\")"
      ],
      "metadata": {
        "id": "jB59fKbtOJVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Processing Pipeline Overview**\n",
        "\n",
        "## üìå **Import Dependencies**\n",
        "The following libraries are imported:\n",
        "- **Data Manipulation**: `pandas`, `pyspark.sql.functions`, `pyspark.sql.types`\n",
        "- **Visualization**: `matplotlib`, `seaborn`\n",
        "- **Automated EDA (Optional)**: `autoviz`\n",
        "- **System Utilities**: `os`\n",
        "\n",
        "---\n",
        "\n",
        "## üì• **Load Data (`load_and_inspect_data`)**\n",
        "- Reads the **consumer complaints dataset** from `DATASET_PATH` into a **Spark DataFrame** using **inferred schema**.\n",
        "- Displays:\n",
        "  - **Schema**\n",
        "  - **Sample rows**\n",
        "  - **Basic descriptive statistics**\n",
        "\n",
        "---\n",
        "\n",
        "## üîç **Exploratory Data Analysis (`perform_eda`)**\n",
        "1. **Sampling & Conversion**  \n",
        "   - Extracts a **sample** from the Spark DataFrame and converts it to a Pandas DataFrame.\n",
        "\n",
        "2. **Basic EDA**\n",
        "   - Computes:\n",
        "     - **Dimensions (rows & columns)**\n",
        "     - **Data types**\n",
        "     - **Missing values analysis**\n",
        "\n",
        "3. **Data Visualization**\n",
        "   - Generates and saves:\n",
        "     - **Product Distribution** plot\n",
        "     - **Company Response Distribution** plot  \n",
        "   - Uses **Matplotlib** & **Seaborn** for visualizations.\n",
        "\n",
        "4. **(Optional) Automated EDA**\n",
        "   - Runs **AutoViz** for **comprehensive visualizations** (if applicable).\n",
        "\n",
        "---\n",
        "\n",
        "## üõ† **Prepare Data for Kafka (`prepare_data_for_kafka`)**\n",
        "- Filters the **original Spark DataFrame** to retain only:\n",
        "  - Non-empty **Consumer complaint narratives**\n",
        "  - Non-null **Complaint IDs**\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ **Write to Kafka (`write_to_kafka`)**\n",
        "- Transforms the **filtered DataFrame** into **key-value pairs**:\n",
        "  - **Key**: `Complaint ID`\n",
        "  - **Value**: Entire row **as a JSON string**\n",
        "- **Writes records in batch** to **Kafka Topic (`KAFKA_TOPIC_RAW`)** using **Spark's Kafka Sink**.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ **Execution & Error Handling**\n",
        "- The **main execution block**:\n",
        "  1. Calls all functions **sequentially**.\n",
        "  2. Uses a **try-except block**:\n",
        "     - ‚úÖ Prints **success messages** if all steps run smoothly.\n",
        "     - ‚ùå Catches and **logs errors** if any occur.\n"
      ],
      "metadata": {
        "id": "oVBIG8vrQXWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pyspark.sql.functions import col, to_json, struct, lit, when, isnull\n",
        "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "# Assuming spark, logger, and constants (DATASET_PATH, RANDOM_SEED, DATA_DIR, VIZ_DIR,\n",
        "# KAFKA_TOPIC_RAW, KAFKA_BROKERS) are defined elsewhere\n",
        "\n",
        "def load_and_inspect_data():\n",
        "    \"\"\"\n",
        "    Loads consumer complaints data from a CSV file into a Spark DataFrame and performs initial inspection.\n",
        "\n",
        "    What:\n",
        "        Reads a specified CSV file containing consumer complaints data, infers or applies a\n",
        "        schema, and provides basic insights into the loaded data structure and content.\n",
        "\n",
        "    Why:\n",
        "        To ingest the raw data into the Spark environment for subsequent processing and analysis.\n",
        "        Initial inspection helps verify successful loading, understand data types, and get a\n",
        "        preliminary sense of the data's characteristics (row count, sample records, basic stats).\n",
        "        Using `inferSchema` provides convenience, though defining an explicit schema (commented out\n",
        "        in the original but structure shown) is generally better for production performance and\n",
        "        data type reliability.\n",
        "\n",
        "    How:\n",
        "        1.  Logs the path of the dataset being loaded using the `logger`.\n",
        "        2.  Defines an optional explicit `StructType` schema (currently relies on `inferSchema`).\n",
        "            *Note: Using `inferSchema=true` can be slow for large files and might guess incorrect types.*\n",
        "        3.  Uses `spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(DATASET_PATH)`\n",
        "            to read the CSV file into a Spark DataFrame (`df`), assuming the file has headers.\n",
        "        4.  Performs basic inspection actions:\n",
        "            -   Logs the total number of rows loaded using `df.count()`.\n",
        "            -   Prints the DataFrame's schema using `df.printSchema()`.\n",
        "            -   Displays the first 5 rows of data without truncation using `df.show(5, truncate=False)`.\n",
        "            -   Calculates and displays basic descriptive statistics for numerical/timestamp columns\n",
        "                using `df.describe().show()`.\n",
        "        5.  Includes error handling: If any exception occurs during loading or inspection, it logs\n",
        "            the error and re-raises the exception.\n",
        "\n",
        "    Returns:\n",
        "        pyspark.sql.DataFrame: The loaded Spark DataFrame containing the consumer complaints data.\n",
        "\n",
        "    Raises:\n",
        "        Exception: Propagates exceptions encountered during file reading or initial inspection\n",
        "                   after logging the error.\n",
        "\n",
        "    Note:\n",
        "        Relies on the globally defined SparkSession `spark`, `DATASET_PATH` constant, and a\n",
        "        pre-configured `logger` object. The current implementation uses `inferSchema`.\n",
        "    \"\"\"\n",
        "    # logger.info(f\"Loading data from: {DATASET_PATH}\") # Assuming logger is available\n",
        "    # Optional explicit schema definition (adjust types as needed)\n",
        "    # schema = StructType([...]) # Example schema structure provided in original code\n",
        "    try:\n",
        "        df = spark.read.option(\"header\", \"true\") \\\n",
        "                       .option(\"inferSchema\", \"true\") \\\n",
        "                       .csv(DATASET_PATH)\n",
        "        row_count = df.count()\n",
        "        # logger.info(f\"DataFrame loaded successfully with {row_count} rows\") # Assuming logger is available\n",
        "        # logger.info(\"DataFrame schema:\") # Assuming logger is available\n",
        "        df.printSchema()\n",
        "        print(\"\\nSample data:\")\n",
        "        df.show(5, truncate=False)\n",
        "        print(\"\\nBasic statistics:\")\n",
        "        df.describe().show()\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        # logger.error(f\"Error loading data: {str(e)}\") # Assuming logger is available\n",
        "        raise\n",
        "\n",
        "def perform_eda(df):\n",
        "    \"\"\"\n",
        "    Performs basic Exploratory Data Analysis (EDA) on a sample of the input Spark DataFrame.\n",
        "\n",
        "    What:\n",
        "        Generates summary statistics, analyzes missing values, and visualizes distributions\n",
        "        of key categorical features from a sample of the consumer complaints data. Optionally\n",
        "        uses AutoViz for automated visualization generation.\n",
        "\n",
        "    Why:\n",
        "        To gain a deeper understanding of the data's characteristics, distributions, potential\n",
        "        quality issues (like missing values), and relationships between variables before\n",
        "        further processing or modeling. Sampling is used because EDA often involves libraries\n",
        "        (like Pandas, Matplotlib, Seaborn, AutoViz) that work more efficiently or exclusively\n",
        "        on in-memory data.\n",
        "\n",
        "    How:\n",
        "        1.  **Sampling:** Takes a sample of the Spark DataFrame (`df`). The sample size is\n",
        "            capped (e.g., 10,000 rows or the total count if smaller) to manage memory usage.\n",
        "            The sampled data is converted to a Pandas DataFrame (`sample_df`) using `.toPandas()`.\n",
        "            *Caution: `.toPandas()` collects all sampled data to the driver node.*\n",
        "        2.  **Save Sample (Optional):** Saves the Pandas sample to a CSV file, which might be\n",
        "            useful for external tools or AutoViz.\n",
        "        3.  **Basic Pandas EDA:** Prints dimensions, data types, and a summary of missing values\n",
        "            (count and percentage) for the sampled data.\n",
        "        4.  **Visualization Prep:** Creates a directory (`VIZ_DIR`) to store generated plots.\n",
        "        5.  **Manual Visualizations:**\n",
        "            -   Generates and saves a bar plot of the top 10 most frequent values in the 'Product' column.\n",
        "            -   Generates and saves a bar plot showing the distribution of 'Company response to consumer'.\n",
        "            (Uses Matplotlib and Seaborn).\n",
        "        6.  **Automated Visualization (Optional):**\n",
        "            -   Attempts to import `AutoViz_Class`.\n",
        "            -   If successful, instantiates `AutoViz_Class` and calls `AutoViz()` on the saved\n",
        "              sample CSV or the Pandas DataFrame to automatically generate various plots.\n",
        "            -   Catches potential errors during AutoViz execution (e.g., library not installed)\n",
        "              and logs a warning.\n",
        "        7.  Logs completion messages using the `logger`.\n",
        "\n",
        "    Args:\n",
        "        df (pyspark.sql.DataFrame): The Spark DataFrame containing the consumer complaints data.\n",
        "\n",
        "    Note:\n",
        "        Relies on global constants `RANDOM_SEED`, `DATA_DIR`, `VIZ_DIR`, and a pre-configured\n",
        "        `logger`. Requires Pandas, Matplotlib, Seaborn, and optionally AutoViz to be installed.\n",
        "        Converts a sample of Spark data to Pandas, which requires sufficient driver memory.\n",
        "        Visualizations are saved to the path specified by `VIZ_DIR`.\n",
        "    \"\"\"\n",
        "    # logger.info(\"Sampling data for EDA...\") # Assuming logger is available\n",
        "    total_count = df.count()\n",
        "    sample_size = min(10000, total_count)\n",
        "    sample_fraction = sample_size / total_count if total_count > 0 else 0\n",
        "    if sample_fraction > 0:\n",
        "        sample_df = df.sample(fraction=sample_fraction, seed=RANDOM_SEED).toPandas()\n",
        "    else:\n",
        "        sample_df = df.limit(0).toPandas() # Create empty pandas df with same schema if no data\n",
        "\n",
        "    # logger.info(f\"Sample data shape: {sample_df.shape}\") # Assuming logger is available\n",
        "    sample_csv_path = os.path.join(DATA_DIR, \"sample_complaints.csv\")\n",
        "    try:\n",
        "       os.makedirs(DATA_DIR, exist_ok=True)\n",
        "       sample_df.to_csv(sample_csv_path, index=False)\n",
        "    except Exception as e:\n",
        "       # logger.warning(f\"Could not save sample CSV to {sample_csv_path}: {e}\") # Assuming logger is available\n",
        "       pass\n",
        "\n",
        "    print(\"\\n=== Basic EDA ===\")\n",
        "    print(f\"Dataset dimensions (sampled): {sample_df.shape}\")\n",
        "    print(\"\\nData types (sampled):\")\n",
        "    print(sample_df.dtypes)\n",
        "    print(\"\\nMissing values by column (sampled):\")\n",
        "    if not sample_df.empty:\n",
        "        missing_values = sample_df.isnull().sum()\n",
        "        missing_pct = (missing_values / len(sample_df)) * 100\n",
        "        missing_df = pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_pct})\n",
        "        print(missing_df[missing_df['Missing Values'] > 0].sort_values('Percentage', ascending=False))\n",
        "    else:\n",
        "        print(\"Sample DataFrame is empty, skipping missing value analysis.\")\n",
        "\n",
        "    try:\n",
        "        os.makedirs(VIZ_DIR, exist_ok=True)\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        if 'Product' in sample_df.columns and not sample_df.empty:\n",
        "            product_counts = sample_df['Product'].value_counts().head(10)\n",
        "            sns.barplot(x=product_counts.values, y=product_counts.index)\n",
        "            plt.title('Top 10 Products (Sampled)')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(VIZ_DIR, 'top_products_sampled.png'))\n",
        "            plt.close() # Close plot to free memory\n",
        "        if 'Company response to consumer' in sample_df.columns and not sample_df.empty:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            response_counts = sample_df['Company response to consumer'].value_counts()\n",
        "            sns.barplot(x=response_counts.values, y=response_counts.index)\n",
        "            plt.title('Company Response Distribution (Sampled)')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(VIZ_DIR, 'response_distribution_sampled.png'))\n",
        "            plt.close() # Close plot\n",
        "    except Exception as plot_e:\n",
        "         # logger.warning(f\"Error during manual visualization: {plot_e}\") # Assuming logger is available\n",
        "         pass\n",
        "\n",
        "    try:\n",
        "        from autoviz.AutoViz_Class import AutoViz_Class\n",
        "        # logger.info(\"Running AutoViz for automated visualizations...\") # Assuming logger is available\n",
        "        av = AutoViz_Class()\n",
        "        # Using dfte=sample_df might be more robust if saving CSV failed\n",
        "        dft = av.AutoViz(filename=\"\", sep=\",\", depVar=\"\", dfte=sample_df, header=0, verbose=0,\n",
        "                         lowess=False, chart_format=\"png\", max_rows_analyzed=10000, max_cols_analyzed=30,\n",
        "                         save_plot_dir=VIZ_DIR) # Specify save directory\n",
        "        # logger.info(f\"AutoViz visualizations saved to: {VIZ_DIR}\") # Assuming logger is available\n",
        "    except ImportError:\n",
        "        # logger.warning(\"AutoViz not installed. Skipping automated visualizations.\") # Assuming logger is available\n",
        "        pass\n",
        "    except Exception as e:\n",
        "        # logger.warning(f\"AutoViz error: {str(e)}. Skipping automated visualizations.\") # Assuming logger is available\n",
        "        pass\n",
        "    # logger.info(\"EDA completed\") # Assuming logger is available\n",
        "\n",
        "def prepare_data_for_kafka(df):\n",
        "    \"\"\"\n",
        "    Filters the Spark DataFrame to select relevant records for Kafka ingestion.\n",
        "\n",
        "    What:\n",
        "        Applies filtering logic to the input DataFrame, primarily keeping records\n",
        "        that have non-empty complaint narratives and valid Complaint IDs.\n",
        "\n",
        "    Why:\n",
        "        To ensure that only meaningful data (complaints with actual text) is sent\n",
        "        to the Kafka topic intended for raw data processing. Filtering out records\n",
        "        without narratives reduces noise and processing overhead downstream. Ensuring\n",
        "        a non-null 'Complaint ID' is crucial if it's used as the Kafka message key\n",
        "        for partitioning or identification.\n",
        "\n",
        "    How:\n",
        "        1.  Logs the start of the preparation phase.\n",
        "        2.  Defines the column name for the complaint narrative.\n",
        "        3.  Filters the DataFrame `df` using `df.filter()` to keep rows where the\n",
        "            narrative column is not null AND is not an empty string.\n",
        "        4.  Logs the number of rows before and after this narrative filtering.\n",
        "        5.  Applies a second filter to ensure the 'Complaint ID' column is not null.\n",
        "        6.  Logs the final count of rows after the ID filter.\n",
        "        7.  Returns the resulting filtered Spark DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pyspark.sql.DataFrame): The original Spark DataFrame loaded from the source.\n",
        "\n",
        "    Returns:\n",
        "        pyspark.sql.DataFrame: A filtered Spark DataFrame containing records suitable\n",
        "                               for sending to the raw Kafka topic.\n",
        "\n",
        "    Note:\n",
        "        Relies on the input DataFrame having columns named 'Consumer complaint narrative'\n",
        "        and 'Complaint ID'. Depends on a pre-configured `logger`.\n",
        "    \"\"\"\n",
        "    # logger.info(\"Preparing data for Kafka...\") # Assuming logger is available\n",
        "    narrative_col = \"Consumer complaint narrative\"\n",
        "    complaint_id_col = \"Complaint ID\"\n",
        "    total_count = df.count()\n",
        "    # Filter for non-empty narrative\n",
        "    prepared_df = df.filter(\n",
        "        col(narrative_col).isNotNull() & (col(narrative_col) != \"\")\n",
        "    )\n",
        "    filtered_count = prepared_df.count()\n",
        "    # logger.info(f\"Filtered for non-empty narrative: {filtered_count} rows (from {total_count})\") # Assuming logger is available\n",
        "    # Filter for non-null Complaint ID\n",
        "    prepared_df = prepared_df.filter(col(complaint_id_col).isNotNull())\n",
        "    final_count = prepared_df.count()\n",
        "    if final_count < filtered_count:\n",
        "         # logger.info(f\"Filtered for non-null Complaint ID: {final_count} rows (from {filtered_count})\") # Assuming logger is available\n",
        "         pass\n",
        "    return prepared_df\n",
        "\n",
        "def write_to_kafka(df):\n",
        "    \"\"\"\n",
        "    Writes the prepared Spark DataFrame to a specified Kafka topic.\n",
        "\n",
        "    What:\n",
        "        Takes a Spark DataFrame, formats it appropriately for Kafka (key-value pairs\n",
        "        with JSON value), and writes it to the target Kafka topic using Spark's\n",
        "        built-in Kafka connector.\n",
        "\n",
        "    Why:\n",
        "        To publish the raw, filtered consumer complaints data onto the Kafka message\n",
        "        bus, making it available for downstream consumers (like streaming applications\n",
        "        or batch processing jobs) to process further. Using 'Complaint ID' as the key\n",
        "        helps with partitioning and potentially idempotent processing downstream.\n",
        "\n",
        "    How:\n",
        "        1.  Logs the target Kafka topic name.\n",
        "        2.  Selects columns from the input DataFrame `df` and transforms them:\n",
        "            -   Casts the 'Complaint ID' column to string and aliases it as `key`.\n",
        "            -   Uses `struct(\"*\")` to gather all columns into a struct.\n",
        "            -   Uses `to_json()` to serialize the struct into a JSON string, aliased as `value`.\n",
        "            The result is a DataFrame (`kafka_df`) with 'key' and 'value' columns.\n",
        "        3.  Initiates a write operation using `kafka_df.write`.\n",
        "        4.  Specifies the format as \"kafka\".\n",
        "        5.  Provides the Kafka broker list via `option(\"kafka.bootstrap.servers\", KAFKA_BROKERS)`.\n",
        "        6.  Specifies the target topic via `option(\"topic\", KAFKA_TOPIC_RAW)`.\n",
        "        7.  Executes the write operation using `.save()`. *Note: `.save()` is typically for batch writes.*\n",
        "            For continuous streaming writes, `.start()` would be used with `.writeStream`.\n",
        "        8.  Logs success upon completion or logs an error and re-raises the exception if writing fails.\n",
        "\n",
        "    Args:\n",
        "        df (pyspark.sql.DataFrame): The prepared Spark DataFrame ready for Kafka.\n",
        "\n",
        "    Raises:\n",
        "        Exception: Propagates exceptions encountered during the Kafka write operation\n",
        "                   after logging the error.\n",
        "\n",
        "    Note:\n",
        "        Relies on global constants `KAFKA_BROKERS`, `KAFKA_TOPIC_RAW`, and a pre-configured\n",
        "        `logger`. Assumes the Spark session has the necessary Kafka connector JARs available\n",
        "        (usually specified during session creation). This function performs a batch write.\n",
        "    \"\"\"\n",
        "    # logger.info(f\"Writing data to Kafka topic: {KAFKA_TOPIC_RAW}\") # Assuming logger is available\n",
        "    try:\n",
        "        kafka_df = df.select(\n",
        "            col(\"Complaint ID\").cast(\"string\").alias(\"key\"),\n",
        "            to_json(struct(\"*\")).alias(\"value\")\n",
        "        )\n",
        "        kafka_df.write \\\n",
        "            .format(\"kafka\") \\\n",
        "            .option(\"kafka.bootstrap.servers\", KAFKA_BROKERS) \\\n",
        "            .option(\"topic\", KAFKA_TOPIC_RAW) \\\n",
        "            .save()\n",
        "        # logger.info(f\"Successfully wrote {kafka_df.count()} records to Kafka topic {KAFKA_TOPIC_RAW}\") # Assuming logger is available\n",
        "    except Exception as e:\n",
        "        # logger.error(f\"Error writing to Kafka: {str(e)}\") # Assuming logger is available\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    complaints_df = load_and_inspect_data()\n",
        "    perform_eda(complaints_df)\n",
        "    prepared_df = prepare_data_for_kafka(complaints_df)\n",
        "    write_to_kafka(prepared_df)\n",
        "    print(\"\\n‚úÖ Phase 2 completed successfully: Data ingested, explored, and loaded to Kafka\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error in Phase 2: {str(e)}\")\n",
        "    # Potentially re-raise e if you want the notebook cell to fail explicitly\n",
        "    # raise e"
      ],
      "metadata": {
        "id": "i0hIybXqOM-U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}