{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcbDKpJnqrPoKT8cJkWjZR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShovalBenjer/Bigdata_Pyspark_Apache/blob/main/Streaming_options.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Queue Stream (RDDs)\n",
        "In this case, we simulate a queue of sentences and process each sentence in batches. For each batch, we need to:\n",
        "\n",
        "Identify the longest word in the batch.\n",
        "Identify the most frequently used word in the batch."
      ],
      "metadata": {
        "id": "4lWRf7Xti-dM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2Dgl6sAZi3NR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "82a36fd1-c02d-4005-9765-f91fefc25a83"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o644.start.\n: java.lang.IllegalStateException: Only one StreamingContext may be started in this JVM. Currently running StreamingContext was started atorg.apache.spark.streaming.api.java.JavaStreamingContext.start(JavaStreamingContext.scala:563)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:566)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n\tat org.apache.spark.streaming.StreamingContext$.org$apache$spark$streaming$StreamingContext$$assertNoOtherContextIsActive(StreamingContext.scala:776)\n\tat org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:582)\n\tat org.apache.spark.streaming.api.java.JavaStreamingContext.start(JavaStreamingContext.scala:563)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-0d0c33b75a33>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Start the computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/streaming/context.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mStart\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mexecution\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mstreams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \"\"\"\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0mStreamingContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activeContext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o644.start.\n: java.lang.IllegalStateException: Only one StreamingContext may be started in this JVM. Currently running StreamingContext was started atorg.apache.spark.streaming.api.java.JavaStreamingContext.start(JavaStreamingContext.scala:563)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:566)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n\tat org.apache.spark.streaming.StreamingContext$.org$apache$spark$streaming$StreamingContext$$assertNoOtherContextIsActive(StreamingContext.scala:776)\n\tat org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:582)\n\tat org.apache.spark.streaming.api.java.JavaStreamingContext.start(JavaStreamingContext.scala:563)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "if 'sc' in globals():\n",
        "    sc.stop()\n",
        "\n",
        "\n",
        "# Initialize SparkContext and StreamingContext\n",
        "sc = SparkContext(\"local[2]\", \"QueueStream\")\n",
        "ssc = StreamingContext(sc, 5)  # 5 seconds batch interval\n",
        "\n",
        "# Define a queue of RDDs containing sentences\n",
        "sentences = [\n",
        "    \"The Walt Disney Studios has acquired the worldwide distribution rights to acclaimed filmmaker Peter Jackson’s previously announced Beatles documentary.\",\n",
        "    \"The film will showcase the warmth, camaraderie and humor of the making of the legendary band’s studio album, 'Let It Be,' and their final live concert as a group, the iconic rooftop performance on London’s Savile Row.\",\n",
        "    \"'The Beatles: Get Back' will be released by The Walt Disney Studios in the United States and Canada on September 4, 2020, with additional details and dates for the film’s global release to follow.\",\n",
        "    \"The power of rock and roll is a constantly amazing process of this group.\",\n",
        "    \"Although it is Bob Dylan who is the single most important figure in rock and roll; and although it is the Rolling Stones who are the embodiment of a rock and roll band; it is nonetheless Our Boys.\",\n",
        "    \"The Beatles, who are the perfect product and result of everything that rock and roll means and encompasses.\"\n",
        "]\n",
        "\n",
        "# Create an RDD queue stream\n",
        "rdd_queue = [ssc.sparkContext.parallelize(sentence.split()) for sentence in sentences]\n",
        "queue_stream = ssc.queueStream(rdd_queue)\n",
        "\n",
        "# Process each batch of words in the stream\n",
        "def process_batch(rdd):\n",
        "    # Find the longest word\n",
        "    longest_word = rdd.reduce(lambda a, b: a if len(a) > len(b) else b)\n",
        "\n",
        "    # Find the most frequent word\n",
        "    word_counts = rdd.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
        "    most_used_word = word_counts.transform(lambda rdd: rdd.sortBy(lambda x: -x[1]).take(1))\n",
        "\n",
        "    # Output the results\n",
        "    print(f\"Longest Word in Batch: {longest_word}\")\n",
        "    print(f\"Most Used Word in Batch: {most_used_word[0][0]} (Count: {most_used_word[0][1]})\")\n",
        "\n",
        "# Apply the processing function to the stream\n",
        "queue_stream.foreachRDD(process_batch)\n",
        "\n",
        "# Start the computation\n",
        "ssc.start()\n",
        "ssc.awaitTermination()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Socket Stream\n",
        "A socket stream allows us to receive data in real-time, e.g., from a network socket. The task is to process batches of words arriving every 5 seconds and track the most used word, including words seen in previous batches."
      ],
      "metadata": {
        "id": "ysjiFGdSju3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "# Initialize SparkContext and StreamingContext\n",
        "sc = SparkContext(\"local[2]\", \"SocketStream\")\n",
        "ssc = StreamingContext(sc, 5)  # 5 seconds batch interval\n",
        "\n",
        "# Create a socket stream that listens on port 9999\n",
        "socket_stream = ssc.socketTextStream(\"localhost\", 9999)\n",
        "\n",
        "# Function to update and display the most used word\n",
        "def update_word_counts(new_words, last_counts):\n",
        "    return new_words.updateStateByKey(lambda new, last: sum(new) + (last or 0))\n",
        "\n",
        "# Split the incoming text into words and map to (word, 1) pairs\n",
        "words = socket_stream.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1))\n",
        "\n",
        "# Update the word counts over time\n",
        "word_counts = words.reduceByKeyAndWindow(lambda a, b: a + b, windowDuration=15, slideDuration=5)\n",
        "word_counts = word_counts.updateStateByKey(update_word_counts)\n",
        "\n",
        "# Display the most used word in the stream\n",
        "def display_most_used_word(rdd):\n",
        "    if not rdd.isEmpty():\n",
        "        most_used_word = rdd.takeOrdered(1, key=lambda x: -x[1])\n",
        "        print(f\"Most Used Word: {most_used_word[0][0]} (Count: {most_used_word[0][1]})\")\n",
        "\n",
        "# Apply the display function\n",
        "word_counts.foreachRDD(display_most_used_word)\n",
        "\n",
        "# Start the computation\n",
        "ssc.start()\n",
        "ssc.awaitTermination()\n"
      ],
      "metadata": {
        "id": "WcKk7yVRi9x-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "File Stream\n",
        "This stream will watch a directory for newly created files and, for each batch, display the total number of lines in the newly created files.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZxJh0H0NkU6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "# Initialize SparkContext and StreamingContext\n",
        "sc = SparkContext(\"local[2]\", \"FileStream\")\n",
        "ssc = StreamingContext(sc, 5)  # 5 seconds batch interval\n",
        "\n",
        "# Monitor a directory for new files\n",
        "directory_stream = ssc.textFileStream(\"path_to_directory\")\n",
        "\n",
        "# Function to count the number of lines in a file\n",
        "def count_lines_in_file(rdd):\n",
        "    if not rdd.isEmpty():\n",
        "        total_lines = rdd.count()  # Count number of lines in the batch\n",
        "        print(f\"Total lines in batch: {total_lines}\")\n",
        "\n",
        "# Apply the line counting function to the stream\n",
        "directory_stream.foreachRDD(count_lines_in_file)\n",
        "\n",
        "# Start the computation\n",
        "ssc.start()\n",
        "ssc.awaitTermination()\n"
      ],
      "metadata": {
        "id": "K0M9Ir7Dkbjv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}