{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgcLupUsEn1ry/1B0twIF9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShovalBenjer/Bigdata_Pyspark_Spark_Hadoop_Apache/blob/ShovalBenjer-patch-1/integral_approximation_with_spark_1_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Notebook explores distributed numerical integration using Apache Spark integrated into google colabs jupyter notebooks, focusing on approximating the integral of a specific f(x) over a specified range. By varying the number of intervals n and Spark workers, the project evaluates the trade-offs between computation accuracy, execution time, and scalability in a distributed environment. The results demonstrate how increasing n improves precision while parallelism enhances performance, highlighting the efficiency and limitations of distributed systems for computational tasks."
      ],
      "metadata": {
        "id": "HGBKg28dhhR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup = Imports and intiallize**"
      ],
      "metadata": {
        "id": "0_nXK29_0Jhz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zeY0wLmpahWe"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "import time\n",
        "import pandas as pd\n",
        "from sympy import symbols, integrate\n",
        "from pyspark import SparkContext\n",
        "\n",
        "if SparkContext._active_spark_context:\n",
        "    sc = SparkContext.getOrCreate()\n",
        "else:\n",
        "    sc = SparkContext(\"local\", \"Integral Approximation\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "    \"\"\"Function to calculate f(x) = 10x^2 - 2. Change the function for different approximation.\"\"\"\n",
        "    return 10 * x**2 - 2\n",
        "\n",
        "def exact_integral_sympy(a, b):\n",
        "    \"\"\"\n",
        "    Calculate the exact integral of f(x) using SymPy.\n",
        "    Args:\n",
        "        a (float): Lower bound of the integral.\n",
        "        b (float): Upper bound of the integral.\n",
        "    Returns:\n",
        "        float: Exact integral value.\n",
        "    \"\"\"\n",
        "    x = symbols('x')\n",
        "    exact_integral = integrate(f(x), (x, a, b))\n",
        "    return float(exact_integral)\n",
        "\n",
        "\n",
        "def calculate_integral(a, b, n, num_workers):\n",
        "    \"\"\"\n",
        "    Calculate the integral approximation using Spark RDDs.\n",
        "    Args:\n",
        "        h (float): Step size for the integral approximation.\n",
        "        a (float): Lower bound of the integral.\n",
        "        b (float): Upper bound of the integral.\n",
        "        n (int): Number of intervals.\n",
        "        map(f): transformation that applies a function to each element of an RDD, producing a new RDD with the transformed elements.\n",
        "        num_workers (int): Number of Spark partitions (workers).\n",
        "        x_values (list): List of x values for the integral approximation.\n",
        "        rdd (pyspark.RDD): Spark RDD for the integral approximation.\n",
        "    Returns:\n",
        "        float: Approximated integral value.\n",
        "    \"\"\"\n",
        "    h = (b - a) / n\n",
        "    x_values = [a + k * h for k in range(n + 1)]\n",
        "    rdd = sc.parallelize(x_values, num_workers)\n",
        "    integral_sum = rdd.map(f).reduce(lambda x, y: x + y)\n",
        "    result = h * ((f(a) + f(b)) / 2 + integral_sum - f(a) - f(b))\n",
        "    return result\n",
        "\n",
        "def run_experiments():\n",
        "    \"\"\"\n",
        "    Run the integral approximation for various configurations of workers and intervals.\n",
        "    The function performs the following steps:\n",
        "    1. Sets the bounds of the integral (a = 1, b = 20).\n",
        "    2. Defines the number of intervals (n_values) for approximation: [100, 1000, 10000].\n",
        "    3. Defines the number of workers (worker_counts) for parallelism: [2, 4].\n",
        "    4. Iterates through combinations of intervals and worker counts to:\n",
        "        - Approximate the integral using Spark RDDs.\n",
        "        - Measure the execution time for the computation.\n",
        "        - Compare the approximate integral with the exact integral calculated using SymPy.\n",
        "        - Compute the error between the approximate and exact integral values.\n",
        "    5. Stores the results (n, number of workers, error, execution time) in a list.\n",
        "    6. Converts the results into a Pandas DataFrame for analysis.\n",
        "    Returns:\n",
        "        pd.DataFrame: Results as a DataFrame with columns:\n",
        "            - \"n\": Number of intervals used in the approximation.\n",
        "            - \"Number of Workers\": Number of workers (parallelism) used in Spark.\n",
        "            - \"Error\": Absolute error between the approximate and exact integral.\n",
        "            - \"Execution Time (s)\": Time taken to compute the integral.\n",
        "    \"\"\"\n",
        "    a, b = 1, 20\n",
        "    n_values = [100, 1000, 10000]\n",
        "    worker_counts = [2, 4]\n",
        "    results = []\n",
        "\n",
        "    for num_workers in worker_counts:\n",
        "        for n in n_values:\n",
        "            start_time = time.time()\n",
        "            integral_value = calculate_integral(a, b, n, num_workers)\n",
        "            execution_time = time.time() - start_time\n",
        "            expected_value = exact_integral_sympy(a, b)\n",
        "            error = abs(integral_value - expected_value)\n",
        "            results.append((n, num_workers, error, execution_time))\n",
        "    df = pd.DataFrame(results, columns=[\"n\", \"Number of Workers\", \"Error\", \"Execution Time (s)\"])\n",
        "    return df"
      ],
      "metadata": {
        "id": "WZKiOU1tfGkA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = run_experiments()\n",
        "results_sorted = results_df.sort_values(by=[\"Number of Workers\", \"n\"])\n",
        "print(results_sorted)\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "LrretjOffLSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| \\( n \\)   | Number of Workers | Error       | Execution Time (s) | Explanation                                                                 |\n",
        "|-----------|-------------------|-------------|---------------------|-----------------------------------------------------------------------------|\n",
        "| 100       | 2                 | 1.143167    | 3.728478            | High error due to fewer intervals (\\( n \\)) leading to less precise results, longer time due to limited parallelism. |\n",
        "| 1000      | 2                 | 0.011432    | 0.910908            | Error significantly reduced with more intervals, faster execution due to better load distribution. |\n",
        "| 10000     | 2                 | 0.000114    | 0.675122            | Very low error with higher intervals, faster execution due to distributed computation. |\n",
        "| 100       | 4                 | 1.143167    | 1.627744            | High error persists, but adding workers reduces execution time significantly. |\n",
        "| 1000      | 4                 | 0.011432    | 1.029233            | Lower error, but slight increase in execution time due to distributed overhead. |\n",
        "| 10000     | 4                 | 0.000114    | 0.980111            | Minimal error, but execution time slightly increases due to coordination overhead between workers. |\n",
        "\n",
        "### Overall Comparison\n",
        "- **Error**:\n",
        "  - The error depends only on the number of intervals (\\( n \\)) and is identical for 2 and 4 workers configurations at each interval.\n",
        "  - Larger intervals (\\( n = 10000 \\)) achieve near-perfect accuracy (\\( 0.000114 \\)).\n",
        "\n",
        "- **Execution Time**:\n",
        "  - For smaller intervals (\\( n = 100 \\)), increasing the number of workers significantly reduces execution time.\n",
        "  - For larger intervals (\\( n = 1000 \\) and \\( n = 10000 \\)), adding more workers slightly increases execution time due to coordination overhead.\n",
        "\n",
        "Expected Behavior with n:\n",
        "Small n: High execution time due to overhead dominating computation.\n",
        "Moderate n: Efficient execution as workers are better utilized, leading to reduced execution time.\n",
        "Large n: Gradual increase in execution time as computation grows linearly and distributed system limitations (e.g., memory or bandwidth) are reached.\n",
        "\n",
        "**Optimal results are achieved with n=10000 and 2 workers for a balance of accuracy and efficiency.**\n",
        "\n"
      ],
      "metadata": {
        "id": "UBw6zfBEgwQW"
      }
    }
  ]
}
