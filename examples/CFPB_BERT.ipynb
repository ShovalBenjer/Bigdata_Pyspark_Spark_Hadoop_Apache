{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShovalBenjer/Bigdata_Pyspark_Spark_Hadoop_Apache/blob/main/CFPB_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install kafka-python transformers torch autoviz mlflow pyspark==3.5.5 findspark pyarrow pandas pyyaml ipython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIB9jJFurO2T",
        "outputId": "aeff1692-6b60-47ea-f546-d5b72958fbef"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kafka-python\n",
            "  Downloading kafka_python-2.1.3-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting autoviz\n",
            "  Downloading autoviz-0.1.905-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting mlflow\n",
            "  Downloading mlflow-2.21.2-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: pyspark==3.5.5 in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (18.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (7.34.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark==3.5.5) (0.10.9.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.11/dist-packages (from autoviz) (2.0.1)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (from autoviz) (1.9.4)\n",
            "Collecting emoji (from autoviz)\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting pyamg (from autoviz)\n",
            "  Downloading pyamg-5.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from autoviz) (1.6.1)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from autoviz) (0.14.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from autoviz) (3.9.1)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (from autoviz) (0.19.0)\n",
            "Collecting xgboost<1.7,>=0.82 (from autoviz)\n",
            "  Downloading xgboost-1.6.2-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting pandas-dq>=1.29 (from autoviz)\n",
            "  Downloading pandas_dq-1.29-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting hvplot>=0.9.2 (from autoviz)\n",
            "  Downloading hvplot-0.11.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: holoviews>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from autoviz) (1.20.2)\n",
            "Requirement already satisfied: panel>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from autoviz) (1.6.1)\n",
            "Requirement already satisfied: matplotlib>3.7.4 in /usr/local/lib/python3.11/dist-packages (from autoviz) (3.10.0)\n",
            "Requirement already satisfied: seaborn>0.12.2 in /usr/local/lib/python3.11/dist-packages (from autoviz) (0.13.2)\n",
            "Collecting mlflow-skinny==2.21.2 (from mlflow)\n",
            "  Downloading mlflow_skinny-2.21.2-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.0)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
            "  Downloading alembic-1.15.1-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.7)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.14.1)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.39)\n",
            "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (8.1.8)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (3.1.1)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.21.2->mlflow)\n",
            "  Downloading databricks_sdk-0.48.0-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting fastapi<1 (from mlflow-skinny==2.21.2->mlflow)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (3.1.44)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (8.6.1)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (1.31.1)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (1.31.1)\n",
            "Requirement already satisfied: protobuf<6,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (5.29.4)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (2.10.6)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (0.5.3)\n",
            "Collecting uvicorn<1 (from mlflow-skinny==2.21.2->mlflow)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython) (4.9.0)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.1.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.3.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: bokeh>=3.1 in /usr/local/lib/python3.11/dist-packages (from holoviews>=1.16.0->autoviz) (3.6.3)\n",
            "Requirement already satisfied: colorcet in /usr/local/lib/python3.11/dist-packages (from holoviews>=1.16.0->autoviz) (3.1.0)\n",
            "Requirement already satisfied: param<3.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from holoviews>=1.16.0->autoviz) (2.2.0)\n",
            "Requirement already satisfied: pyviz-comms>=2.1 in /usr/local/lib/python3.11/dist-packages (from holoviews>=1.16.0->autoviz) (3.0.4)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython) (0.8.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>3.7.4->autoviz) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>3.7.4->autoviz) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>3.7.4->autoviz) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>3.7.4->autoviz) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>3.7.4->autoviz) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>3.7.4->autoviz) (3.2.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from panel>=1.4.0->autoviz) (6.2.0)\n",
            "Requirement already satisfied: linkify-it-py in /usr/local/lib/python3.11/dist-packages (from panel>=1.4.0->autoviz) (2.0.3)\n",
            "Requirement already satisfied: markdown-it-py in /usr/local/lib/python3.11/dist-packages (from panel>=1.4.0->autoviz) (3.0.0)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.11/dist-packages (from panel>=1.4.0->autoviz) (0.4.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->autoviz) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->autoviz) (3.6.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->autoviz) (1.0.1)\n",
            "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.1->holoviews>=1.16.0->autoviz) (6.4.2)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.1->holoviews>=1.16.0->autoviz) (2025.1.0)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.2->mlflow) (2.38.0)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi<1->mlflow-skinny==2.21.2->mlflow)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.21.2->mlflow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.21.2->mlflow) (3.21.0)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.21.2->mlflow) (1.2.18)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.21.2->mlflow) (0.52b1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.21.2->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.21.2->mlflow) (2.27.2)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn<1->mlflow-skinny==2.21.2->mlflow) (0.14.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->panel>=1.4.0->autoviz) (0.5.1)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.11/dist-packages (from linkify-it-py->panel>=1.4.0->autoviz) (1.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py->panel>=1.4.0->autoviz) (0.1.2)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.21.2->mlflow) (1.17.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.21.2->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.2->mlflow) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.2->mlflow) (4.9)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==2.21.2->mlflow) (4.9.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==2.21.2->mlflow) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.2->mlflow) (0.6.1)\n",
            "Downloading kafka_python-2.1.3-py2.py3-none-any.whl (276 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.1/276.1 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m114.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoviz-0.1.905-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.5/67.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow-2.21.2-py3-none-any.whl (28.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.2/28.2 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-2.21.2-py3-none-any.whl (6.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m111.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Downloading alembic-1.15.1-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.8/231.8 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hvplot-0.11.2-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.9/161.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas_dq-1.29-py3-none-any.whl (29 kB)\n",
            "Downloading xgboost-1.6.2-py3-none-manylinux2014_x86_64.whl (255.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.9/255.9 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyamg-5.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.48.0-py3-none-any.whl (677 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m677.6/677.6 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kafka-python, findspark, uvicorn, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jedi, gunicorn, graphql-core, emoji, xgboost, starlette, pyamg, nvidia-cusparse-cu12, nvidia-cudnn-cu12, graphql-relay, docker, alembic, pandas-dq, nvidia-cusolver-cu12, graphene, fastapi, databricks-sdk, mlflow-skinny, mlflow, hvplot, autoviz\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 2.1.4\n",
            "    Uninstalling xgboost-2.1.4:\n",
            "      Successfully uninstalled xgboost-2.1.4\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed alembic-1.15.1 autoviz-0.1.905 databricks-sdk-0.48.0 docker-7.1.0 emoji-2.14.1 fastapi-0.115.12 findspark-2.0.1 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 hvplot-0.11.2 jedi-0.19.2 kafka-python-2.1.3 mlflow-2.21.2 mlflow-skinny-2.21.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pandas-dq-1.29 pyamg-5.2.1 starlette-0.46.1 uvicorn-0.34.0 xgboost-1.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# Comprehensive Big Data Pipeline with Spark Structured Streaming, Kafka\n",
        "# Designed for Google Colab Pro with External Kafka Setup\n",
        "# VERSION incorporating fixes for Indentation, Task 2 Filter, Task 4 GPU/Except, Task 5 TypeError, AutoViz Inline, Superset Memory Sink\n",
        "\n",
        "print(\"--- Initializing Pipeline Script ---\")\n",
        "print(\"Ensure Kafka/Zookeeper are running externally and topics are created.\")\n",
        "print(\"Ensure Consumer_Complaints.csv is available at /content/Consumer_Complaints.csv\")\n",
        "print(\"Ensure required packages are installed (run pip install cell).\")\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "import mlflow\n",
        "import findspark\n",
        "import contextlib\n",
        "from typing import Iterator\n",
        "import traceback # Import traceback\n",
        "\n",
        "# --- Spark Configuration ---\n",
        "# Set Spark environment variables\n",
        "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
        "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
        "findspark.init() # Finds Spark installation\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "from pyspark.ml.feature import (\n",
        "    StringIndexer, OneHotEncoder, VectorAssembler, SQLTransformer # Import SQLTransformer\n",
        ")\n",
        "from pyspark.ml.base import Transformer\n",
        "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
        "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable, MLWriter, MLReader, DefaultParamsWriter, DefaultParamsReader\n",
        "from pyspark.ml.torch.distributor import TorchDistributor\n",
        "from pyspark.sql.streaming import StreamingQueryListener\n",
        "from kafka import KafkaProducer, errors as kafka_errors\n",
        "\n",
        "# --- Transformers & PyTorch ---\n",
        "from transformers import DistilBertTokenizer, DistilBertModel, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, DistributedSampler\n",
        "import torch.nn as nn\n",
        "import torch.distributed as dist\n",
        "from collections import OrderedDict\n",
        "\n",
        "# --- Visualization ---\n",
        "try:\n",
        "    from autoviz import AutoViz_Class\n",
        "    import matplotlib.pyplot as plt\n",
        "    AUTOVIZ_AVAILABLE = True\n",
        "except ImportError:\n",
        "    AUTOVIZ_AVAILABLE = False\n",
        "    print(\"WARN: autoviz or matplotlib not found. Visualization will be skipped.\")\n",
        "\n",
        "# --- Configuration Variables ---\n",
        "BASE_DIR = \"/content/consumer_complaints\"\n",
        "CSV_FILE_PATH = \"/content/Consumer_Complaints.csv\"\n",
        "TEST_DATA_PERSISTENCE_PATH = f\"{BASE_DIR}/data/test_data_source.parquet\"\n",
        "TRAINING_PIPELINE_SAVE_PATH = f\"{BASE_DIR}/models/training_pipeline\"\n",
        "EMBEDDING_MODEL_SAVE_PATH = f\"{BASE_DIR}/models/embedding_model\"\n",
        "STREAMING_CHECKPOINT_LOCATION = f\"{BASE_DIR}/checkpoints\"\n",
        "MLFLOW_TRACKING_URI = f\"file://{BASE_DIR}/mlflow\"\n",
        "VISUALIZATION_DIR = f\"{BASE_DIR}/visualizations\"\n",
        "TRAIN_PARQUET_PATH = f\"{BASE_DIR}/data/train_data.parquet\"\n",
        "VAL_PARQUET_PATH = f\"{BASE_DIR}/data/val_data.parquet\"\n",
        "PREDICTIONS_MEM_TABLE = \"predictions_mem_table\" # For Superset demo\n",
        "\n",
        "# Kafka configuration (assuming external setup on localhost)\n",
        "KAFKA_BROKERS = \"localhost:9092\"\n",
        "KAFKA_TOPIC_RAW = \"complaints-raw\"\n",
        "KAFKA_TOPIC_TRAINING = \"complaints-training-data\"\n",
        "KAFKA_TOPIC_TESTING_STREAM = \"complaints-testing-stream\"\n",
        "KAFKA_TOPIC_PREDICTIONS = \"complaint-predictions\"\n",
        "KAFKA_TOPIC_METRICS = \"streaming-metrics\"\n",
        "\n",
        "# Simulation parameters\n",
        "MESSAGES_PER_MINUTE = 100\n",
        "BATCH_SIZE = 20\n",
        "\n",
        "# Training parameters\n",
        "TRAIN_SAMPLE_LIMIT = 20000 # Adjust based on Colab RAM\n",
        "VAL_SAMPLE_LIMIT = 2000   # Adjust based on Colab RAM\n",
        "BERT_MAX_LENGTH = 128\n",
        "BERT_BATCH_SIZE = 16 # Per GPU/Process\n",
        "NUM_EPOCHS = 3       # Reduced for faster demo\n",
        "\n",
        "# --- Create Directories ---\n",
        "for path in [\n",
        "    f\"{BASE_DIR}/data\", f\"{BASE_DIR}/models\", f\"{BASE_DIR}/checkpoints\",\n",
        "    MLFLOW_TRACKING_URI.replace(\"file://\", \"\"), VISUALIZATION_DIR\n",
        "]:\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "# ==============================================================================\n",
        "# CELL 1 (REVISED): SETUP KAFKA FILES AND DATASET IN COLAB\n",
        "# ==============================================================================\n",
        "# Run this cell BEFORE the main Python pipeline script cell.\n",
        "# This downloads/extracts Kafka files and prepares the dataset.\n",
        "# *** YOU MUST START ZOOKEEPER AND KAFKA MANUALLY IN SEPARATE COLAB TERMINALS ***\n",
        "# *** AND CREATE THE TOPICS MANUALLY BEFORE RUNNING THE MAIN SCRIPT CELL ***\n",
        "\n",
        "import os\n",
        "import time\n",
        "from IPython import get_ipython # Import for running shell commands\n",
        "\n",
        "print(\"--- Starting Setup Cell (Files & Data Only) ---\")\n",
        "\n",
        "# --- Configuration ---\n",
        "KAFKA_VERSION = \"3.2.1\"\n",
        "SCALA_VERSION = \"2.13\"\n",
        "KAFKA_DIR = \"/content/kafka\"\n",
        "KAFKA_PKG = f\"kafka_{SCALA_VERSION}-{KAFKA_VERSION}\"\n",
        "KAFKA_HOME = f\"{KAFKA_DIR}/{KAFKA_PKG}\"\n",
        "DATASET_ZIP_URL = \"https://files.consumerfinance.gov/ccdb/complaints.csv.zip\"\n",
        "DATASET_ZIP_PATH = \"/content/complaints.zip\"\n",
        "FINAL_CSV_PATH = \"/content/Consumer_Complaints.csv\"\n",
        "REQUIRED_TOPICS = [ # Topics needed by the main script\n",
        "    \"complaints-raw\",\n",
        "    \"complaints-training-data\",\n",
        "    \"complaints-testing-stream\",\n",
        "    \"complaint-predictions\",\n",
        "    \"streaming-metrics\"\n",
        "]\n",
        "\n",
        "\n",
        "# --- 1. Download and Extract Kafka ---\n",
        "print(\"\\n[1/3] Downloading and Extracting Kafka Files...\")\n",
        "if not os.path.exists(KAFKA_HOME):\n",
        "    print(f\"Downloading Kafka {KAFKA_VERSION}...\")\n",
        "    get_ipython().system(f'mkdir -p {KAFKA_DIR}')\n",
        "    get_ipython().system(f'wget -q -O {KAFKA_DIR}/kafka.tgz https://archive.apache.org/dist/kafka/{KAFKA_VERSION}/{KAFKA_PKG}.tgz')\n",
        "    print(\"Extracting Kafka...\")\n",
        "    get_ipython().system(f'tar -xzf {KAFKA_DIR}/kafka.tgz -C {KAFKA_DIR}')\n",
        "    get_ipython().system(f'rm {KAFKA_DIR}/kafka.tgz') # Clean up tarball\n",
        "    print(f\"Kafka files extracted to {KAFKA_HOME}\")\n",
        "else:\n",
        "    print(\"Kafka directory already exists. Skipping download/extraction.\")\n",
        "# Verify extraction by checking for a key script\n",
        "print(f\"Checking for Kafka start script: {KAFKA_HOME}/bin/kafka-server-start.sh\")\n",
        "if os.path.exists(f\"{KAFKA_HOME}/bin/kafka-server-start.sh\"):\n",
        "    print(\" -> Kafka scripts seem present.\")\n",
        "else:\n",
        "    print(\" -> ERROR: Kafka scripts not found after extraction attempt!\")\n",
        "\n",
        "\n",
        "# --- 2. Download and Prepare Dataset ---\n",
        "print(\"\\n[2/3] Downloading and Preparing Dataset...\")\n",
        "if not os.path.exists(FINAL_CSV_PATH):\n",
        "    print(f\"Downloading dataset from {DATASET_ZIP_URL}...\")\n",
        "    get_ipython().system(f'wget -q -O {DATASET_ZIP_PATH} {DATASET_ZIP_URL}')\n",
        "    print(\"Unzipping dataset...\")\n",
        "    get_ipython().system(f'unzip -o {DATASET_ZIP_PATH} -d /content/') # -o overwrites\n",
        "    # Try to find the unzipped CSV (common pattern or specific name)\n",
        "    potential_csv = get_ipython().getoutput('ls /content/*.csv | grep -v Consumer_Complaints.csv || echo \"\"')\n",
        "    unzipped_csv_path = \"\"\n",
        "    if potential_csv and potential_csv[0] and os.path.exists(potential_csv[0]):\n",
        "         unzipped_csv_path = potential_csv[0]\n",
        "         print(f\"Found unzipped file: {unzipped_csv_path}\")\n",
        "         # Rename the found CSV to the expected name\n",
        "         print(f\"Renaming {unzipped_csv_path} to {FINAL_CSV_PATH}\")\n",
        "         get_ipython().system(f'mv \"{unzipped_csv_path}\" \"{FINAL_CSV_PATH}\"')\n",
        "    else:\n",
        "         # Check if it was already named correctly\n",
        "         if os.path.exists(\"/content/complaints.csv\") and not os.path.exists(FINAL_CSV_PATH):\n",
        "              print(\"Renaming /content/complaints.csv to {FINAL_CSV_PATH}\")\n",
        "              get_ipython().system(f'mv \"/content/complaints.csv\" \"{FINAL_CSV_PATH}\"')\n",
        "         elif not os.path.exists(FINAL_CSV_PATH):\n",
        "              print(f\"WARN: Could not automatically find the unzipped CSV. Please ensure '{FINAL_CSV_PATH}' exists.\")\n",
        "\n",
        "    # Clean up zip file\n",
        "    if os.path.exists(DATASET_ZIP_PATH):\n",
        "         get_ipython().system(f'rm {DATASET_ZIP_PATH}')\n",
        "else:\n",
        "    print(\"Dataset CSV file already exists. Skipping download.\")\n",
        "\n",
        "# Final check for the dataset file\n",
        "print(\"\\nChecking for final dataset file:\")\n",
        "get_ipython().system(f'ls -lh {FINAL_CSV_PATH}')\n",
        "if not os.path.exists(FINAL_CSV_PATH):\n",
        "    print(f\"ERROR: Final dataset file {FINAL_CSV_PATH} not found.\")\n",
        "    print(\"Setup incomplete. Cannot proceed without data.\")\n",
        "else:\n",
        "    print(\" -> Dataset file looks ready.\")\n",
        "\n",
        "\n",
        "# --- 3. Instructions for Manual Kafka/ZK Start & Topic Creation ---\n",
        "print(\"\\n[3/3] MANUAL ACTIONS REQUIRED:\")\n",
        "print(\"   -----------------------------\")\n",
        "print(\"   1. Open Colab Terminal (Icon: >_).\")\n",
        "print(f\"   2. Navigate: cd {KAFKA_HOME}\")\n",
        "print(\"   3. Start Zookeeper (leave terminal open): bin/zookeeper-server-start.sh config/zookeeper.properties\")\n",
        "print(\"   4. Open a SECOND Colab Terminal tab (+ icon).\")\n",
        "print(f\"   5. Navigate again: cd {KAFKA_HOME}\")\n",
        "print(\"   6. Start Kafka Broker (leave terminal open): bin/kafka-server-start.sh config/server.properties\")\n",
        "print(\"   7. Open a THIRD Colab Terminal tab (optional, can reuse).\")\n",
        "print(f\"   8. Navigate again: cd {KAFKA_HOME}\")\n",
        "print(\"   9. Create REQUIRED topics (if they don't exist):\")\n",
        "for topic in REQUIRED_TOPICS:\n",
        "    print(f\"      bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic {topic}\")\n",
        "print(\"  10. Verify topics exist: bin/kafka-topics.sh --list --bootstrap-server localhost:9092\")\n",
        "print(\"  11. AFTER completing steps 1-10, run the next cell containing the main Python pipeline script.\")\n",
        "print(\"   -----------------------------\")\n",
        "print(\"\\n--- Setup Cell Completed (Files & Data Prep Only) ---\")\n",
        "\n",
        "# --- Initialize Spark Session ---\n",
        "print(\"Initializing Spark Session...\")\n",
        "# CORRECTED INDENTATION: Ensure NO extra spaces before each dot (.)\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Consumer Complaints ML Pipeline\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.5\") \\\n",
        "    .config(\"spark.sql.streaming.checkpointLocation\", STREAMING_CHECKPOINT_LOCATION) \\\n",
        "    .config(\"spark.executor.memory\", \"6g\") \\\n",
        "    .config(\"spark.driver.memory\", \"6g\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"\\n--- Spark Configuration ---\")\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"Application ID: {spark.sparkContext.applicationId}\")\n",
        "print(f\"Using PySpark: {spark.sparkContext.pythonVer}\")\n",
        "\n",
        "# --- MLflow Setup ---\n",
        "def setup_mlflow_tracking():\n",
        "    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
        "    mlflow.set_experiment(\"complaint-classification\")\n",
        "    print(f\"MLflow tracking configured. URI: {MLFLOW_TRACKING_URI}\")\n",
        "    return mlflow\n",
        "\n",
        "mlflow = setup_mlflow_tracking()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3fecc8e-8c75-4526-ff88-45bfc12a4255",
        "id": "4mfFY7h7mhrb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initializing Pipeline Script ---\n",
            "Ensure Kafka/Zookeeper are running externally and topics are created.\n",
            "Ensure Consumer_Complaints.csv is available at /content/Consumer_Complaints.csv\n",
            "Ensure required packages are installed (run pip install cell).\n",
            "Imported v0.1.905. Please call AutoViz in this sequence:\n",
            "    AV = AutoViz_Class()\n",
            "    %matplotlib inline\n",
            "    dfte = AV.AutoViz(filename, sep=',', depVar='', dfte=None, header=0, verbose=1, lowess=False,\n",
            "               chart_format='svg',max_rows_analyzed=150000,max_cols_analyzed=30, save_plot_dir=None)\n",
            "--- Starting Setup Cell (Files & Data Only) ---\n",
            "\n",
            "[1/3] Downloading and Extracting Kafka Files...\n",
            "Downloading Kafka 3.2.1...\n",
            "Extracting Kafka...\n",
            "Kafka files extracted to /content/kafka/kafka_2.13-3.2.1\n",
            "Checking for Kafka start script: /content/kafka/kafka_2.13-3.2.1/bin/kafka-server-start.sh\n",
            " -> Kafka scripts seem present.\n",
            "\n",
            "[2/3] Downloading and Preparing Dataset...\n",
            "Downloading dataset from https://files.consumerfinance.gov/ccdb/complaints.csv.zip...\n",
            "Unzipping dataset...\n",
            "Archive:  /content/complaints.zip\n",
            "  inflating: /content/complaints.csv  \n",
            "Found unzipped file: /content/complaints.csv\n",
            "Renaming /content/complaints.csv to /content/Consumer_Complaints.csv\n",
            "\n",
            "Checking for final dataset file:\n",
            "-rw-r--r-- 1 root root 5.0G Mar 27 09:10 /content/Consumer_Complaints.csv\n",
            " -> Dataset file looks ready.\n",
            "\n",
            "[3/3] MANUAL ACTIONS REQUIRED:\n",
            "   -----------------------------\n",
            "   1. Open Colab Terminal (Icon: >_).\n",
            "   2. Navigate: cd /content/kafka/kafka_2.13-3.2.1\n",
            "   3. Start Zookeeper (leave terminal open): bin/zookeeper-server-start.sh config/zookeeper.properties\n",
            "   4. Open a SECOND Colab Terminal tab (+ icon).\n",
            "   5. Navigate again: cd /content/kafka/kafka_2.13-3.2.1\n",
            "   6. Start Kafka Broker (leave terminal open): bin/kafka-server-start.sh config/server.properties\n",
            "   7. Open a THIRD Colab Terminal tab (optional, can reuse).\n",
            "   8. Navigate again: cd /content/kafka/kafka_2.13-3.2.1\n",
            "   9. Create REQUIRED topics (if they don't exist):\n",
            "      bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic complaints-raw\n",
            "      bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic complaints-training-data\n",
            "      bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic complaints-testing-stream\n",
            "      bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic complaint-predictions\n",
            "      bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic streaming-metrics\n",
            "  10. Verify topics exist: bin/kafka-topics.sh --list --bootstrap-server localhost:9092\n",
            "  11. AFTER completing steps 1-10, run the next cell containing the main Python pipeline script.\n",
            "   -----------------------------\n",
            "\n",
            "--- Setup Cell Completed (Files & Data Prep Only) ---\n",
            "Initializing Spark Session...\n",
            "\n",
            "--- Spark Configuration ---\n",
            "Spark Version: 3.5.5\n",
            "Application ID: local-1743130791052\n",
            "Using PySpark: 3.11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/03/28 02:59:53 INFO mlflow.tracking.fluent: Experiment with name 'complaint-classification' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLflow tracking configured. URI: file:///content/consumer_complaints/mlflow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Schema Definitions ---\n",
        "def get_full_schema():\n",
        "    # ... (schema definition remains the same)\n",
        "    return StructType([\n",
        "        StructField(\"Date received\", StringType(), True), StructField(\"Product\", StringType(), True),\n",
        "        StructField(\"Sub-product\", StringType(), True), StructField(\"Issue\", StringType(), True),\n",
        "        StructField(\"Sub-issue\", StringType(), True), StructField(\"Consumer complaint narrative\", StringType(), True),\n",
        "        StructField(\"Company public response\", StringType(), True), StructField(\"Company\", StringType(), True),\n",
        "        StructField(\"State\", StringType(), True), StructField(\"ZIP code\", StringType(), True),\n",
        "        StructField(\"Tags\", StringType(), True), StructField(\"Consumer consent provided?\", StringType(), True),\n",
        "        StructField(\"Submitted via\", StringType(), True), StructField(\"Date sent to company\", StringType(), True),\n",
        "        StructField(\"Company response to consumer\", StringType(), True), StructField(\"Timely response?\", StringType(), True),\n",
        "        StructField(\"Consumer disputed?\", StringType(), True), StructField(\"Complaint ID\", StringType(), True)\n",
        "    ])\n",
        "\n",
        "def get_streaming_schema():\n",
        "    # ... (schema definition remains the same)\n",
        "    return StructType([\n",
        "        StructField(\"Date received\", StringType(), True), StructField(\"Complaint ID\", StringType(), True),\n",
        "        StructField(\"Company\", StringType(), True), StructField(\"State\", StringType(), True),\n",
        "        StructField(\"ZIP code\", StringType(), True), StructField(\"Submitted via\", StringType(), True),\n",
        "        StructField(\"Consumer complaint narrative\", StringType(), True)\n",
        "    ])\n",
        "\n",
        "# --- Custom BERT Embedding Transformer ---\n",
        "class BERTEmbeddingTransformer(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
        "    # ... (class definition remains the same)\n",
        "    def __init__(self, inputCol=None, outputCol=None, modelPath=None):\n",
        "        super().__init__()\n",
        "        self.inputCol = Param(self, \"inputCol\", \"\")\n",
        "        self.outputCol = Param(self, \"outputCol\", \"\")\n",
        "        self.modelPath = Param(self, \"modelPath\", \"\")\n",
        "        self._setDefault(inputCol=inputCol, outputCol=outputCol, modelPath=modelPath)\n",
        "        self.setModelPath(modelPath)\n",
        "    def setInputCol(self, value): return self._set(inputCol=value)\n",
        "    def getInputCol(self): return self.getOrDefault(self.inputCol)\n",
        "    def setOutputCol(self, value): return self._set(outputCol=value)\n",
        "    def getOutputCol(self): return self.getOrDefault(self.outputCol)\n",
        "    def setModelPath(self, value): return self._set(modelPath=value)\n",
        "    def getModelPath(self): return self.getOrDefault(self.modelPath)\n",
        "    def _transform(self, dataset):\n",
        "        schema = dataset.schema; input_col_name = self.getInputCol(); output_col_name = self.getOutputCol(); _model_path = self.getModelPath()\n",
        "        if input_col_name not in schema.fieldNames(): raise ValueError(f\"Input column '{input_col_name}' does not exist.\")\n",
        "        @F.pandas_udf(ArrayType(FloatType()))\n",
        "        def bert_embed_batch(texts_series: pd.Series) -> pd.Series:\n",
        "            if not hasattr(bert_embed_batch, 'model') or not hasattr(bert_embed_batch, 'tokenizer'):\n",
        "                model_dir = _model_path\n",
        "                try:\n",
        "                    if not model_dir or not os.path.exists(model_dir): raise ValueError(f\"Model path '{model_dir}' not found.\")\n",
        "                    bert_embed_batch.tokenizer = DistilBertTokenizer.from_pretrained(model_dir)\n",
        "                    bert_embed_batch.model = DistilBertModel.from_pretrained(model_dir)\n",
        "                    bert_embed_batch.model.to(\"cpu\").eval(); bert_embed_batch.embedding_dim = bert_embed_batch.model.config.dim\n",
        "                    print(f\"Worker loaded BERT embedder from {model_dir}.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Worker ERROR loading BERT embedder from '{model_dir}': {e}. Zero embeds.\")\n",
        "                    bert_embed_batch.tokenizer = None; bert_embed_batch.model = None; bert_embed_batch.embedding_dim = 768\n",
        "            results = []\n",
        "            if bert_embed_batch.model is None or bert_embed_batch.tokenizer is None: return pd.Series([[0.0] * bert_embed_batch.embedding_dim] * len(texts_series))\n",
        "            for text in texts_series:\n",
        "                try:\n",
        "                    clean_text = str(text) if text is not None else \"\";\n",
        "                    if len(clean_text.strip()) == 0: results.append([0.0] * bert_embed_batch.embedding_dim); continue\n",
        "                    inputs = bert_embed_batch.tokenizer(clean_text, return_tensors=\"pt\", truncation=True, max_length=BERT_MAX_LENGTH, padding=\"max_length\")\n",
        "                    with torch.no_grad(): outputs = bert_embed_batch.model(**inputs)\n",
        "                    results.append(outputs.last_hidden_state[:, 0, :].squeeze().tolist())\n",
        "                except Exception as e: results.append([0.0] * bert_embed_batch.embedding_dim)\n",
        "            return pd.Series(results)\n",
        "        return dataset.withColumn(output_col_name, bert_embed_batch(F.col(input_col_name)))\n",
        "    def write(self):\n",
        "        writer = DefaultParamsWriter(self); original_save = writer.save\n",
        "        def custom_save(path):\n",
        "            original_save(path); extra_metadata = {\"modelPath\": self.getModelPath()}\n",
        "            extra_metadata_path = os.path.join(path, \"bert_model_metadata.json\")\n",
        "            with open(extra_metadata_path, \"w\") as f: json.dump(extra_metadata, f)\n",
        "        writer.save = custom_save; return writer\n",
        "    @classmethod\n",
        "    def read(cls):\n",
        "        reader = DefaultParamsReader(cls); original_load = reader.load\n",
        "        def custom_load(path):\n",
        "            instance = original_load(path); extra_metadata_path = os.path.join(path, \"bert_model_metadata.json\")\n",
        "            if os.path.exists(extra_metadata_path):\n",
        "                with open(extra_metadata_path, \"r\") as f: extra_metadata = json.load(f); instance.setModelPath(extra_metadata.get(\"modelPath\"))\n",
        "            return instance\n",
        "        reader.load = custom_load; return reader\n",
        "\n",
        "# --- PyTorch Classes ---\n",
        "class ComplaintDataset(Dataset):\n",
        "    # ... (class definition remains the same)\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts; self.labels = labels; self.tokenizer = tokenizer; self.max_length = max_length\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx] if self.texts[idx] else \"\";\n",
        "        encoding = self.tokenizer.encode_plus(text, add_special_tokens=True, max_length=self.max_length, padding='max_length', truncation=True, return_attention_mask=True, return_tensors='pt')\n",
        "        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'labels': torch.tensor(self.labels[idx], dtype=torch.long)}\n",
        "\n",
        "class EnhancedDistilBERTClassifier(torch.nn.Module):\n",
        "    # ... (class definition remains the same, including BatchNorm check)\n",
        "    def __init__(self, bert_model, dropout_rate=0.3):\n",
        "        super().__init__(); self.bert = bert_model; self.dropout1 = torch.nn.Dropout(dropout_rate)\n",
        "        hidden_size = self.bert.config.dim; self.dense1 = torch.nn.Linear(hidden_size, 256)\n",
        "        self.batch_norm1 = torch.nn.BatchNorm1d(256); self.relu1 = torch.nn.ReLU()\n",
        "        self.dropout2 = torch.nn.Dropout(dropout_rate); self.dense2 = torch.nn.Linear(256, 64)\n",
        "        self.batch_norm2 = torch.nn.BatchNorm1d(64); self.relu2 = torch.nn.ReLU()\n",
        "        self.dropout3 = torch.nn.Dropout(dropout_rate); self.classifier = torch.nn.Linear(64, 2)\n",
        "    def forward(self, input_ids=None, attention_mask=None, embeddings=None):\n",
        "        if embeddings is None:\n",
        "            if input_ids is None: raise ValueError(\"Need input_ids or embeddings\"); outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask); sequence_output = outputs.last_hidden_state[:, 0, :]\n",
        "        else: sequence_output = embeddings\n",
        "        x = self.dropout1(sequence_output); x = self.dense1(x)\n",
        "        if x.shape[0] > 1 or not self.training:\n",
        "             try: x = self.batch_norm1(x)\n",
        "             except ValueError as e: print(f\"WARN: BN1 err (shape {x.shape}): {e}. Skip.\")\n",
        "        x = self.relu1(x); x = self.dropout2(x); x = self.dense2(x)\n",
        "        if x.shape[0] > 1 or not self.training:\n",
        "             try: x = self.batch_norm2(x)\n",
        "             except ValueError as e: print(f\"WARN: BN2 err (shape {x.shape}): {e}. Skip.\")\n",
        "        x = self.relu2(x); x = self.dropout3(x); logits = self.classifier(x); return logits\n",
        "\n",
        "# --- Kafka Metrics Listener ---\n",
        "class MetricsListener(StreamingQueryListener):\n",
        "    # ... (class definition remains the same, including close_producer)\n",
        "    def __init__(self, kafka_brokers, topic):\n",
        "        self.topic = topic; self.producer = None\n",
        "        try:\n",
        "            self.producer = KafkaProducer(bootstrap_servers=kafka_brokers.split(','), value_serializer=lambda v: json.dumps(v).encode('utf-8'), retries=3, linger_ms=5, request_timeout_ms=10000)\n",
        "            print(\"MetricsListener Kafka Producer initialized.\")\n",
        "        except Exception as e: print(f\"ERROR: MetricsListener failed init: {e}\")\n",
        "    def send_metric(self, metrics):\n",
        "        if self.producer:\n",
        "            try: self.producer.send(self.topic, value=metrics)\n",
        "            except Exception as e: print(f\"ERROR sending metric: {e}\")\n",
        "    def onQueryStarted(self, event): self.send_metric({\"queryName\": event.name, \"id\": str(event.id), \"runId\": str(event.runId),\"timestamp\": event.timestamp, \"event\": \"started\" })\n",
        "    def onQueryProgress(self, event): self.send_metric({\"queryName\": event.progress.name, \"id\": str(event.progress.id), \"runId\": str(event.progress.runId),\"timestamp\": event.progress.timestamp, \"event\": \"progress\", \"numInputRows\": event.progress.numInputRows,\"inputRowsPerSecond\": event.progress.inputRowsPerSecond, \"processedRowsPerSecond\": event.progress.processedRowsPerSecond,\"batchId\": event.progress.batchId })\n",
        "    def onQueryTerminated(self, event): self.send_metric({\"queryName\": getattr(event, 'name', None), \"id\": str(event.id), \"runId\": str(event.runId),\"timestamp\": time.time() * 1000, \"event\": \"terminated\", \"exception\": str(event.exception) if event.exception else None })\n",
        "    def close_producer(self):\n",
        "        if self.producer:\n",
        "            try: print(\"Closing MetricsListener Kafka Producer...\"); self.producer.flush(timeout=5); self.producer.close(timeout=5); self.producer = None; print(\"MetricsListener Producer closed.\")\n",
        "            except Exception as e: print(f\"ERROR closing MetricsListener Producer: {e}\")\n",
        "    def __del__(self): self.close_producer()"
      ],
      "metadata": {
        "id": "H9EgQ0ZtkcG0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Task 1: Load Data to Kafka ---\n",
        "# Use the version that filters narrative BEFORE sampling\n",
        "def load_data_to_kafka():\n",
        "    # ... (function remains the same as previous correct version)\n",
        "    print(f\"\\n--- Task 1: Loading Data to Kafka ---\"); print(f\"Reading CSV: {CSV_FILE_PATH}\")\n",
        "    if not os.path.exists(CSV_FILE_PATH): print(f\"ERROR: CSV not found: {CSV_FILE_PATH}\"); return None\n",
        "    raw_df_unfiltered = spark.read.format(\"csv\").option(\"header\", \"true\").schema(get_full_schema()).option(\"escape\", \"\\\"\").option(\"multiLine\", \"true\").load(CSV_FILE_PATH)\n",
        "    initial_count = raw_df_unfiltered.count(); print(f\"Total records loaded initially: {initial_count}\")\n",
        "    if initial_count == 0: return None\n",
        "    print(\"Filtering for non-empty narrative...\"); raw_df = raw_df_unfiltered.filter((F.col(\"Consumer complaint narrative\").isNotNull()) &(F.length(F.trim(F.col(\"Consumer complaint narrative\"))) > 0))\n",
        "    filtered_count = raw_df.cache().count(); print(f\"Records after narrative filter: {filtered_count}\"); raw_df_unfiltered.unpersist()\n",
        "    if filtered_count == 0: print(\"ERROR: No records with non-empty narratives.\"); raw_df.unpersist(); return None\n",
        "    MAX_KAFKA_LOAD_RECORDS = 50000; df_to_write = raw_df\n",
        "    if filtered_count > MAX_KAFKA_LOAD_RECORDS:\n",
        "        sample_fraction = MAX_KAFKA_LOAD_RECORDS / filtered_count; print(f\"Sampling {sample_fraction:.2%} ({MAX_KAFKA_LOAD_RECORDS}) for Kafka...\"); df_to_write = raw_df.sample(False, sample_fraction, seed=42)\n",
        "        write_count = df_to_write.cache().count(); print(f\"Sample size for Kafka: {write_count} records\") # Cache sample\n",
        "    try:\n",
        "        print(f\"Writing data to Kafka topic: {KAFKA_TOPIC_RAW}...\"); kafka_df = df_to_write.selectExpr(\"`Complaint ID` AS key\", \"to_json(struct(*)) AS value\")\n",
        "        kafka_df.write.format(\"kafka\").option(\"kafka.bootstrap.servers\", KAFKA_BROKERS).option(\"topic\", KAFKA_TOPIC_RAW).option(\"kafka.request.timeout.ms\", \"120000\").option(\"kafka.delivery.timeout.ms\", \"180000\").save()\n",
        "        print(f\"Data written to Kafka topic: {KAFKA_TOPIC_RAW}\")\n",
        "    except Exception as e: print(f\"ERROR writing to Kafka: {e}\"); return None\n",
        "    finally:\n",
        "         if 'write_count' in locals() and df_to_write.is_cached: df_to_write.unpersist() # Unpersist sample if created\n",
        "         if raw_df.is_cached: raw_df.unpersist()\n",
        "    return df_to_write\n",
        "\n",
        "# --- Task 2: Preprocess, Filter, Visualize ---\n",
        "# Use the version with inline AutoViz and no date filter\n",
        "def visualize_with_autoviz(df, max_rows=5000, save_dir=VISUALIZATION_DIR):\n",
        "    # ... (function remains the same as previous correct version)\n",
        "    if not AUTOVIZ_AVAILABLE: print(\"AutoViz skip.\"); return\n",
        "    try:\n",
        "        print(\"\\nStarting AutoViz...\"); os.makedirs(save_dir, exist_ok=True); df = df.cache(); df_count = df.count()\n",
        "        if df_count == 0: print(\"No data to visualize.\"); df.unpersist(); return\n",
        "        if df_count > max_rows: print(f\"Sampling {max_rows} for AutoViz.\"); fraction = max_rows / df_count; sample_df = df.sample(False, fraction, seed=42)\n",
        "        else: sample_df = df\n",
        "        pandas_df = sample_df.limit(max_rows).toPandas(); df.unpersist()\n",
        "        if not pandas_df.empty:\n",
        "            AV = AutoViz_Class(); print(\"Setting inline display...\");\n",
        "            try: from IPython import get_ipython; ipy = get_ipython(); ipy.run_line_magic('matplotlib', 'inline')\n",
        "            except Exception as magic_e: print(f\"WARN: Inline failed: {magic_e}\")\n",
        "            print(\"Generating AutoViz charts (verbose=1)...\")\n",
        "            viz_df = AV.AutoViz(\"\", dfte=pandas_df, depVar=\"\", header=0, verbose=1, lowess=False, chart_format=\"html\", max_rows_analyzed=max_rows, save_plot_dir=save_dir)\n",
        "            print(f\"\\nAutoViz saved to: {save_dir}\")\n",
        "        else: print(\"Sample empty, skip AutoViz.\")\n",
        "    except Exception as e: print(f\"Error during AutoViz: {e}\"); traceback.print_exc()\n",
        "    finally:\n",
        "        if 'df' in locals() and df.is_cached: df.unpersist()\n",
        "\n",
        "def preprocess_filter_and_visualize():\n",
        "    # ... (function remains the same - reads from kafka, filters response, calls visualize)\n",
        "    print(f\"\\n--- Task 2: Preprocessing, Filtering & Visualization ---\"); print(f\"Reading from Kafka: {KAFKA_TOPIC_RAW}\")\n",
        "    full_schema = get_full_schema(); print(\"Waiting 5s...\"); time.sleep(5)\n",
        "    try:\n",
        "        kafka_raw_df = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", KAFKA_BROKERS).option(\"subscribe\", KAFKA_TOPIC_RAW).option(\"startingOffsets\", \"earliest\").option(\"failOnDataLoss\", \"false\").load()\n",
        "        kafka_read_count = kafka_raw_df.count(); print(f\"Read {kafka_read_count} raw msgs.\");\n",
        "        if kafka_read_count == 0: return None\n",
        "        parsed_df = kafka_raw_df.select(F.from_json(F.col(\"value\").cast(\"string\"), full_schema).alias(\"data\")).select(\"data.*\").na.drop(subset=[\"Complaint ID\"])\n",
        "        dedup_df = parsed_df.dropDuplicates([\"Complaint ID\"]).cache(); print(f\"Records post parse/dedup: {dedup_df.count()}\")\n",
        "    except Exception as e: print(f\"ERROR reading/parsing Kafka: {e}\"); traceback.print_exc(); return None\n",
        "    # Apply filters (narrative already filtered in Task 1, keep response filter)\n",
        "    filtered_df = dedup_df.filter(F.col(\"Company response to consumer\") != \"In progress\")\n",
        "    filtered_df = filtered_df.filter((F.col(\"Consumer complaint narrative\").isNotNull()) & (F.length(F.trim(F.col(\"Consumer complaint narrative\"))) > 0)).cache() # Keep narrative check for safety\n",
        "    filtered_count = filtered_df.count(); print(f\"Records after filtering (response !='In progress'): {filtered_count}\")\n",
        "    dedup_df.unpersist()\n",
        "    if filtered_count == 0: print(\"WARNING: No records left after filtering.\"); filtered_df.unpersist(); return None\n",
        "    visualize_with_autoviz(filtered_df)\n",
        "    return filtered_df\n",
        "\n",
        "# --- Task 3: Split, Label, Prepare Data ---\n",
        "# Keep this function as is\n",
        "def split_label_and_prepare_data(filtered_df, seed_value=42):\n",
        "    # ... (function remains the same)\n",
        "    print(\"\\n--- Task 3: Data Splitting & Labeling ---\");\n",
        "    if filtered_df is None: return None, None; df_for_split = filtered_df\n",
        "    training_base_df, test_base_df = df_for_split.randomSplit([0.8, 0.2], seed=seed_value)\n",
        "    training_labeled_df = training_base_df.withColumn(\"is_target_complaint\",F.when((F.col(\"Consumer disputed?\") == 'No') & (F.col(\"Timely response?\") == 'Yes') & (F.col(\"Company response to consumer\").isin('Closed with explanation', 'Closed','Closed with monetary relief', 'Closed with non-monetary relief')), 1).otherwise(0)).cache()\n",
        "    print(\"Target distribution in training data:\"); training_labeled_df.groupBy(\"is_target_complaint\").count().show()\n",
        "    try:\n",
        "        print(f\"Writing training data to Kafka: {KAFKA_TOPIC_TRAINING}...\"); training_kafka_df = training_labeled_df.selectExpr(\"`Complaint ID` AS key\", \"to_json(struct(*)) AS value\")\n",
        "        training_kafka_df.write.format(\"kafka\").option(\"kafka.bootstrap.servers\", KAFKA_BROKERS).option(\"topic\", KAFKA_TOPIC_TRAINING).save(); print(f\"Training data sent.\")\n",
        "    except Exception as e: print(f\"ERROR writing train data to Kafka: {e}\")\n",
        "    try:\n",
        "        print(f\"Saving test data to Parquet: {TEST_DATA_PERSISTENCE_PATH}...\"); test_cols = [\"Date received\", \"Complaint ID\", \"Company\", \"State\", \"ZIP code\", \"Submitted via\", \"Consumer complaint narrative\"]\n",
        "        test_base_df.select(*test_cols).write.format(\"parquet\").mode(\"overwrite\").save(TEST_DATA_PERSISTENCE_PATH); print(f\"Test data saved.\")\n",
        "    except Exception as e: print(f\"ERROR saving test data: {e}\")\n",
        "    if df_for_split.is_cached: df_for_split.unpersist()\n",
        "    return training_labeled_df, test_base_df\n",
        "\n",
        "# --- Task 4: Distributed BERT Training ---\n",
        "# Use the version with the outer try/except and TorchDistributor fallback\n",
        "# --- Task 4: Distributed BERT Training ---\n",
        "# CORRECTED SyntaxError in train_function's except block (Model Init)\n",
        "def train_bert_model_distributed():\n",
        "    print(\"\\n--- Task 4: Distributed BERT Model Training ---\")\n",
        "    def train_function():\n",
        "        # Imports needed on worker\n",
        "        import pandas as pd\n",
        "        import mlflow\n",
        "        from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, DistributedSampler\n",
        "        from transformers import DistilBertTokenizer, DistilBertModel, get_linear_schedule_with_warmup\n",
        "        from torch.optim import AdamW\n",
        "        import torch.distributed as dist\n",
        "        import torch.nn as nn\n",
        "        from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "        import os, torch\n",
        "        from collections import OrderedDict\n",
        "        import numpy as np\n",
        "        import contextlib # Added contextlib\n",
        "\n",
        "        use_gpu = torch.cuda.is_available()\n",
        "        # Check if TorchDistributor initialized distributed environment\n",
        "        is_distributed = dist.is_available() and dist.is_initialized()\n",
        "        rank = dist.get_rank() if is_distributed else 0\n",
        "        world_size = dist.get_world_size() if is_distributed else 1\n",
        "        local_rank = rank % torch.cuda.device_count() if use_gpu else 0\n",
        "        device = torch.device(f\"cuda:{local_rank}\" if use_gpu else \"cpu\")\n",
        "        if use_gpu: torch.cuda.set_device(device)\n",
        "        is_rank_0 = (rank == 0)\n",
        "        if is_rank_0: print(f\"Worker {rank}/{world_size}: Start train_function. Device: {device}\")\n",
        "\n",
        "        if is_rank_0: print(f\"Worker {rank}: Reading Parquet...\");\n",
        "        try:\n",
        "            train_pd = pd.read_parquet(TRAIN_PARQUET_PATH)\n",
        "            val_pd = pd.read_parquet(VAL_PARQUET_PATH)\n",
        "        except Exception as e:\n",
        "            print(f\"Worker {rank} ERROR reading Parquet: {e}\")\n",
        "            if is_distributed:\n",
        "                dist.barrier() # Ensure all workers wait if one fails here\n",
        "            return None # Indicate failure\n",
        "\n",
        "        if is_rank_0: print(f\"Worker {rank}: Loaded {len(train_pd)} train, {len(val_pd)} val.\")\n",
        "        train_texts = train_pd[\"Consumer complaint narrative\"].fillna(\"\").astype(str).tolist(); train_labels = train_pd[\"is_target_complaint\"].tolist()\n",
        "        val_texts = val_pd[\"Consumer complaint narrative\"].fillna(\"\").astype(str).tolist(); val_labels = val_pd[\"is_target_complaint\"].tolist()\n",
        "\n",
        "        if is_rank_0: print(f\"Worker {rank}: Initializing model/tokenizer...\");\n",
        "        try:\n",
        "            if is_distributed and not is_rank_0: dist.barrier();\n",
        "            tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased');\n",
        "            base_model = DistilBertModel.from_pretrained('distilbert-base-uncased');\n",
        "            if is_distributed and is_rank_0: dist.barrier(); # Rank 0 waits for others\n",
        "            classifier = EnhancedDistilBERTClassifier(base_model, dropout_rate=0.3);\n",
        "            classifier.to(device)\n",
        "        # --- CORRECTED except block ---\n",
        "        except Exception as e:\n",
        "            print(f\"Worker {rank}: ERROR init model: {e}\")\n",
        "            if is_distributed:\n",
        "                 dist.barrier()\n",
        "            return None\n",
        "        # --- End Correction ---\n",
        "\n",
        "        train_dataset = ComplaintDataset(train_texts, train_labels, tokenizer, BERT_MAX_LENGTH); val_dataset = ComplaintDataset(val_texts, val_labels, tokenizer, BERT_MAX_LENGTH)\n",
        "        train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True) if is_distributed else RandomSampler(train_dataset); val_sampler = SequentialSampler(val_dataset)\n",
        "        train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BERT_BATCH_SIZE, num_workers=2, pin_memory=use_gpu); val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=BERT_BATCH_SIZE * 2, num_workers=2, pin_memory=use_gpu)\n",
        "\n",
        "        if is_distributed: classifier = torch.nn.parallel.DistributedDataParallel(classifier, device_ids=[local_rank] if use_gpu else None, find_unused_parameters=False)\n",
        "\n",
        "        optimizer_params = classifier.parameters(); optimizer = AdamW(optimizer_params, lr=3e-5)\n",
        "        num_training_steps = len(train_dataloader) * NUM_EPOCHS; scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * num_training_steps), num_training_steps=num_training_steps)\n",
        "        class_weights_tensor = torch.tensor([1.0, 1.0], dtype=torch.float)\n",
        "        if is_rank_0:\n",
        "            total = len(train_labels); pos_count = sum(train_labels); neg_count = total - pos_count\n",
        "            if pos_count > 0 and neg_count > 0: weight_for_0 = total / (2.0 * neg_count); weight_for_1 = total / (2.0 * pos_count); class_weights_tensor = torch.tensor([weight_for_0, weight_for_1], dtype=torch.float)\n",
        "            print(f\"Rank 0 weights: {class_weights_tensor}\")\n",
        "        if is_distributed: class_weights_tensor = class_weights_tensor.to(device); dist.broadcast(class_weights_tensor, src=0)\n",
        "        class_weights = class_weights_tensor.to(device); criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "        if is_rank_0 or not is_distributed: print(f\"Effective weights on {device}: {class_weights}\")\n",
        "\n",
        "        best_val_f1 = 0.0; mlflow_run_id = None\n",
        "        with mlflow.start_run(run_name=\"bert_classifier_dist\", nested=True) if is_rank_0 else contextlib.nullcontext() as run:\n",
        "            if is_rank_0 and run:\n",
        "                mlflow_run_id = run.info.run_id; mlflow.log_params({\"learning_rate\": 3e-5, \"batch_size_per_worker\": BERT_BATCH_SIZE, \"world_size\": world_size,\"total_batch_size\": BERT_BATCH_SIZE * world_size, \"epochs\": NUM_EPOCHS, \"bert_model\": \"distilbert-base-uncased\", \"data_loading\": \"Parquet_Worker_Read\", \"num_train_samples_in_parquet\": len(train_texts),\"num_val_samples_in_parquet\": len(val_texts), \"train_sample_limit\": TRAIN_SAMPLE_LIMIT,\"val_sample_limit\": VAL_SAMPLE_LIMIT})\n",
        "            for epoch in range(NUM_EPOCHS):\n",
        "                if is_rank_0: print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
        "                if isinstance(train_sampler, DistributedSampler): train_sampler.set_epoch(epoch)\n",
        "                classifier.train(); total_train_loss = 0.0; train_steps = 0\n",
        "                for step, batch in enumerate(train_dataloader):\n",
        "                    input_ids = batch['input_ids'].to(device); attention_mask = batch['attention_mask'].to(device); labels = batch['labels'].to(device)\n",
        "                    optimizer.zero_grad(); logits = classifier(input_ids=input_ids, attention_mask=attention_mask); loss = criterion(logits, labels)\n",
        "                    if torch.isnan(loss): print(f\"Worker {rank}: NaN loss step {step}! Skip.\"); continue\n",
        "                    loss.backward(); optimizer.step(); scheduler.step(); total_train_loss += loss.item(); train_steps += 1\n",
        "                    if is_rank_0 and step % 50 == 0 and step > 0: print(f\"  Ep{epoch+1}, Step {step}/{len(train_dataloader)}, Loss: {loss.item():.4f}\")\n",
        "                avg_train_loss = total_train_loss / train_steps if train_steps > 0 else 0.0\n",
        "                if is_distributed: loss_tensor = torch.tensor(avg_train_loss, device=device); dist.all_reduce(loss_tensor, op=dist.ReduceOp.AVG); avg_train_loss = loss_tensor.item()\n",
        "                if is_rank_0: print(f\"Ep{epoch+1} Avg Train Loss: {avg_train_loss:.4f}\");\n",
        "                if is_rank_0 and mlflow_run_id: mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
        "                classifier.eval(); all_val_preds = []; all_val_labels = []; total_val_loss = 0.0; val_steps = 0\n",
        "                with torch.no_grad():\n",
        "                    for batch in val_dataloader:\n",
        "                        input_ids = batch['input_ids'].to(device); attention_mask = batch['attention_mask'].to(device); labels = batch['labels'].to(device)\n",
        "                        logits = classifier(input_ids=input_ids, attention_mask=attention_mask); loss = criterion(logits, labels)\n",
        "                        if not torch.isnan(loss): total_val_loss += loss.item(); val_steps += 1\n",
        "                        else: print(f\"Worker {rank}: NaN val loss! Skip.\")\n",
        "                        preds = torch.argmax(logits, dim=1); all_val_preds.extend(preds.cpu().numpy()); all_val_labels.extend(labels.cpu().numpy())\n",
        "                avg_val_loss = 0.0\n",
        "                if is_distributed:\n",
        "                    val_loss_sum_tensor = torch.tensor(total_val_loss, device=device); val_steps_tensor = torch.tensor(val_steps, device=device)\n",
        "                    dist.all_reduce(val_loss_sum_tensor, op=dist.ReduceOp.SUM); dist.all_reduce(val_steps_tensor, op=dist.ReduceOp.SUM)\n",
        "                    total_val_loss_agg = val_loss_sum_tensor.item(); total_val_steps_agg = val_steps_tensor.item()\n",
        "                    avg_val_loss = total_val_loss_agg / total_val_steps_agg if total_val_steps_agg > 0 else 0.0\n",
        "                else: avg_val_loss = total_val_loss / val_steps if val_steps > 0 else 0.0\n",
        "                if is_rank_0:\n",
        "                    np_val_labels = np.array(all_val_labels); np_val_preds = np.array(all_val_preds)\n",
        "                    val_accuracy=accuracy_score(np_val_labels, np_val_preds); val_precision=precision_score(np_val_labels, np_val_preds, average='binary', zero_division=0)\n",
        "                    val_recall=recall_score(np_val_labels, np_val_preds, average='binary', zero_division=0); val_f1=f1_score(np_val_labels, np_val_preds, average='binary', zero_division=0)\n",
        "                    print(f\"Ep{epoch+1} Avg Val Loss: {avg_val_loss:.4f}\"); print(f\"Val Metrics (R0): Acc:{val_accuracy:.4f}, P:{val_precision:.4f}, R:{val_recall:.4f}, F1:{val_f1:.4f}\")\n",
        "                    if mlflow_run_id: mlflow.log_metrics({\"val_loss\": avg_val_loss, \"val_accuracy\": val_accuracy, \"val_precision\": val_precision, \"val_recall\": val_recall, \"val_f1\": val_f1}, step=epoch)\n",
        "                    if val_f1 > best_val_f1:\n",
        "                        best_val_f1 = val_f1; print(f\"  >>> New best F1: {val_f1:.4f}. Saving model...\"); os.makedirs(EMBEDDING_MODEL_SAVE_PATH, exist_ok=True)\n",
        "                        model_to_save = classifier.module if is_distributed else classifier\n",
        "                        try: torch.save(model_to_save.state_dict(), f\"{EMBEDDING_MODEL_SAVE_PATH}/classifier.pt\"); model_to_save.bert.save_pretrained(EMBEDDING_MODEL_SAVE_PATH); tokenizer.save_pretrained(EMBEDDING_MODEL_SAVE_PATH); print(f\"  >>> Model saved.\");\n",
        "                        except Exception as save_e: print(f\"  >>> ERROR saving model: {save_e}\")\n",
        "                        if mlflow_run_id: mlflow.log_metric(\"best_val_f1\", best_val_f1, step=epoch)\n",
        "                if is_distributed: dist.barrier()\n",
        "            if is_rank_0: print(f\"\\nTraining complete. Best Val F1: {best_val_f1:.4f}\");\n",
        "            if is_rank_0 and mlflow_run_id: mlflow.log_metric(\"final_best_f1\", best_val_f1)\n",
        "        return EMBEDDING_MODEL_SAVE_PATH\n",
        "    # --- Driver setup ---\n",
        "    try:\n",
        "        print(\"Reading training data from Kafka...\"); full_schema_with_target = get_full_schema().add(StructField(\"is_target_complaint\", IntegerType(), True))\n",
        "        kafka_df = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", KAFKA_BROKERS).option(\"subscribe\", KAFKA_TOPIC_TRAINING).option(\"startingOffsets\", \"earliest\").option(\"failOnDataLoss\", \"false\").load()\n",
        "        kafka_read_count = kafka_df.count(); print(f\"Read {kafka_read_count} raw training msgs.\");\n",
        "        if kafka_read_count == 0: return None, None\n",
        "        training_df = kafka_df.select(F.from_json(F.col(\"value\").cast(\"string\"), full_schema_with_target).alias(\"data\")).select(\"data.*\").na.drop(subset=[\"Complaint ID\"]).dropDuplicates([\"Complaint ID\"])\n",
        "        print(f\"Parsed {training_df.count()} unique training records.\")\n",
        "        pos_df = training_df.filter(F.col(\"is_target_complaint\") == 1); neg_df = training_df.filter(F.col(\"is_target_complaint\") == 0)\n",
        "        pos_count = pos_df.count(); neg_count = neg_df.count(); print(f\"Counts: +ve={pos_count}, -ve={neg_count}\")\n",
        "        ratio = pos_count / (pos_count + neg_count) if (pos_count + neg_count) > 0 else 0.5; print(f\"Ratio +ve: {ratio:.2f}\")\n",
        "        if ratio > 0 and ratio < 0.1 and neg_count > pos_count:\n",
        "            print(\"Balancing: Undersample negative...\"); target_neg_fraction = min(1.0, (pos_count * 7.0) / neg_count) if neg_count > 0 else 1.0; print(f\"Neg fraction: {target_neg_fraction:.2f}\")\n",
        "            neg_sample_df = neg_df.sample(False, target_neg_fraction, seed=42); balanced_df = pos_df.unionByName(neg_sample_df); print(f\"Balanced size: {balanced_df.count()} (+ve={pos_df.count()}, -ve={neg_sample_df.count()})\")\n",
        "        elif ratio > 0.9 and pos_count > neg_count:\n",
        "             print(\"Balancing: Undersample positive...\"); target_pos_fraction = min(1.0, (neg_count * 7.0) / pos_count) if pos_count > 0 else 1.0; print(f\"Pos fraction: {target_pos_fraction:.2f}\")\n",
        "             pos_sample_df = pos_df.sample(False, target_pos_fraction, seed=42); balanced_df = neg_df.unionByName(pos_sample_df); print(f\"Balanced size: {balanced_df.count()} (+ve={pos_sample_df.count()}, -ve={neg_df.count()})\")\n",
        "        else: print(\"No balancing needed.\"); balanced_df = training_df\n",
        "        balanced_df = balanced_df.cache(); balanced_count = balanced_df.count()\n",
        "        if balanced_count == 0: print(\"ERROR: Balanced DF empty.\"); return None, None\n",
        "        train_spark_df, val_spark_df = balanced_df.randomSplit([0.9, 0.1], seed=42); print(f\"Split sizes: Train={train_spark_df.count()}, Val={val_spark_df.count()}\")\n",
        "        final_train_df = train_spark_df.limit(TRAIN_SAMPLE_LIMIT).cache(); final_val_df = val_spark_df.limit(VAL_SAMPLE_LIMIT).cache()\n",
        "        final_train_count = final_train_df.count(); final_val_count = final_val_df.count(); print(f\"Final sampled sizes: Train={final_train_count}, Val={final_val_count}\")\n",
        "        if final_train_count == 0 or final_val_count == 0: print(\"ERROR: Not enough data post-sampling.\"); return None, None\n",
        "        print(f\"Saving train to Parquet: {TRAIN_PARQUET_PATH}\"); final_train_df.select(\"Consumer complaint narrative\", \"is_target_complaint\").write.mode(\"overwrite\").parquet(TRAIN_PARQUET_PATH)\n",
        "        print(f\"Saving val to Parquet: {VAL_PARQUET_PATH}\"); final_val_df.select(\"Consumer complaint narrative\", \"is_target_complaint\").write.mode(\"overwrite\").parquet(VAL_PARQUET_PATH)\n",
        "        balanced_df.unpersist(); final_train_df.unpersist(); final_val_df.unpersist()\n",
        "        print(\"Starting TorchDistributor...\")\n",
        "        if torch.cuda.is_available(): num_processes = torch.cuda.device_count(); use_gpu_dist_flag = True; print(f\"GPU available. procs={num_processes}, use_gpu=True.\")\n",
        "        else: num_processes = 1; use_gpu_dist_flag = False; print(\"NO GPU. procs=1, use_gpu=False.\")\n",
        "        print(\"NOTE: Workers load full Parquet subset.\"); distributor = None\n",
        "        try: distributor = TorchDistributor(num_processes=num_processes, local_mode=True, use_gpu=use_gpu_dist_flag, _ssl_conf=None); print(\"Distributor init OK.\")\n",
        "        except RuntimeError as e:\n",
        "            if \"GPUs were unable to be found on the driver\" in str(e):\n",
        "                print(\"WARN: Driver GPU check failed. Force CPU.\"); num_processes = 1; use_gpu_dist_flag = False\n",
        "                distributor = TorchDistributor(num_processes=num_processes, local_mode=True, use_gpu=use_gpu_dist_flag, _ssl_conf=None); print(\"Distributor init OK (CPU fallback).\")\n",
        "            else: print(f\"ERROR init Distributor: {e}\"); raise e\n",
        "        except Exception as e_init: print(f\"UNEXPECTED ERROR init Distributor: {e_init}\"); raise e_init\n",
        "        saved_model_path = distributor.run(train_function)\n",
        "        if saved_model_path is None or not os.path.exists(os.path.join(saved_model_path, \"config.json\")): raise RuntimeError(\"Training failed or model not saved.\")\n",
        "        print(f\"Training finished. Model in: {saved_model_path}\"); tokenizer = DistilBertTokenizer.from_pretrained(saved_model_path); return saved_model_path, tokenizer\n",
        "    except Exception as e: print(f\"ERROR during training setup/dist: {e}\"); traceback.print_exc(); return None, None\n",
        "\n",
        "# --- Task 5: Unified Feature Pipeline ---\n",
        "def create_unified_pipeline():\n",
        "    # ... (function definition remains the same - uses SQLTransformer)\n",
        "    print(\"\\n--- Task 5: Creating Unified Feature Pipeline ---\");\n",
        "    categorical_columns = [\"Company\", \"State\", \"Submitted via\"]; zip_col = \"ZIP code\"; stages = []; imputed_cols_map = {}\n",
        "    impute_select_exprs = [\"*\"]\n",
        "    for col_name in categorical_columns + [zip_col]: imputed_col_name = f\"{col_name}_imputed\"; impute_select_exprs.append(f\"COALESCE(CAST(`{col_name}` AS STRING), 'Unknown') AS {imputed_col_name}\"); imputed_cols_map[col_name] = imputed_col_name # Use backticks for safety\n",
        "    impute_sql = f\"SELECT {', '.join(impute_select_exprs)} FROM __THIS__\"; stages.append(SQLTransformer(statement=impute_sql))\n",
        "    stages.append(BERTEmbeddingTransformer(inputCol=\"Consumer complaint narrative\", outputCol=\"narrative_features\", modelPath=EMBEDDING_MODEL_SAVE_PATH))\n",
        "    encoded_columns = []\n",
        "    for col_name in categorical_columns + [zip_col]:\n",
        "        imputed_col_name = imputed_cols_map[col_name]; indexer_output = f\"{col_name}_indexed\"; encoder_output = f\"{col_name}_encoded\"\n",
        "        stages.append(StringIndexer(inputCol=imputed_col_name, outputCol=indexer_output, handleInvalid=\"keep\"))\n",
        "        stages.append(OneHotEncoder(inputCol=indexer_output, outputCol=encoder_output, dropLast=False))\n",
        "        encoded_columns.append(encoder_output)\n",
        "    feature_columns = [\"narrative_features\"] + encoded_columns; stages.append(VectorAssembler(inputCols=feature_columns, outputCol=\"features\", handleInvalid=\"keep\"))\n",
        "    pipeline = Pipeline(stages=stages); print(f\"Pipeline stages: {[type(s).__name__ for s in stages]}\"); return pipeline\n",
        "\n",
        "# --- Task 6: Simulation Script ---\n",
        "def simulate_test_data_to_kafka():\n",
        "    # ... (function remains the same)\n",
        "    print(\"\\n--- Task 6: Test Data Simulation ---\");\n",
        "    if not os.path.exists(TEST_DATA_PERSISTENCE_PATH): print(f\"ERROR: Test data not found: {TEST_DATA_PERSISTENCE_PATH}\"); return 0\n",
        "    print(f\"Loading test data: {TEST_DATA_PERSISTENCE_PATH}\")\n",
        "    try: test_pd = pd.read_parquet(TEST_DATA_PERSISTENCE_PATH); print(f\"Loaded {len(test_pd)} test records.\")\n",
        "    except Exception as e: print(f\"ERROR loading test data: {e}\"); return 0\n",
        "    messages = test_pd.to_dict('records'); total_messages = len(messages)\n",
        "    if total_messages == 0: print(\"No test messages.\"); return 0\n",
        "    producer = None\n",
        "    try: producer = KafkaProducer(bootstrap_servers=KAFKA_BROKERS.split(','), value_serializer=lambda v: json.dumps(v).encode('utf-8'), key_serializer=lambda k: str(k).encode('utf-8'), batch_size=16384, linger_ms=10, retries=3); print(f\"Sim Producer connected.\")\n",
        "    except kafka_errors.NoBrokersAvailable: print(f\"ERROR: Sim Producer connect failed.\"); return 0\n",
        "    delay = (60.0 / MESSAGES_PER_MINUTE) * BATCH_SIZE if MESSAGES_PER_MINUTE > 0 else 0; print(f\"Starting sim: {total_messages} msgs @ ~{MESSAGES_PER_MINUTE}/min (Batch: {BATCH_SIZE}, Delay: {delay:.2f}s)\")\n",
        "    start_time = time.time(); messages_sent = 0\n",
        "    try:\n",
        "        for i in range(0, total_messages, BATCH_SIZE):\n",
        "            batch_start = time.time(); batch_end = min(i + BATCH_SIZE, total_messages); batch = messages[i:batch_end]\n",
        "            for msg in batch: producer.send(KAFKA_TOPIC_TESTING_STREAM, key=msg.get(\"Complaint ID\", str(messages_sent)), value=msg); messages_sent += 1\n",
        "            producer.flush(); batch_elapsed = time.time() - batch_start; sleep_time = max(0, delay - batch_elapsed)\n",
        "            if sleep_time > 0: time.sleep(sleep_time)\n",
        "            if messages_sent % (BATCH_SIZE * 10) == 0 or messages_sent == total_messages: elapsed = time.time() - start_time; rate = (messages_sent / elapsed * 60) if elapsed > 0 else 0; print(f\"  Sim Progress: {messages_sent}/{total_messages} ({messages_sent/total_messages*100:.1f}%) @ {rate:.1f} msgs/min\")\n",
        "    except KeyboardInterrupt: print(\"\\nSim interrupted.\")\n",
        "    except Exception as e: print(f\"\\nERROR during sim: {e}\")\n",
        "    finally:\n",
        "        if producer: producer.close()\n",
        "        elapsed = time.time() - start_time; rate = (messages_sent / elapsed * 60) if elapsed > 0 else 0\n",
        "        print(\"\\nSim Summary:\"); print(f\"- Sent: {messages_sent}/{total_messages}\"); print(f\"- Time: {elapsed:.2f}s\"); print(f\"- Rate: {rate:.1f} msg/min\")\n",
        "    return messages_sent\n",
        "\n",
        "# --- Task 7: Streaming Inference Job ---\n",
        "# Use the version with improved error handling in UDF\n",
        "@F.pandas_udf(DoubleType())\n",
        "def predict_udf(features_iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
        "    # ... (UDF definition remains the same as previous correct version)\n",
        "    import pandas as pd; import torch; import numpy as np; from transformers import DistilBertModel; import os; from collections import OrderedDict # Need imports\n",
        "    model_path = EMBEDDING_MODEL_SAVE_PATH; classifier_state_path = os.path.join(model_path, \"classifier.pt\")\n",
        "    device = torch.device(\"cpu\"); classifier_head = None; embedding_dim = 768\n",
        "    try: # Load model components once\n",
        "        if os.path.exists(classifier_state_path) and os.path.exists(model_path):\n",
        "            bert_config = DistilBertModel.from_pretrained(model_path).config; embedding_dim = bert_config.dim\n",
        "            temp_bert = DistilBertModel(bert_config); classifier_head = EnhancedDistilBERTClassifier(temp_bert, dropout_rate=0.3) # Assuming class is accessible\n",
        "            state_dict = torch.load(classifier_state_path, map_location=device); new_state_dict = OrderedDict(); is_ddp = any(k.startswith('module.') for k in state_dict.keys())\n",
        "            for k, v in state_dict.items(): name = k[7:] if is_ddp else k; new_state_dict[name] = v\n",
        "            classifier_head.load_state_dict(new_state_dict); classifier_head.to(device); classifier_head.eval()\n",
        "        # else: print(f\"Worker WARN: Model/State missing. Default preds.\") # Reduce noise\n",
        "    except Exception as e: print(f\"Worker ERROR loading model: {e}. Default preds.\"); classifier_head = None\n",
        "    # Process partitions\n",
        "    for features_series in features_iterator:\n",
        "        if features_series.empty: yield pd.Series([], dtype=float); continue\n",
        "        results = []; expected_len = -1; valid_indices = [] # Reset per batch\n",
        "        if classifier_head is not None:\n",
        "            try: # Process batch\n",
        "                feature_list = []\n",
        "                for i, f in enumerate(features_series.tolist()):\n",
        "                    if f is not None:\n",
        "                        vec = np.array(f); current_len = len(vec)\n",
        "                        if expected_len == -1: expected_len = current_len\n",
        "                        if current_len == expected_len: feature_list.append(vec); valid_indices.append(i)\n",
        "                        # else: print(f\"WARN: Skip vector len {current_len} != exp {expected_len}\") # Noisy\n",
        "                    # else: print(\"WARN: Skip None vector\") # Noisy\n",
        "                if not feature_list: yield pd.Series([0.0] * len(features_series), dtype=float); continue\n",
        "                # Check shapes before stacking\n",
        "                first_shape = feature_list[0].shape\n",
        "                if not all(arr.shape == first_shape for arr in feature_list):\n",
        "                    print(f\"WARN: Inconsistent shapes. Default preds for batch.\"); yield pd.Series([0.0] * len(features_series), dtype=float); continue\n",
        "\n",
        "                feature_vectors = np.stack(feature_list); batch_tensor = torch.tensor(feature_vectors, dtype=torch.float32).to(device)\n",
        "                if batch_tensor.shape[1] < embedding_dim: print(f\"WARN: Feat vector len {batch_tensor.shape[1]} < embed dim {embedding_dim}. Default preds.\"); yield pd.Series([0.0] * len(features_series), dtype=float); continue\n",
        "                bert_embeddings = batch_tensor[:, :embedding_dim]\n",
        "                with torch.no_grad(): logits = classifier_head(embeddings=bert_embeddings); predictions = torch.argmax(logits, dim=1).cpu().numpy().astype(float)\n",
        "                # Map predictions back\n",
        "                results_array = np.full(len(features_series), 0.0)\n",
        "                for i, pred_idx in enumerate(valid_indices): results_array[pred_idx] = predictions[i]\n",
        "                results = results_array.tolist()\n",
        "            except Exception as e: print(f\"Worker ERROR pred batch: {e}\"); results = [0.0] * len(features_series)\n",
        "        else: results = np.random.binomial(1, 0.3, len(features_series)).astype(float).tolist() # Fallback\n",
        "        yield pd.Series(results, dtype=float)\n",
        "\n",
        "def run_streaming_inference_job(pipeline_model_path):\n",
        "    print(\"\\n--- Task 7: Streaming Inference Job ---\")\n",
        "    try: print(f\"Loading pipeline: {pipeline_model_path}\"); pipeline_model = PipelineModel.load(pipeline_model_path); print(\"Pipeline loaded.\")\n",
        "    except Exception as e: print(f\"ERROR loading pipeline: {e}\"); traceback.print_exc(); return None, None, None # Return None for all queries\n",
        "\n",
        "    # Define metrics listener instance here to close it later if needed\n",
        "    metrics_listener_instance = MetricsListener(KAFKA_BROKERS, KAFKA_TOPIC_METRICS)\n",
        "    spark.streams.addListener(metrics_listener_instance)\n",
        "\n",
        "    stream_schema = get_streaming_schema(); print(f\"Setting up stream source: {KAFKA_TOPIC_TESTING_STREAM}\")\n",
        "    kafka_stream = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", KAFKA_BROKERS).option(\"subscribe\", KAFKA_TOPIC_TESTING_STREAM).option(\"startingOffsets\", \"latest\").option(\"failOnDataLoss\", \"false\").load()\n",
        "    parsed_stream = kafka_stream.select(F.from_json(F.col(\"value\").cast(\"string\"), stream_schema).alias(\"data\")).select(\"data.*\").na.drop(subset=[\"Complaint ID\"])\n",
        "    processed_stream = pipeline_model.transform(parsed_stream)\n",
        "    prediction_stream = processed_stream.filter(F.col(\"features\").isNotNull()).withColumn(\"prediction\", predict_udf(F.col(\"features\")))\n",
        "    final_stream = prediction_stream.select( F.col(\"Complaint ID\").alias(\"complaint_id\"), F.col(\"prediction\"), F.col(\"State\").alias(\"state\"), F.col(\"ZIP code\").alias(\"zip_code\"), F.col(\"Submitted via\").alias(\"submitted_via\"), F.current_timestamp().alias(\"inference_time\")).withColumn(\"inference_time_str\", F.date_format(\"inference_time\", \"yyyy-MM-dd HH:mm:ss\"))\n",
        "\n",
        "    # Sink 1: Kafka\n",
        "    kafka_output = final_stream.selectExpr(\"complaint_id AS key\", \"to_json(struct(*)) AS value\")\n",
        "    print(f\"Starting Kafka sink query -> {KAFKA_TOPIC_PREDICTIONS}\")\n",
        "    kafka_query = kafka_output.writeStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", KAFKA_BROKERS).option(\"topic\", KAFKA_TOPIC_PREDICTIONS).option(\"checkpointLocation\", f\"{STREAMING_CHECKPOINT_LOCATION}/predictions_kafka\").outputMode(\"append\").trigger(processingTime=\"10 seconds\").queryName(\"kafka_predictions_sink\").start()\n",
        "\n",
        "    # Sink 2: Console\n",
        "    print(\"Starting Console sink query...\")\n",
        "    console_query = final_stream.writeStream.format(\"console\").option(\"truncate\", \"false\").option(\"numRows\", 5).trigger(processingTime=\"15 seconds\").outputMode(\"append\").queryName(\"console_sink\").start()\n",
        "\n",
        "    # Sink 3: Memory (for Superset Demo)\n",
        "    print(f\"Starting Memory sink query -> table '{PREDICTIONS_MEM_TABLE}'\")\n",
        "    memory_query = final_stream.writeStream.format(\"memory\").queryName(PREDICTIONS_MEM_TABLE).outputMode(\"append\").trigger(processingTime=\"10 seconds\").start()\n",
        "\n",
        "    print(\"Streaming queries started.\")\n",
        "    # Return all queries and the listener instance\n",
        "    return kafka_query, console_query, memory_query, metrics_listener_instance"
      ],
      "metadata": {
        "id": "5HoknUAWaUc5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Main Execution ---\n",
        "def main():\n",
        "    print(\"\\n--- Starting Main Pipeline Execution ---\")\n",
        "    start_pipeline_time = time.time()\n",
        "    metrics_listener_main = None\n",
        "    pipeline_failed = False\n",
        "    active_queries = []\n",
        "\n",
        "    try:\n",
        "        print(\"Checking Kafka connection...\");\n",
        "        try: temp_producer = KafkaProducer(bootstrap_servers=KAFKA_BROKERS.split(','), request_timeout_ms=5000); temp_producer.close(); print(\"Kafka ok.\")\n",
        "        except kafka_errors.NoBrokersAvailable: print(f\"FATAL: Kafka connect failed at {KAFKA_BROKERS}. Check server.\"); return\n",
        "\n",
        "        # Task 1\n",
        "        load_data_to_kafka()\n",
        "\n",
        "        # Task 2\n",
        "        filtered_df = preprocess_filter_and_visualize()\n",
        "        if filtered_df is None: raise ValueError(\"Preprocessing failed.\")\n",
        "\n",
        "        # Task 3\n",
        "        training_df_cached, _ = split_label_and_prepare_data(filtered_df)\n",
        "        if training_df_cached is None: raise ValueError(\"Split/Label failed.\")\n",
        "\n",
        "        # Task 4\n",
        "        model_path, tokenizer = train_bert_model_distributed()\n",
        "        if model_path is None or tokenizer is None: raise RuntimeError(\"Training failed.\")\n",
        "        print(f\"Model training ok. Path: {model_path}\")\n",
        "\n",
        "        # Task 5\n",
        "        print(\"Fitting pipeline...\"); pipeline = create_unified_pipeline()\n",
        "        # Use a slightly larger sample for fitting, ensure necessary columns are present\n",
        "        fit_cols = [\"Consumer complaint narrative\", \"Company\", \"State\", \"Submitted via\", \"ZIP code\"] # Cols needed by pipeline stages\n",
        "        fit_sample_df = training_df_cached.select(*fit_cols).limit(500) # Increased sample\n",
        "        pipeline_model = pipeline.fit(fit_sample_df)\n",
        "        pipeline_model.write().overwrite().save(TRAINING_PIPELINE_SAVE_PATH)\n",
        "        print(f\"Fitted pipeline saved: {TRAINING_PIPELINE_SAVE_PATH}\")\n",
        "        if training_df_cached.is_cached: training_df_cached.unpersist()\n",
        "\n",
        "        # Task 6\n",
        "        simulate_test_data_to_kafka()\n",
        "\n",
        "        # Task 7\n",
        "        kafka_q, console_q, memory_q, metrics_listener_main = run_streaming_inference_job(TRAINING_PIPELINE_SAVE_PATH)\n",
        "        if kafka_q is None: raise RuntimeError(\"Failed to start streaming.\")\n",
        "        active_queries.extend([q for q in [kafka_q, console_q, memory_q] if q is not None])\n",
        "\n",
        "        print(\"\\n--- Pipeline Running ---\"); print(\">>> Press Ctrl+C in Colab cell to stop. <<<\")\n",
        "        # Wait for termination or interruption\n",
        "        # This simplified loop allows interruption\n",
        "        while any(q.isActive for q in active_queries): time.sleep(5)\n",
        "        print(\"Streaming queries seem to have stopped naturally or were stopped externally.\")\n",
        "\n",
        "    except KeyboardInterrupt: print(\"\\nKeyboardInterrupt. Stopping...\"); pipeline_failed = True\n",
        "    except Exception as e: print(f\"\\nFATAL ERROR: {e}\"); traceback.print_exc(); pipeline_failed = True\n",
        "    finally:\n",
        "        print(\"\\n--- Cleaning up ---\"); stopped_count = 0\n",
        "        # Check active streams using spark context\n",
        "        for q in spark.streams.active:\n",
        "             print(f\"Stopping query '{q.name}' (id: {q.id})...\")\n",
        "             try: q.stop(); stopped_count += 1; print(\"Stopped.\")\n",
        "             except Exception as stop_e: print(f\"Error stopping query '{q.name}': {stop_e}\")\n",
        "        print(f\"Stopped {stopped_count} active streaming queries.\")\n",
        "        if metrics_listener_main: metrics_listener_main.close_producer() # Close producer explicitly\n",
        "        end_pipeline_time = time.time()\n",
        "        print(f\"\\nTotal Pipeline Time: {end_pipeline_time - start_pipeline_time:.2f}s\")\n",
        "        status = \"errors or interrupted\" if pipeline_failed else \"successfully (streaming stopped)\"\n",
        "        print(f\"Pipeline finished {status}.\")\n",
        "\n",
        "# --- Run Main ---\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# --- Post-Execution: Query Memory Table & Superset/Grafana Guidance ---\n",
        "print(\"\\n--- Post Execution ---\")\n",
        "print(f\"To view predictions collected in the memory table (if streaming ran):\")\n",
        "print(f\">>> spark.sql(\\\"SELECT * FROM {PREDICTIONS_MEM_TABLE} ORDER BY inference_time DESC LIMIT 20\\\").show(truncate=False)\")\n",
        "\n",
        "print(\"\\n--- Superset Integration Guidance ---\")\n",
        "print(\"1. Install and run Superset separately (Docker or local install recommended).\")\n",
        "print(f\"2. Add a Database Connection in Superset:\")\n",
        "print(f\"   - OPTION A (Recommended): Set up a persistent database (e.g., PostgreSQL, ClickHouse). Modify the Spark streaming job (Task 7) to write to this database using JDBC instead of/besides the 'memory' sink.\")\n",
        "print(f\"   - OPTION B (Advanced Demo): Set up Spark Thrift Server connected to your running Spark Session (difficult in Colab). Connect Superset to Spark SQL via Thrift JDBC/ODBC driver. Query the '{PREDICTIONS_MEM_TABLE}'.\")\n",
        "print(f\"   - OPTION C (Manual Load): Periodically run `spark.table('{PREDICTIONS_MEM_TABLE}').write.jdbc(...)` or save to files and load into your target DB.\")\n",
        "print(\"3. Create a Dataset in Superset based on the predictions table/view.\")\n",
        "print(\"4. Build dashboards visualizing 'prediction' counts/trends by 'state', 'submitted_via', 'inference_time_str'.\")\n",
        "\n",
        "print(\"\\n--- Grafana Integration Guidance (Kafka KPIs) ---\")\n",
        "print(\"1. Standard method requires external setup (not feasible in this Colab script):\")\n",
        "print(\"   - Download Prometheus JMX Exporter JAR.\")\n",
        "print(\"   - Modify `kafka-server-start.sh` in your external Kafka setup to run the JMX Exporter as a Java agent (`-javaagent:/path/to/jmx_prometheus_javaagent.jar=PORT:config.yaml`).\")\n",
        "print(\"   - Configure Prometheus to scrape the JMX Exporter's HTTP endpoint.\")\n",
        "print(\"   - Configure Grafana with Prometheus as a data source.\")\n",
        "print(\"   - Import a Kafka dashboard template into Grafana or build custom panels.\")\n",
        "print(\"2. Alternative using Spark Metrics:\")\n",
        "print(f\"   - Metrics about the Spark *streaming job* (rates, batch duration) were sent to Kafka topic: '{KAFKA_TOPIC_METRICS}'.\")\n",
        "print(f\"   - Set up a separate consumer for '{KAFKA_TOPIC_METRICS}'.\")\n",
        "print(f\"   - Ingest these metrics into a database Grafana can query (e.g., InfluxDB, PostgreSQL).\")\n",
        "print(f\"   - Build Grafana dashboards querying this database for Spark streaming performance.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amkd1CmqkvrV",
        "outputId": "800b66f2-141b-4bde-d538-7717030115eb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:kafka.conn:Connect attempt to <BrokerConnection client_id=kafka-python-producer-1, node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 111. Disconnecting.\n",
            "ERROR:kafka.conn:<BrokerConnection client_id=kafka-python-producer-1, node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. KafkaConnectionError: 111 ECONNREFUSED\n",
            "ERROR:kafka.conn:Connect attempt to <BrokerConnection client_id=kafka-python-producer-1, node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]> returned error 99. Disconnecting.\n",
            "ERROR:kafka.conn:<BrokerConnection client_id=kafka-python-producer-1, node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. KafkaConnectionError: 99 EADDRNOTAVAIL\n",
            "WARNING:kafka.client:No node available during check_version; sleeping 0.05 secs\n",
            "ERROR:kafka.conn:Connect attempt to <BrokerConnection client_id=kafka-python-producer-1, node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 111. Disconnecting.\n",
            "ERROR:kafka.conn:<BrokerConnection client_id=kafka-python-producer-1, node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. KafkaConnectionError: 111 ECONNREFUSED\n",
            "ERROR:kafka.conn:Connect attempt to <BrokerConnection client_id=kafka-python-producer-1, node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]> returned error 99. Disconnecting.\n",
            "ERROR:kafka.conn:<BrokerConnection client_id=kafka-python-producer-1, node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. KafkaConnectionError: 99 EADDRNOTAVAIL\n",
            "WARNING:kafka.client:No node available during check_version; sleeping 0.08 secs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Main Pipeline Execution ---\n",
            "Checking Kafka connection...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:kafka.conn:Connect attempt to <BrokerConnection client_id=kafka-python-producer-1, node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 111. Disconnecting.\n",
            "ERROR:kafka.conn:<BrokerConnection client_id=kafka-python-producer-1, node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. KafkaConnectionError: 111 ECONNREFUSED\n",
            "ERROR:kafka.conn:Connect attempt to <BrokerConnection client_id=kafka-python-producer-1, node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]> returned error 99. Disconnecting.\n",
            "ERROR:kafka.conn:<BrokerConnection client_id=kafka-python-producer-1, node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. KafkaConnectionError: 99 EADDRNOTAVAIL\n",
            "WARNING:kafka.client:No node available during check_version; sleeping 0.18 secs\n",
            "ERROR:kafka.conn:Connect attempt to <BrokerConnection client_id=kafka-python-producer-1, node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 111. Disconnecting.\n",
            "ERROR:kafka.conn:<BrokerConnection client_id=kafka-python-producer-1, node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. KafkaConnectionError: 111 ECONNREFUSED\n",
            "ERROR:kafka.conn:Connect attempt to <BrokerConnection client_id=kafka-python-producer-1, node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]> returned error 99. Disconnecting.\n",
            "ERROR:kafka.conn:<BrokerConnection client_id=kafka-python-producer-1, node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. KafkaConnectionError: 99 EADDRNOTAVAIL\n",
            "WARNING:kafka.client:No node available during check_version; sleeping 0.35 secs\n",
            "ERROR:kafka.conn:Connect attempt to <BrokerConnection client_id=kafka-python-producer-1, node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 111. Disconnecting.\n",
            "ERROR:kafka.conn:<BrokerConnection client_id=kafka-python-producer-1, node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. KafkaConnectionError: 111 ECONNREFUSED\n",
            "ERROR:kafka.conn:Connect attempt to <BrokerConnection client_id=kafka-python-producer-1, node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]> returned error 99. Disconnecting.\n",
            "ERROR:kafka.conn:<BrokerConnection client_id=kafka-python-producer-1, node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. KafkaConnectionError: 99 EADDRNOTAVAIL\n",
            "WARNING:kafka.client:No node available during check_version; sleeping 0.87 secs\n",
            "ERROR:kafka.conn:Connect attempt to <BrokerConnection client_id=kafka-python-producer-1, node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 111. Disconnecting.\n",
            "ERROR:kafka.conn:<BrokerConnection client_id=kafka-python-producer-1, node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. KafkaConnectionError: 111 ECONNREFUSED\n",
            "ERROR:kafka.conn:Connect attempt to <BrokerConnection client_id=kafka-python-producer-1, node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]> returned error 99. Disconnecting.\n",
            "ERROR:kafka.conn:<BrokerConnection client_id=kafka-python-producer-1, node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. KafkaConnectionError: 99 EADDRNOTAVAIL\n",
            "WARNING:kafka.client:No node available during check_version; sleeping 0.43 secs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FATAL: Kafka connect failed at localhost:9092. Check server.\n",
            "\n",
            "--- Cleaning up ---\n",
            "Stopped 0 active streaming queries.\n",
            "\n",
            "Total Pipeline Time: 2.07s\n",
            "Pipeline finished successfully (streaming stopped).\n",
            "\n",
            "--- Post Execution ---\n",
            "To view predictions collected in the memory table (if streaming ran):\n",
            ">>> spark.sql(\"SELECT * FROM predictions_mem_table ORDER BY inference_time DESC LIMIT 20\").show(truncate=False)\n",
            "\n",
            "--- Superset Integration Guidance ---\n",
            "1. Install and run Superset separately (Docker or local install recommended).\n",
            "2. Add a Database Connection in Superset:\n",
            "   - OPTION A (Recommended): Set up a persistent database (e.g., PostgreSQL, ClickHouse). Modify the Spark streaming job (Task 7) to write to this database using JDBC instead of/besides the 'memory' sink.\n",
            "   - OPTION B (Advanced Demo): Set up Spark Thrift Server connected to your running Spark Session (difficult in Colab). Connect Superset to Spark SQL via Thrift JDBC/ODBC driver. Query the 'predictions_mem_table'.\n",
            "   - OPTION C (Manual Load): Periodically run `spark.table('predictions_mem_table').write.jdbc(...)` or save to files and load into your target DB.\n",
            "3. Create a Dataset in Superset based on the predictions table/view.\n",
            "4. Build dashboards visualizing 'prediction' counts/trends by 'state', 'submitted_via', 'inference_time_str'.\n",
            "\n",
            "--- Grafana Integration Guidance (Kafka KPIs) ---\n",
            "1. Standard method requires external setup (not feasible in this Colab script):\n",
            "   - Download Prometheus JMX Exporter JAR.\n",
            "   - Modify `kafka-server-start.sh` in your external Kafka setup to run the JMX Exporter as a Java agent (`-javaagent:/path/to/jmx_prometheus_javaagent.jar=PORT:config.yaml`).\n",
            "   - Configure Prometheus to scrape the JMX Exporter's HTTP endpoint.\n",
            "   - Configure Grafana with Prometheus as a data source.\n",
            "   - Import a Kafka dashboard template into Grafana or build custom panels.\n",
            "2. Alternative using Spark Metrics:\n",
            "   - Metrics about the Spark *streaming job* (rates, batch duration) were sent to Kafka topic: 'streaming-metrics'.\n",
            "   - Set up a separate consumer for 'streaming-metrics'.\n",
            "   - Ingest these metrics into a database Grafana can query (e.g., InfluxDB, PostgreSQL).\n",
            "   - Build Grafana dashboards querying this database for Spark streaming performance.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
