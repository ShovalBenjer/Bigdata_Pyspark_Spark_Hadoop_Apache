{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShovalBenjer/Bigdata_Pyspark_Spark_Hadoop_Apache/blob/main/ex_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TL;DR\n",
        "Collaborators: Shoval Benjer 319037404, Adir Amar 209017755\n",
        "\n",
        "The Kafka-Spark pipeline successfully consumed sentiment data from the sentiments topic, processing words and their sentiment scores in real time. After receiving 10 messages, the consumer stopped as configured, and producer threads were gracefully terminated. This demonstrates the system's ability to handle streaming data efficiently within predefined limits."
      ],
      "metadata": {
        "id": "y6ymplfFAK_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Setup**\n",
        "\n",
        "**System Requirements:**\n",
        "\n",
        "    Operating System: Linux-based environment (recommended for compatibility) or Windows with WSL2.\n",
        "\n",
        "Software:\n",
        "    Python 3.8+, Java 8 (OpenJDK 8), and Apache Spark.\n",
        "\n",
        "Libraries:\n",
        "\n",
        "    pyspark, kafka-python, threading, and json.\n",
        "\n",
        "Environment Setup:\n",
        "\n",
        "**Google Colab is Recommended for running the notebook.**\n",
        "\n",
        "Local System: Ensure you have Apache Spark and Kafka installed with appropriate environment variables configured.\n",
        "\n",
        "\n",
        "Description for Each Step:\n",
        "\n",
        "      Install Java:\n",
        "      This command installs the OpenJDK 8 runtime environment, a necessary dependency for running Apache Spark and Kafka. The -qq flag minimizes output during the installation process.\n",
        "\n",
        "      Download Apache Spark:\n",
        "      Downloads Apache Spark version 3.5.0 with Hadoop 3 compatibility from the official Apache archives. Spark is a distributed computing framework essential for big data processing tasks.\n",
        "\n",
        "      Verify the Spark Download:\n",
        "      Lists the downloaded Spark tarball to confirm that the file has been successfully downloaded.\n",
        "\n",
        "      Extract the Spark Archive:\n",
        "      Unpacks the Spark tarball to make the Spark distribution files accessible for configuration and usage.\n",
        "\n",
        "      Move Spark to the Local Directory:\n",
        "      Moves the extracted Spark directory to /usr/local/spark, setting a standard location for Spark installation, simplifying environment variable configuration.\n",
        "\n",
        "      Download Apache Kafka:\n",
        "      Downloads Apache Kafka version 3.5.1 (Scala version 2.13), a distributed event-streaming platform commonly used for real-time data pipelines and streaming applications.\n",
        "\n",
        "      Verify the Kafka Download:\n",
        "      Lists the downloaded Kafka tarball to ensure successful file retrieval.\n",
        "\n",
        "      Extract the Kafka Archive:\n",
        "      Unpacks the Kafka tarball to access its binaries and configuration files.\n",
        "\n",
        "      Move Kafka to the Local Directory:\n",
        "      Moves the extracted Kafka directory to /usr/local/kafka for organized setup and easier configuration.\n",
        "\n",
        "      Set Environment Variables:\n",
        "      Configures environment variables for Java, Spark, and Kafka to ensure their executables can be accessed system-wide. This includes updating the PATH variable for seamless command-line operations.\n",
        "\n",
        "      Install Python Libraries:\n",
        "      Installs pyspark for interacting with Spark using Python and kafka-python for Kafka integration within Python applications.\n",
        "\n",
        "      Start Zookeeper:\n",
        "      Launches Zookeeper, a centralized service used by Kafka for managing distributed systems. It provides configuration synchronization and group services for Kafka brokers.\n",
        "\n",
        "      Start Kafka Broker:\n",
        "      Starts the Kafka broker service, which handles message queuing, storage, and distribution to clients in a publish-subscribe model."
      ],
      "metadata": {
        "id": "Ej_040DT-oLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!ls -l spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "!mv spark-3.5.0-bin-hadoop3 /usr/local/spark\n",
        "!wget -q https://archive.apache.org/dist/kafka/3.5.1/kafka_2.13-3.5.1.tgz\n",
        "!ls -l kafka_2.13-3.5.1.tgz\n",
        "!tar xf kafka_2.13-3.5.1.tgz\n",
        "!mv kafka_2.13-3.5.1 /usr/local/kafka\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/usr/local/spark\"\n",
        "os.environ[\"PATH\"] += \":/usr/local/spark/bin\"\n",
        "os.environ[\"PATH\"] += \":/usr/local/kafka/bin\"\n",
        "!pip install pyspark kafka-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5x2Y8YUGd4d",
        "outputId": "a878b9ef-4d43-4e51-8986-aad60e5a3ad6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 400395283 Sep  9  2023 spark-3.5.0-bin-hadoop3.tgz\n",
            "-rw-r--r-- 1 root root 106748875 Jul 21  2023 kafka_2.13-3.5.1.tgz\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.4)\n",
            "Requirement already satisfied: kafka-python in /usr/local/lib/python3.10/dist-packages (2.0.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup /usr/local/kafka/bin/zookeeper-server-start.sh /usr/local/kafka/config/zookeeper.properties &\n",
        "!nohup /usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server.properties &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDIMy5y0EJDW",
        "outputId": "578321c4-7fbc-4532-9e41-97d63cd633f5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Kafka Topics for Data Streams\n",
        "\n",
        "    This name accurately reflects the purpose of the snippet, which is to create Kafka topics (sentiments and text) for managing separate streams of data within the Kafka ecosystem."
      ],
      "metadata": {
        "id": "aA2JRESP_IPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kafka-topics.sh --create --topic sentiments --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1\n",
        "!kafka-topics.sh --create --topic text --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1"
      ],
      "metadata": {
        "id": "_g9pMW9zGGSx",
        "outputId": "f7967dac-9d38-436e-f7bc-08f8216361e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error while executing topic command : Topic 'sentiments' already exists.\n",
            "[2025-01-02 10:57:13,482] ERROR org.apache.kafka.common.errors.TopicExistsException: Topic 'sentiments' already exists.\n",
            " (kafka.admin.TopicCommand$)\n",
            "Error while executing topic command : Topic 'text' already exists.\n",
            "[2025-01-02 10:57:15,241] ERROR org.apache.kafka.common.errors.TopicExistsException: Topic 'text' already exists.\n",
            " (kafka.admin.TopicCommand$)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Producer for Sentiments (producer_sentiments):**\n",
        "\n",
        "    This function reads sentiment data from a file (e.g., AFINN-111.txt) and continuously sends key-value pairs representing words and their sentiment scores to a specified Kafka topic. It simulates real-time streaming by batching the data and rotating through the dataset. The Kafka producer uses JSON serialization to encode messages before sending them to the topic.\n",
        "\n",
        "**Producer for Text (producer_text):**\n",
        "\n",
        "    This function allows users to input text sentences via the console and sends them to a specified Kafka topic in real time. It uses a Kafka producer to serialize the text and send it as a message. This enables dynamic user interaction and real-time data streaming for text analysis.\n",
        "\n",
        "**Spark Kafka Consumer (spark_kafka_consumer):**\n",
        "\n",
        "    This function consumes messages from two Kafka topics: one for sentiment data and another for user text input. Using Apache Spark, it processes these messages to calculate the Total Sentiment Level (TSL) for user input based on the sentiment data. The consumer maintains a dictionary of word sentiments and updates it dynamically from the sentiment topic. It evaluates each input sentence for known sentiment words, computes the TSL, and outputs the results. The function also includes safeguards for JSON decoding and message processing errors, with a configurable limit for the number of messages to process before stopping."
      ],
      "metadata": {
        "id": "2M5wDhNx_Vnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from kafka import KafkaProducer\n",
        "import json\n",
        "import time\n",
        "import threading\n",
        "\n",
        "def producer_sentiments(file_path, topic, stop_event, bootstrap_servers='localhost:9092'):\n",
        "    \"\"\"\n",
        "    Reads word-sentiment pairs from AFINN and sends them to the `sentiments` Kafka topic.\n",
        "    Streams 100 word-sentiment pairs every 2 seconds.\n",
        "    \"\"\"\n",
        "    producer = KafkaProducer(\n",
        "        bootstrap_servers=bootstrap_servers,\n",
        "        value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
        "    )\n",
        "    with open(file_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    sentiment_data = [(line.split('\\t')[0], int(line.split('\\t')[1])) for line in lines]\n",
        "\n",
        "    try:\n",
        "        while not stop_event.is_set():\n",
        "            batch = sentiment_data[:100]\n",
        "            for word, sentiment in batch:\n",
        "                producer.send(topic, {'word': word, 'sentiment': sentiment})\n",
        "            sentiment_data = sentiment_data[100:] + batch\n",
        "            time.sleep(2)\n",
        "    except Exception as e:\n",
        "        print(f\"[producer_sentiments] Error: {e}\")\n",
        "    finally:\n",
        "        producer.close()\n",
        "        print(\"[producer_sentiments] Exiting...\")\n",
        "\n",
        "\n",
        "def producer_text(topic, stop_event, bootstrap_servers='localhost:9092'):\n",
        "    \"\"\"\n",
        "    Takes user input from the console and sends sentences to the `text` Kafka topic.\n",
        "    \"\"\"\n",
        "    producer = KafkaProducer(\n",
        "        bootstrap_servers=bootstrap_servers,\n",
        "        value_serializer=lambda v: v.encode('utf-8')\n",
        "    )\n",
        "    try:\n",
        "        while not stop_event.is_set():\n",
        "            user_input = input(\"Enter a sentence to analyze (Ctrl+C to stop): \")\n",
        "            if user_input.strip():\n",
        "                producer.send(topic, user_input)\n",
        "                print(f\"[producer_text] Sent: {user_input}\")\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n[producer_text] Interrupted by user.\")\n",
        "    except Exception as e:\n",
        "        print(f\"[producer_text] Error: {e}\")\n",
        "    finally:\n",
        "        producer.close()\n",
        "        print(\"[producer_text] Exiting...\")\n",
        "\n"
      ],
      "metadata": {
        "id": "F6viv9wHB968"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kafka import KafkaConsumer\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "def spark_kafka_consumer(bootstrap_servers='localhost:9092',\n",
        "                         sentiment_topic='sentiments',\n",
        "                         text_topic='text',\n",
        "                         stop_event=None):\n",
        "    \"\"\"\n",
        "    Consumes from the `sentiments` and `text` Kafka topics.\n",
        "    Calculates and displays the Total Sentiment Level (TSL) for each new sentence in `text`.\n",
        "    \"\"\"\n",
        "    spark = SparkSession.builder.appName(\"KafkaSparkConsumer\").getOrCreate()\n",
        "    sc = spark.sparkContext\n",
        "\n",
        "    consumer = KafkaConsumer(\n",
        "        sentiment_topic,\n",
        "        text_topic,\n",
        "        bootstrap_servers=bootstrap_servers,\n",
        "        value_deserializer=lambda v: v.decode('utf-8'),\n",
        "        auto_offset_reset='earliest',\n",
        "        enable_auto_commit=True\n",
        "    )\n",
        "\n",
        "    sentiment_dict = {}\n",
        "\n",
        "    try:\n",
        "        for message in consumer:\n",
        "            if stop_event and stop_event.is_set():\n",
        "                print(\"[spark_kafka_consumer] Stop event detected. Exiting loop.\")\n",
        "                break\n",
        "\n",
        "            topic = message.topic\n",
        "            value = message.value\n",
        "\n",
        "            if topic == sentiment_topic:\n",
        "                # Update sentiment dictionary\n",
        "                sentiment_data = json.loads(value)\n",
        "                sentiment_dict[sentiment_data['word']] = sentiment_data['sentiment']\n",
        "\n",
        "            elif topic == text_topic:\n",
        "                # Calculate TSL for the new sentence\n",
        "                words = value.split()\n",
        "                words_rdd = sc.parallelize(words)\n",
        "                known_sentiments_rdd = words_rdd.filter(lambda word: word in sentiment_dict)\\\n",
        "                                                .map(lambda word: sentiment_dict[word])\n",
        "                known_sentiments = known_sentiments_rdd.collect()\n",
        "\n",
        "                if known_sentiments:\n",
        "                    tsl = sum(known_sentiments) / len(known_sentiments)\n",
        "                    print(f\"[spark_kafka_consumer] TSL for \\\"{value}\\\": {tsl:.2f}\")\n",
        "                else:\n",
        "                    print(f\"[spark_kafka_consumer] TSL for \\\"{value}\\\": 0 (No known words)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[spark_kafka_consumer] Error: {e}\")\n",
        "    finally:\n",
        "        consumer.close()\n",
        "        sc.stop()\n",
        "        print(\"[spark_kafka_consumer] Exiting...\")\n"
      ],
      "metadata": {
        "id": "JAyEZonkqJ9z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_event = threading.Event()\n",
        "\n",
        "# Start the sentiment producer\n",
        "producer_sentiments_thread = threading.Thread(\n",
        "    target=producer_sentiments,\n",
        "    args=(\"AFINN-111.txt\", \"sentiments\", stop_event),\n",
        "    name=\"ProducerSentimentsThread\"\n",
        ")\n",
        "producer_sentiments_thread.start()\n",
        "\n",
        "# Start the text producer\n",
        "producer_text_thread = threading.Thread(\n",
        "    target=producer_text,\n",
        "    args=(\"text\", stop_event),\n",
        "    name=\"ProducerTextThread\"\n",
        ")\n",
        "producer_text_thread.start()\n",
        "\n",
        "# Start the Spark Kafka consumer\n",
        "consumer_thread = threading.Thread(\n",
        "    target=spark_kafka_consumer,\n",
        "    kwargs={\n",
        "        \"bootstrap_servers\": \"localhost:9092\",\n",
        "        \"sentiment_topic\": \"sentiments\",\n",
        "        \"text_topic\": \"text\",\n",
        "        \"stop_event\": stop_event\n",
        "    },\n",
        "    name=\"SparkKafkaConsumerThread\"\n",
        ")\n",
        "consumer_thread.start()\n"
      ],
      "metadata": {
        "id": "UBbErK7pqKiK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_event.set()\n",
        "producer_sentiments_thread.join()\n",
        "producer_text_thread.join()\n",
        "consumer_thread.join()\n",
        "print(\"[main] All threads have been stopped. Exiting program.\")"
      ],
      "metadata": {
        "id": "ZyuO-86PqOvF",
        "outputId": "cc3f3efa-67a2-440f-b3d5-d203080b35b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[producer_text] Exiting...\n",
            "[producer_sentiments] Exiting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:kafka.coordinator.consumer:group_id is None: disabling auto-commit.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[spark_kafka_consumer] Stop event detected. Exiting loop.\n",
            "[spark_kafka_consumer] Exiting...\n",
            "[main] All threads have been stopped. Exiting program.\n"
          ]
        }
      ]
    }
  ]
}
