{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solosolve AI Big Data Pipeline\n",
    "\n",
    "## Architecture Overview\n",
    "```\n",
    "┌─────────────────┐        ┌─────────────────┐        ┌───────────────┐\n",
    "│                 │        │ Kafka Setup/Sim │        │ Spark Batch   │\n",
    "│  Data Sources   │───────▶│  - Topic Init │───────▶│ Processing    │\n",
    "│  - CSV          │        │  - Producer Sim│        │  (Sampled CSV)│\n",
    "│  - Simulation   │        │                 │        │               │\n",
    "└─────────────────┘        └─────────────────┘        └───────┬───────┘\n",
    "      │                                                       │ ▲\n",
    "      │ Simulation                                            │ │ Fallback\n",
    "      ▼                                                       ▼ │ Data\n",
    "┌─────────────────┐        ┌───────────────┐        ┌───────────────┐\n",
    "│                 │        │               │        │               │\n",
    "│ Dash Dashboard  │◀───────│ Model         │◀───────│ AFE Pipeline  │\n",
    "│  - Metrics      │        │ Training (GBT)│        │  - HashingTF  │\n",
    "│  - Charts       │        └───────┬───────┘        │  - OHE        │\n",
    "└────────┬────────┘                │                └───────────────┘\n",
    "         │                         │                        ▲\n",
    "         │ Simulation              │ Model Loading          │\n",
    "         ▼                         │                        │\n",
    "┌─────────────────┐        ┌─────────────────┐        ┌───────────────┐\n",
    "│                 │        │   Simulated     │        │ Continuous    │\n",
    "│  (Not Impl.)    │<───────│   Streaming     │<───────│ Learning Goal │\n",
    "│  User Feedback  │        │   Inference     │        │ (Retraining)  │\n",
    "└─────────────────┘        └─────────────────┘        └───────────────┘\n",
    "```\n",
    "\n",
    "\n",
    "## Component Breakdown\n",
    "\n",
    "### 1. Environment Setup\n",
    "-   **Spark:** Initialized via `initialize_spark()` with specific memory (`driver=3g`, `executor=2g`), parallelism, and Kryo serialization settings for optimization. Uses `SparkSession.builder`. `psutil` optionally used for memory info.\n",
    "-   **Kafka:**\n",
    "    -   Topics setup via `setup_kafka_topics()` using `kafka-python`'s `KafkaAdminClient` & `NewTopic` (if Kafka available). Defines topics like `consumer-complaints-raw`.\n",
    "    -   Producer simulation via `kafka_producer_job()` using `KafkaProducer` (if Kafka available). *Note: Main inference uses local simulation.*\n",
    "-   **MLflow:** Used for tracking (`mlflow.log_metric`, `log_param`) and model logging/registry (`mlflow.spark.log_model`) if available (`MLFLOW_AVAILABLE` flag).\n",
    "-   **Dashboard:** Uses `Dash`, `Plotly`, `dcc`, `html` for UI and visualization if available (`DASH_AVAILABLE` flag). Served via `JupyterDash`. `threading.Lock` used for safe data updates.\n",
    "\n",
    "### 2. Core Pipeline Functions\n",
    "\n",
    "#### Data Handling & Labeling\n",
    "-   **Loading:** `load_data_optimized()` reads CSV (`spark.read.csv`) using an explicit `COMPLAINT_SCHEMA`, applies sampling (`.sample()`), persists (`.persist(pyspark.StorageLevel.MEMORY_AND_DISK)`), and has a fallback to `create_simulation_data()`.\n",
    "-   **Labeling:** `create_label_column()` uses `pyspark.sql.functions.when` and `col` to derive the binary `is_successful_resolution` target variable based on specific conditions.\n",
    "\n",
    "#### Feature Engineering (AFE)\n",
    "-   Implemented in `create_feature_pipeline()` and applied via `apply_feature_engineering()`. Uses `pyspark.ml.Pipeline`.\n",
    "-   **Text:** `length()` calculates narrative length. `Tokenizer` splits text, `StopWordsRemover` filters common words, `HashingTF` converts tokens into fixed-size vectors.\n",
    "-   **Categorical:** Uses `StringIndexer` (maps strings to indices) and `OneHotEncoder` (converts indices to sparse vectors). Attempts optimization using `Window` functions (`row_number`) to find top categories before encoding.\n",
    "-   **Date:** `to_timestamp()` parses date strings, `month()` extracts the month feature.\n",
    "-   **Assembly:** `VectorAssembler` combines all generated features into a single `features` vector column. Includes fallback to simpler features (`narrative_length` only) on error.\n",
    "\n",
    "#### Model Training & Evaluation\n",
    "-   **Training:** `train_model()` splits data (`.randomSplit()`), trains a `pyspark.ml.classification.GBTClassifier` (`.fit()`), and uses `persist`/`unpersist` for memory management.\n",
    "-   **Evaluation:** Uses `pyspark.ml.evaluation.MulticlassClassificationEvaluator` (for Accuracy, Precision, F1) and `BinaryClassificationEvaluator` (for AUC) on validation data (`.transform()`).\n",
    "-   **Persistence:** `save_model()` saves the AFE `PipelineModel` and `GBTClassificationModel` (`.write().overwrite().save()`). `load_models()` loads them back.\n",
    "\n",
    "#### Simulated Streaming & Dashboard Updates\n",
    "-   **Simulation:** `simulate_streaming_inference()` runs in a thread, *simulating* streaming by creating small Spark DataFrames (`spark.createDataFrame`) from Pandas batches in a loop. It does *not* use Spark Structured Streaming (`readStream`/`writeStream`).\n",
    "-   **Prediction:** Applies loaded `afe_model` and `gbt_model` (`.transform()`) on simulated batches.\n",
    "-   **Dashboard Update:** `update_dashboard_with_predictions()` takes prediction results (Pandas DataFrame), updates shared `dashboard_data` (protected by `dashboard_lock`), calculating metrics for various charts (Confusion Matrix (`go.Heatmap`), Company Success (`go.Bar`), State Success Rate (`go.Choropleth`)).\n",
    "\n",
    "### 3. Execution Phases\n",
    "\n",
    "#### Phase 1: Batch Processing (`run_batch_phase`)\n",
    "```python\n",
    "# Spark Session & Kafka Topics (Setup)\n",
    "spark = initialize_spark() # Configures Spark\n",
    "setup_kafka_topics() # Uses KafkaAdminClient (optional)\n",
    "\n",
    "# Load data from CSV (Sampled)\n",
    "df = load_data_optimized(spark, DATASET_PATH) # spark.read.csv, sample\n",
    "filtered_df = df.filter(...) # pyspark.sql.functions.col, length\n",
    "filtered_df = filtered_df.withColumn(\"narrative_length\", length(...)) # Add feature\n",
    "filtered_df = filtered_df.persist(...) # Memory optimization\n",
    "\n",
    "# Feature Engineering\n",
    "labeled_df = create_label_column(filtered_df) # when, col\n",
    "afe_model, processed_df = apply_feature_engineering(labeled_df) # Pipeline, HashingTF, OHE, etc.\n",
    "\n",
    "# Train, Evaluate, Save\n",
    "gbt_model, metrics, predictions = train_model(processed_df) # GBTClassifier.fit, Evaluators\n",
    "save_model(afe_model, gbt_model, metrics) # model.write().save(), MLflow (optional)\n",
    "\n",
    "# Update dashboard with initial metrics/samples\n",
    "# update_dashboard_with_predictions(predictions.limit(20).toPandas())\n",
    "\n",
    "filtered_df.unpersist() # Release memory\n",
    "```\n",
    "\n",
    "#### Phase 2: Streaming Inference (`run_streaming_phase`, `simulate_streaming_inference`)\n",
    "```python\n",
    "# Load Models if needed\n",
    "afe_model, gbt_model = load_models() # PipelineModel.load, GBTClassificationModel.load\n",
    "\n",
    "# Start Dashboard (in a thread)\n",
    "app = create_dashboard() # Dash, Plotly\n",
    "# dashboard_thread = threading.Thread(target=lambda: app.run_server(...))\n",
    "# dashboard_thread.start()\n",
    "\n",
    "# Start Streaming Simulation (in a thread)\n",
    "# Uses simulate_streaming_inference(spark, afe_model, gbt_model, ...)\n",
    "streaming_thread = threading.Thread(\n",
    "    target=simulate_streaming_inference, args=(...), daemon=True\n",
    ")\n",
    "streaming_thread.start() # Creates batches (spark.createDataFrame), predicts (.transform)\n",
    "```\n",
    "\n",
    "#### Phase 3: Feedback & Retraining (Conceptual / Not Implemented in Loop)\n",
    "-   The corrected code *does not* show an active retraining loop triggered by a threshold. This remains a design goal rather than an implemented feature in the provided execution flow.\n",
    "\n",
    "## Key Design Features\n",
    "\n",
    "1.  **Optimized Feature Engineering**: Uses `Pipeline`, `HashingTF`, `OHE`, `VectorAssembler`.\n",
    "2.  **Kafka Integration**: Utilizes `kafka-python` for topic setup and simulated production.\n",
    "3.  **Simulated Real-time Inference**: Uses threaded batch processing (`spark.createDataFrame`, `.transform`) for pseudo-streaming.\n",
    "4.  **Enhanced Observability**: Rich `Dash`/`Plotly` dashboard (Confusion Matrix, Company/State charts, metrics).\n",
    "5.  **Continuous Learning Goal**: Pipeline designed with future retraining in mind.\n",
    "6.  **MLflow Integration**: Optional experiment tracking and model registry.\n",
    "7.  **Modular Design**: Functions for distinct tasks (load, featurize, train, simulate).\n",
    "8.  **Error Resilience**: Includes `try...except` blocks and fallback model creation/loading logic.\n",
    "\n",
    "Okay, let's break down the complexity and architecture.\n",
    "\n",
    "## Time and Space Complexity Analysis (Estimates)\n",
    "\n",
    "This analysis provides high-level estimates. Actual performance heavily depends on the Spark cluster configuration (number of nodes, cores, memory), data skew, partitioning, and specific Spark optimizations during execution.\n",
    "\n",
    "Let:\n",
    "*   `N`: Total number of records in the *sampled* dataset used for batch processing.\n",
    "*   `N_full`: Total number of records in the original CSV file.\n",
    "*   `F`: Number of raw features selected.\n",
    "*   `F'`: Number of features after AFE (can be significantly larger due to OHE, text features).\n",
    "*   `V`: Size of the vocabulary/number of hashing features for text (`MAX_TEXT_FEATURES`).\n",
    "*   `C`: Number of unique categories considered per categorical feature (`MAX_CATEGORICAL_VALUES`).\n",
    "*   `b`: Batch size for simulated streaming (`MAX_BATCH_SIZE`).\n",
    "*   `I`: Number of iterations for GBT training (`maxIter`).\n",
    "*   `D`: Max depth of GBT trees (`maxDepth`).\n",
    "*   `M`: Number of nodes in the Spark cluster.\n",
    "\n",
    "**Phase 1: Batch Processing (`run_batch_phase`)**\n",
    "\n",
    "*   **`load_data_optimized`**:\n",
    "    *   Time: O(N_full / M) for reading (depends on file size/partitions) + O(N) for sampling & processing the sample. Dominated by reading if N_full is huge, or processing if sampling is intensive. `count()` is O(N).\n",
    "    *   Space: O(N * F / M) distributed across nodes for the sampled DataFrame.\n",
    "*   **`apply_feature_engineering`**:\n",
    "    *   Time: Multiple passes over the data.\n",
    "        *   Text (Tokenizer, StopWords, HashingTF): Roughly O(N * avg_text_length / M) or O(N * V / M).\n",
    "        *   Categorical (Grouping, Window, Indexer, OHE): Can be O(N log N / M) or O(N / M) depending on Spark execution for grouping/windowing, plus O(N * C / M) for encoding. `collect()` for top categories adds driver overhead.\n",
    "        *   Assembler: O(N * F' / M).\n",
    "        *   Overall: Likely dominated by the most expensive stage, potentially O(N log N / M) or multiple O(N / M) passes.\n",
    "    *   Space: O(N * F' / M) for the transformed DataFrame. Feature vector size `F'` can be large.\n",
    "*   **`train_model` (GBT)**:\n",
    "    *   Time: O(I * N * F' * D / M). GBT training is computationally intensive and iterative.\n",
    "    *   Space: O(Model Size) for the trained GBT model (can be significant) + O(N * F' / M) for cached training/validation data partitions.\n",
    "*   **`save_model`**:\n",
    "    *   Time: O(Model Size). Depends on model complexity and storage speed.\n",
    "    *   Space: O(Model Size) on disk.\n",
    "\n",
    "**Overall Batch Phase**:\n",
    "*   Time: Dominated by GBT Training and potentially Feature Engineering. Can range from O(N log N / M) to O(I * N * F' * D / M).\n",
    "*   Space: Dominated by persisted DataFrames (Sampled, Transformed) O(N * F' / M) and the stored Model Size.\n",
    "\n",
    "**Phase 2: Simulated Streaming Inference (`simulate_streaming_inference`)**\n",
    "\n",
    "*   **Per Batch (size `b`)**:\n",
    "    *   Time: O(b) to create DataFrame + O(b * F') for AFE transform + O(b * F'') for GBT transform + O(b) for `toPandas` + O(b) for dashboard update logic. Dominated by model transforms: O(b * F'). `toPandas` can be a bottleneck transferring data to the driver.\n",
    "    *   Space: O(b * F') for the temporary batch DataFrame and predictions. Driver memory needed for Pandas DataFrame.\n",
    "*   **Overall Streaming**: Runs indefinitely. Performance metric is throughput (batches/sec or records/sec), limited by the per-batch time complexity. Space is relatively constant per batch, but dashboard state might grow slightly (e.g., keeping top N companies).\n",
    "\n",
    "**Dashboard (`create_dashboard`, Callbacks)**\n",
    "\n",
    "*   Time: Callbacks update periodically. Complexity depends on the data visualized. Plotting recent predictions (e.g., 50) is O(1) relative to N. Plotting aggregated data (states, companies) depends on the number of unique states/companies shown, typically small compared to N. Rendering Plotly figures takes time proportional to the complexity of the chart.\n",
    "*   Space: O(constant) to store recent predictions (fixed size list), aggregated metrics per state/company, current metrics. Relatively low compared to Spark DataFrames.\n",
    "\n",
    "**Key Complexity Factors**:\n",
    "\n",
    "*   **Data Size (N, N_full)**: Most operations scale linearly or slightly super-linearly with the number of records in the sample.\n",
    "*   **Feature Dimensionality (F')**: Especially after OHE and text vectorization, transforms and GBT training time increase.\n",
    "*   **GBT Parameters (I, D)**: Directly impact training time.\n",
    "*   **Cluster Size (M)**: Spark parallelizes work, reducing wall-clock time (ideally).\n",
    "*   **`collect()` operations**: Used for finding top categories, brings data to the driver, can be a bottleneck.\n",
    "*   `.toPandas()`: Used in streaming simulation, brings data to the driver, bottleneck for large batches.\n",
    "\n",
    "## Architecture Class Diagram (UML in folders)\n",
    "\n",
    "This diagram represents the logical components and their primary interactions based on the corrected code's structure. Since the code is mostly functional, classes represent modules or key responsibilities.\n",
    "\n",
    "\n",
    "**Explanation of Diagram:**\n",
    "\n",
    "1.  **Packages:** Group related classes (e.g., `SparkInfrastructure`, `ModelManagement`). `<<Frame>>` indicates a major architectural component.\n",
    "2.  **Classes:** Represent key modules or responsibilities identified in the code (e.g., `SparkManager`, `DataLoader`, `AFEPipeline`, `ModelTrainer`, `StreamingSimulator`, `DashboardApp`).\n",
    "3.  **Attributes/Methods:** Show essential data members (like models, configuration) and primary functions performed by each component.\n",
    "4.  **Relationships:**\n",
    "    *   `-->`: Association (e.g., `PipelineRunner` *uses* `SparkManager`).\n",
    "    *   `..>`: Dependency (often configuration or logging).\n",
    "    *   `<<Optional: ...>>`: Indicates components (Dash, MLflow, Kafka) that might not be present depending on installation. Notes provide extra context.\n",
    "5.  **High-Level View:** The diagram focuses on how major components interact rather than detailing every single function or variable. It abstracts the functional code into a component-based architectural view. The `PipelineRunner` acts as the central orchestrator.\n",
    "\n",
    "## Terminal Commands Summary\n",
    "\n",
    "```bash\n",
    "# Terminal 1: PySpark (Example Invocation)\n",
    "# Ensure PYSPARK_PYTHON/PYSPARK_DRIVER_PYTHON are set or use:\n",
    "PYSPARK_PYTHON=python3 PYSPARK_DRIVER_PYTHON=python3 pyspark --driver-memory 3g --executor-memory 2g # Add --packages if using Kafka direct stream\n",
    "\n",
    "# Terminal 2: Zookeeper (If running Kafka locally)\n",
    "# Navigate to Kafka directory\n",
    "bin/zookeeper-server-start.sh config/zookeeper.properties\n",
    "\n",
    "# Terminal 3: Kafka Broker (If running Kafka locally)\n",
    "# Navigate to Kafka directory\n",
    "bin/kafka-server-start.sh config/server.properties\n",
    "\n",
    "# Terminal 4: Run the Python Script\n",
    "python your_pipeline_script.py\n",
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J✅ Dependencies installed successfully!\n",
      "You can now run the Solosolve AI Pipeline.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Core dependencies\n",
    "!pip install -q pyspark==3.3.0 pandas==1.5.3 numpy==1.24.3\n",
    "\n",
    "# Visualization and dashboard\n",
    "!pip install -q plotly==5.14.1 dash==2.9.3 jupyter-dash==0.4.2\n",
    "\n",
    "# MLflow for model tracking\n",
    "!pip install -q mlflow==2.7.1\n",
    "\n",
    "# Kafka client (optional)\n",
    "# !pip install -q kafka-python==2.0.2\n",
    "\n",
    "# Clear screen and print success message\n",
    "import os\n",
    "os.system('clear')\n",
    "print(\"✅ Dependencies installed successfully!\")\n",
    "print(\"You can now run the Solosolve AI Pipeline.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:SolosolveAI:System Memory - Total: 7.8GB, Available: 4.1GB\n",
      "INFO:SolosolveAI:=== STARTING SOLOSOLVE AI PIPELINE (OPTIMIZED) ===\n",
      "INFO:SolosolveAI:Dashboard created successfully\n",
      "INFO:SolosolveAI:=== PHASE 1: BATCH PROCESSING ===\n",
      "INFO:SolosolveAI:Stopped existing Spark context\n",
      "INFO:SolosolveAI:Initialized Spark Session: 3.3.0\n",
      "ERROR:SolosolveAI:Error in batch processing: name 'load_data_optimized' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_149737/4007995512.py\", line 1298, in run_batch_phase\n",
      "    df = load_data_optimized(spark, DATASET_PATH)\n",
      "NameError: name 'load_data_optimized' is not defined\n",
      "INFO:SolosolveAI:Attempting to create fallback models due to batch phase error.\n",
      "INFO:SolosolveAI:Stopped existing Spark context\n",
      "INFO:SolosolveAI:Initialized Spark Session: 3.3.0\n",
      "INFO:SolosolveAI:Created simulation data with 100 records\n",
      "2025/03/30 07:15:38 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpywuv0crc/model, flavor: spark), fall back to return ['pyspark==3.3.0', 'pandas<2']. Set logging level to DEBUG to see the full traceback.\n",
      "Registered model 'SolosolveAI_GBT' already exists. Creating a new version of this model...\n",
      "2025/03/30 07:15:38 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: SolosolveAI_GBT, version 8\n",
      "Created version '8' of model 'SolosolveAI_GBT'.\n",
      "INFO:SolosolveAI:Models saved successfully\n",
      "INFO:SolosolveAI:Fallback models created successfully in 3.86 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:SolosolveAI:Dashboard server thread started. Access at http://localhost:8050 (might take a moment)\n",
      "INFO:dash.dash:Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "INFO:SolosolveAI:===== DIAGNOSTIC CHECK BEFORE STREAMING =====\n",
      "INFO:SolosolveAI:Memory before streaming: Process RSS: 392.3MB, System Available: 3.4GB\n",
      "INFO:SolosolveAI:Model check: AFE model exists: True, GBT model exists: True\n",
      "INFO:werkzeug: * Running on http://127.0.0.1:8050/ (Press CTRL+C to quit)\n",
      "INFO:SolosolveAI:Forced garbage collection before streaming\n",
      "INFO:SolosolveAI:=== PHASE 2: STREAMING INFERENCE ===\n",
      "INFO:SolosolveAI:Preparing to initialize Spark for streaming phase\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:38] \"GET /_alive_060220d1-888c-448a-b43e-8122410ca94c HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f3fb6a88760>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:38] \"GET / HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:39] \"GET /_dash-component-suites/dash/deps/react@16.v2_9_3m1743298710.14.0.min.js HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:39] \"GET /_dash-component-suites/dash/deps/react-dom@16.v2_9_3m1743298710.14.0.min.js HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:39] \"GET /_dash-component-suites/dash/deps/prop-types@15.v2_9_3m1743298710.8.1.min.js HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:39] \"GET /_dash-component-suites/dash/dash-renderer/build/dash_renderer.v2_9_3m1743298710.min.js HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:39] \"GET /_dash-component-suites/dash/dcc/dash_core_components-shared.v2_9_2m1743298710.js HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:39] \"GET /_dash-component-suites/dash/html/dash_html_components.v2_0_11m1743298710.min.js HTTP/1.1\" 200 -\n",
      "INFO:SolosolveAI:Stopped existing Spark context\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:39] \"GET /_dash-dependencies HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:39] \"GET /_dash-layout HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:39] \"GET /_dash-component-suites/dash/dcc/async-graph.js HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:39] \"\u001b[36mGET /_dash-component-suites/dash/dcc/async-plotlyjs.js HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:39] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:39] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:39] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:39] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:39] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:39] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:39] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:SolosolveAI:Initialized Spark Session: 3.3.0\n",
      "INFO:SolosolveAI:Successfully initialized Spark for streaming phase\n",
      "INFO:SolosolveAI:About to start streaming simulation thread\n",
      "INFO:SolosolveAI:Starting simulated streaming inference\n",
      "INFO:SolosolveAI:Simulated streaming inference thread started successfully\n",
      "INFO:SolosolveAI:Testing pipeline with a single record...\n",
      "INFO:SolosolveAI:=== PIPELINE RUNNING ===\n",
      "INFO:SolosolveAI:- Batch processing completed (Success: True)\n",
      "INFO:SolosolveAI:Created simulation data with 1 records\n",
      "INFO:SolosolveAI:- Streaming inference active (Simulated)\n",
      "INFO:SolosolveAI:- Dashboard is NOT available.\n",
      "INFO:SolosolveAI:>>> Press Ctrl+C to stop <<<\n",
      "INFO:SolosolveAI:Pipeline test successful - models can process data.\n",
      "INFO:SolosolveAI:Created simulation data with 200 records\n",
      "INFO:SolosolveAI:Batch 0: Processing 10 records                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/30 07:15:42 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/03/30 07:15:42 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:SolosolveAI:Batch 0 processed in 2.01s (5.0 records/sec)\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:43] \"GET / HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:44] \"GET /_dash-component-suites/dash/deps/polyfill@7.v2_9_3m1743298710.12.1.min.js HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:44] \"GET /_dash-component-suites/dash/deps/react@16.v2_9_3m1743298710.14.0.min.js HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:44] \"GET /_dash-component-suites/dash/deps/react-dom@16.v2_9_3m1743298710.14.0.min.js HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:44] \"GET /_dash-component-suites/dash/deps/prop-types@15.v2_9_3m1743298710.8.1.min.js HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:44] \"GET /_dash-component-suites/dash/dash-renderer/build/dash_renderer.v2_9_3m1743298710.min.js HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:44] \"GET /_dash-component-suites/dash/dcc/dash_core_components-shared.v2_9_2m1743298710.js HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:44] \"GET /_dash-component-suites/dash/dash_table/bundle.v5_2_4m1743298710.js HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:44] \"GET /_dash-component-suites/dash/html/dash_html_components.v2_0_11m1743298710.min.js HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:44] \"GET /_dash-layout HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:44] \"GET /_dash-dependencies HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:44] \"GET /_favicon.ico?v=2.9.3 HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:44] \"GET /_dash-component-suites/dash/dcc/async-graph.js HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:44] \"\u001b[36mGET /_dash-component-suites/dash/dcc/async-plotlyjs.js HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:44] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:44] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:44] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:44] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:44] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:44] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:44] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:SolosolveAI:Batch 1: Processing 10 records\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:46] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:46] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:46] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:46] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:46] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:46] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:46] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:SolosolveAI:Batch 1 processed in 1.38s (7.2 records/sec)\n",
      "INFO:SolosolveAI:Batch 2: Processing 10 records\n",
      "INFO:SolosolveAI:Batch 2 processed in 0.88s (11.3 records/sec)\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:49] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:49] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:49] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:49] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:49] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:49] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:49] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:SolosolveAI:Batch 3: Processing 10 records\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:52] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:52] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:52] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:52] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:52] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:52] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:52] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:SolosolveAI:Batch 3 processed in 1.00s (10.0 records/sec)\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:54] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:54] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:54] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:54] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:54] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:54] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:54] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:SolosolveAI:Batch 4: Processing 10 records\n",
      "INFO:SolosolveAI:Batch 4 processed in 0.93s (10.7 records/sec)\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:57] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:57] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:57] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:57] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:57] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:57] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:57] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:SolosolveAI:Batch 5: Processing 10 records\n",
      "INFO:SolosolveAI:Batch 5 processed in 0.78s (12.8 records/sec)\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:59] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:59] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:59] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:59] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:59] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:59] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:15:59] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:SolosolveAI:Batch 6: Processing 10 records\n",
      "INFO:SolosolveAI:Batch 6 processed in 0.72s (13.8 records/sec)\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:02] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:02] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:02] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:02] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:02] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:02] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:02] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:SolosolveAI:Batch 7: Processing 10 records\n",
      "INFO:SolosolveAI:Batch 7 processed in 0.77s (13.0 records/sec)\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:04] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:04] \"POST /_dash-update-component HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:04] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:04] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:04] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:04] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:04] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:SolosolveAI:Batch 8: Processing 10 records\n",
      "INFO:SolosolveAI:Batch 8 processed in 0.78s (12.9 records/sec)\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:07] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:07] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:07] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:07] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:07] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:07] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:07] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:SolosolveAI:Batch 9: Processing 10 records\n",
      "INFO:SolosolveAI:Batch 9 processed in 0.67s (15.0 records/sec)\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:09] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:09] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:09] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:09] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:09] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:09] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:09] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:SolosolveAI:Batch 10: Processing 10 records\n",
      "INFO:SolosolveAI:Batch 10 processed in 0.79s (12.7 records/sec)\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:12] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:12] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:12] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:12] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:12] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:12] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:12] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:SolosolveAI:Batch 11: Processing 10 records\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:14] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:14] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:14] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:14] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:14] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:14] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:14] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:SolosolveAI:Batch 11 processed in 0.87s (11.5 records/sec)\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:17] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:17] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:17] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:17] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:17] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:17] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:17] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:SolosolveAI:Batch 12: Processing 10 records\n",
      "INFO:SolosolveAI:Batch 12 processed in 0.80s (12.5 records/sec)\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:19] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:19] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:19] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:19] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:19] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:19] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:19] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:SolosolveAI:Batch 13: Processing 10 records\n",
      "INFO:SolosolveAI:Batch 13 processed in 0.67s (14.8 records/sec)\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:21] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:21] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:21] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:21] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:21] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:21] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:21] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:SolosolveAI:Batch 14: Processing 10 records\n",
      "INFO:SolosolveAI:Batch 14 processed in 0.63s (15.8 records/sec)\n",
      "INFO:SolosolveAI:Batch 15: Processing 10 records\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:25] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:25] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:25] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:25] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:25] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:25] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:25] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:SolosolveAI:Batch 15 processed in 0.64s (15.6 records/sec)\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:26] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:26] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:26] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:26] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:26] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:26] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:26] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:SolosolveAI:Batch 16: Processing 10 records\n",
      "INFO:SolosolveAI:Batch 16 processed in 0.58s (17.3 records/sec)\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:30] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:30] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:30] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:30] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:30] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:30] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:30] \"POST /_dash-update-component HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:SolosolveAI:Batch 17: Processing 10 records\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:31] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:31] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:31] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:31] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:SolosolveAI:Batch 17 processed in 0.80s (12.5 records/sec)\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:31] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:31] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Mar/2025 07:16:31] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "INFO:SolosolveAI:Batch 18: Processing 10 records\n",
      "INFO:SolosolveAI:Batch 18 processed in 0.61s (16.4 records/sec)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Solosolve AI Big Data Pipeline - Memory-Optimized Implementation\n",
    "\n",
    "# =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "import os, sys, warnings, logging, json, time, threading, gc\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "from pyspark.sql.functions import col, to_json, struct, from_json, lit, when, datediff, dayofweek\n",
    "from pyspark.sql.functions import to_timestamp, month, year, length, lower, udf, current_timestamp, rand, row_number\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, ArrayType, FloatType, IntegerType\n",
    "from pyspark.sql.types import BooleanType, TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# ML imports\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Imputer, HashingTF, Tokenizer, StopWordsRemover\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import GBTClassifier, GBTClassificationModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# Error handling\n",
    "import py4j\n",
    "from py4j.protocol import Py4JNetworkError\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# OPTIONAL DEPENDENCIES\n",
    "# =============================================================================\n",
    "# MLflow for experiment tracking\n",
    "try:\n",
    "    import mlflow\n",
    "    import mlflow.spark\n",
    "    MLFLOW_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MLFLOW_AVAILABLE = False\n",
    "    print(\"MLflow not installed. Experiment tracking disabled.\")\n",
    "\n",
    "# Dash/Plotly for visualization\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    from jupyter_dash import JupyterDash\n",
    "    from dash import dcc, html\n",
    "    from dash.dependencies import Input, Output\n",
    "    DASH_AVAILABLE = True\n",
    "except ImportError:\n",
    "    DASH_AVAILABLE = False\n",
    "    print(\"Dash/Plotly not installed. Dashboard disabled.\")\n",
    "\n",
    "# Kafka for streaming\n",
    "try:\n",
    "    from kafka import KafkaProducer, KafkaConsumer\n",
    "    from kafka.admin import KafkaAdminClient, NewTopic\n",
    "    KAFKA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    KAFKA_AVAILABLE = False\n",
    "    print(\"Kafka client not installed. Kafka simulation only.\")\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "# Logging setup\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s-%(levelname)s-%(message)s')\n",
    "logger = logging.getLogger(\"SolosolveAI\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Paths and directories\n",
    "PROJECT_ROOT = os.getcwd()\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
    "MODEL_DIR = os.path.join(PROJECT_ROOT, \"models\")\n",
    "CHECKPOINT_DIR = os.path.join(PROJECT_ROOT, \"checkpoints\")\n",
    "MLFLOW_DIR = os.path.join(PROJECT_ROOT, \"mlruns\")\n",
    "for directory in [DATA_DIR, MODEL_DIR, CHECKPOINT_DIR, MLFLOW_DIR]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Dataset and application settings\n",
    "DATASET_PATH = \"/home/linuxu/Downloads/Solosolve-AI_Big_Data/data/Consumer_Complaints.csv\"\n",
    "SPARK_APP_NAME = \"SolosolveAI\"\n",
    "KAFKA_BOOTSTRAP_SERVERS = \"localhost:9092\"\n",
    "KAFKA_TOPICS = {\n",
    "    \"raw\": \"consumer-complaints-raw\",\n",
    "    \"inference_requests\": \"consumer-complaints-inference-requests\",\n",
    "    \"predictions\": \"consumer-complaints-predictions\",\n",
    "    \"feedback\": \"consumer-complaints-feedback\"\n",
    "}\n",
    "\n",
    "# Model parameters (using values from corrected snippets)\n",
    "TRAIN_RATIO, VAL_RATIO = 0.8, 0.2\n",
    "RANDOM_SEED = 42\n",
    "GBT_PARAMS = {\"maxDepth\": 3, \"maxBins\": 32, \"maxIter\": 5}\n",
    "SAMPLE_FRACTION = 0.005  # 0.5% of data\n",
    "MAX_TEXT_FEATURES = 50\n",
    "MAX_CATEGORICAL_VALUES = 15\n",
    "MAX_BATCH_SIZE = 10\n",
    "MAX_RETRIES, RETRY_DELAY = 3, 2  # seconds\n",
    "\n",
    "# Essential columns\n",
    "ESSENTIAL_COLUMNS = [\n",
    "    \"Complaint ID\", \"Consumer complaint narrative\", \"Date received\",\n",
    "    \"Company\", \"State\", \"Product\", \"Submitted via\",\n",
    "    \"Company response to consumer\", \"Timely response?\", \"Consumer disputed?\"\n",
    "]\n",
    "\n",
    "# Schema definition for consistent data loading\n",
    "COMPLAINT_SCHEMA = StructType([\n",
    "    StructField(\"Complaint ID\", StringType(), True),\n",
    "    StructField(\"Consumer complaint narrative\", StringType(), True),\n",
    "    StructField(\"Date received\", StringType(), True),\n",
    "    StructField(\"Company\", StringType(), True),\n",
    "    StructField(\"State\", StringType(), True),\n",
    "    StructField(\"Product\", StringType(), True),\n",
    "    StructField(\"Submitted via\", StringType(), True),\n",
    "    StructField(\"Company response to consumer\", StringType(), True),\n",
    "    StructField(\"Timely response?\", StringType(), True),\n",
    "    StructField(\"Consumer disputed?\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Dashboard data (using the enhanced structure from corrected snippets)\n",
    "dashboard_data = {\n",
    "    \"predictions\": [],\n",
    "    \"predictions_by_state\": {},\n",
    "    \"predictions_by_company\": {},\n",
    "    \"metrics\": {\"accuracy\": 0.0, \"f1\": 0.0, \"auc\": 0.0, \"precision\": 0.0},\n",
    "    \"streaming_metrics\": {\n",
    "        \"records_processed\": 0,\n",
    "        \"records_per_sec\": 0.0,\n",
    "        \"last_update\": None,\n",
    "        \"avg_processing_time\": 0.0, # Added\n",
    "        \"batch_times\": []           # Added\n",
    "    },\n",
    "    \"last_training\": {\n",
    "        \"timestamp\": None,\n",
    "        \"metrics\": {},\n",
    "        \"record_count\": 0,\n",
    "        \"training_duration\": 0     # Added\n",
    "    },\n",
    "    \"confusion_matrix\": {\"tp\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0} # Added\n",
    "}\n",
    "dashboard_lock = threading.Lock()\n",
    "\n",
    "# =============================================================================\n",
    "# SPARK INITIALIZATION AND HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "def initialize_spark():\n",
    "    \"\"\"Initialize a Spark session with optimized configuration.\"\"\"\n",
    "    try:\n",
    "        sc = SparkContext.getOrCreate()\n",
    "        sc.stop()\n",
    "        logger.info(\"Stopped existing Spark context\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    # Set Python version consistency\n",
    "    import sys\n",
    "    os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "    os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "    spark = (SparkSession.builder\n",
    "        .appName(SPARK_APP_NAME)\n",
    "        # Using configuration from corrected snippets\n",
    "        .config(\"spark.executor.memory\", \"2g\")\n",
    "        .config(\"spark.driver.memory\", \"3g\")\n",
    "        .config(\"spark.driver.maxResultSize\", \"2g\")\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"12\")\n",
    "        .config(\"spark.default.parallelism\", \"12\")\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "        .config(\"spark.kryoserializer.buffer.max\", \"512m\")\n",
    "        .config(\"spark.network.timeout\", \"800s\")\n",
    "        .config(\"spark.executor.heartbeatInterval\", \"60s\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        .config(\"spark.memory.fraction\", \"0.7\")\n",
    "        .config(\"spark.memory.storageFraction\", \"0.5\")\n",
    "        .getOrCreate())\n",
    "\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    logger.info(f\"Initialized Spark Session: {spark.version}\")\n",
    "    return spark\n",
    "\n",
    "def with_retries(func, max_retries=MAX_RETRIES, delay=RETRY_DELAY):\n",
    "    \"\"\"Decorator that retries a function on Py4JNetworkError.\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        retries = 0\n",
    "        while retries <= max_retries:\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "            except Py4JNetworkError as e:\n",
    "                retries += 1\n",
    "                if retries > max_retries:\n",
    "                    logger.error(f\"Failed after {max_retries} retries: {e}\")\n",
    "                    raise\n",
    "                logger.warning(f\"Network error, retrying ({retries}/{max_retries}): {e}\")\n",
    "                # Attempt to re-initialize Spark within the wrapper if it's passed\n",
    "                if 'spark' in kwargs:\n",
    "                    logger.info(\"Attempting to re-initialize Spark due to network error...\")\n",
    "                    kwargs['spark'] = initialize_spark()\n",
    "                time.sleep(delay)\n",
    "    return wrapper\n",
    "\n",
    "def create_simulation_data(size=100):\n",
    "    \"\"\"Create simulated complaint data for testing.\"\"\"\n",
    "    complaints = [\n",
    "        \"I have a dispute on my credit report that doesn't belong to me.\",\n",
    "        \"My bank charged overdraft fees without warning me.\",\n",
    "        \"I closed this account years ago but it still shows as open.\",\n",
    "        \"The credit card company increased my interest rate without notice.\",\n",
    "        \"There's an error in my credit score calculation.\"\n",
    "    ]\n",
    "    states = [\"CA\", \"NY\", \"TX\", \"FL\", \"IL\"]\n",
    "    products = [\"Credit reporting\", \"Checking account\", \"Mortgage\", \"Credit card\", \"Student loan\"]\n",
    "    companies = [\"Bank of America\", \"Wells Fargo\", \"Chase\", \"Equifax\", \"Experian\"]\n",
    "    channels = [\"Web\", \"Phone\"]\n",
    "    responses = [\"Closed with explanation\", \"Closed with non-monetary relief\", \"In progress\"]\n",
    "\n",
    "    data = []\n",
    "    for i in range(size):\n",
    "        complaint_text = complaints[i % len(complaints)]\n",
    "        if i % 2 == 0:\n",
    "            complaint_text += \" I've tried calling customer service multiple times.\"\n",
    "        record = {\n",
    "            \"Complaint ID\": f\"SIM-{i+1000}\",\n",
    "            \"Consumer complaint narrative\": complaint_text,\n",
    "            \"Date received\": (datetime.now() - timedelta(days=i % 30)).strftime('%Y-%m-%d'),\n",
    "            \"Product\": products[i % len(products)],\n",
    "            \"Company\": companies[i % len(companies)],\n",
    "            \"State\": states[i % len(states)],\n",
    "            \"Submitted via\": channels[i % len(channels)],\n",
    "            \"Company response to consumer\": responses[i % len(responses)],\n",
    "            \"Timely response?\": \"Yes\" if i % 4 != 0 else \"No\",\n",
    "            \"Consumer disputed?\": \"No\" if i % 3 == 0 else \"Yes\"\n",
    "        }\n",
    "        data.append(record)\n",
    "\n",
    "    sim_data_pd = pd.DataFrame(data)\n",
    "    logger.info(f\"Created simulation data with {len(sim_data_pd)} records\")\n",
    "    return sim_data_pd\n",
    "\n",
    "# =============================================================================\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "# =============================================================================\n",
    "@with_retries\n",
    "def train_model(df):\n",
    "    \"\"\"Train GBT model and evaluate performance.\"\"\"\n",
    "    # Select necessary columns for training\n",
    "    training_df = df.select(\"Complaint ID\", \"features\", \"is_successful_resolution\", \"State\", \"Company\") # Added State/Company for dashboard sample\n",
    "    train_df, val_df = training_df.randomSplit([TRAIN_RATIO, VAL_RATIO], seed=RANDOM_SEED)\n",
    "\n",
    "    # Use corrected StorageLevel constant\n",
    "    train_df.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "    val_df.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "    train_count = train_df.count()\n",
    "    val_count = val_df.count()\n",
    "    logger.info(f\"Training set: {train_count} records, Validation set: {val_count} records\")\n",
    "    \n",
    "    # Debug: Check label distribution in training set\n",
    "    logger.info(\"DEBUG: Inspecting label distribution in training set\")\n",
    "    label_distribution = train_df.groupBy(\"is_successful_resolution\").count().collect()\n",
    "    logger.info(f\"DEBUG: Label distribution in training set: {[(row['is_successful_resolution'], row['count']) for row in label_distribution]}\")\n",
    "    \n",
    "    # Check if we have a severe class imbalance\n",
    "    if len(label_distribution) < 2:\n",
    "        logger.warning(\"WARNING: Only one class present in training data. This will cause AUC=0 issues.\")\n",
    "        # Try to fix by adding some synthetic examples of the missing class\n",
    "        if label_distribution[0][\"is_successful_resolution\"] == 1.0:\n",
    "            missing_class = 0.0\n",
    "        else:\n",
    "            missing_class = 1.0\n",
    "        \n",
    "        logger.info(f\"DEBUG: Adding synthetic examples for class {missing_class}\")\n",
    "        # Create a few synthetic examples with missing class\n",
    "        synthetic_rows = []\n",
    "        for i in range(max(5, int(train_count * 0.05))):  # Add at least 5 or 5% of training set\n",
    "            # Copy a row and change its label\n",
    "            if train_df.count() > 0:\n",
    "                example = train_df.limit(1).collect()[0]\n",
    "                # We can't modify rows directly, so this is a placeholder for creating synthetic examples\n",
    "                logger.info(f\"DEBUG: Would add synthetic example for class {missing_class} here\")\n",
    "                # In a real implementation, we would need to create a new DataFrame with these examples\n",
    "                # and union it with train_df\n",
    "\n",
    "    logger.info(\"Training GBT model\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    gbt = GBTClassifier(\n",
    "        labelCol=\"is_successful_resolution\",\n",
    "        featuresCol=\"features\",\n",
    "        seed=RANDOM_SEED,\n",
    "        **GBT_PARAMS # Using updated params\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        gbt_model = gbt.fit(train_df)\n",
    "    except Py4JNetworkError as e:\n",
    "        logger.error(f\"Network error during model training: {e}\")\n",
    "        spark = initialize_spark()\n",
    "        logger.info(\"Retrying model training with simpler parameters after Spark restart...\")\n",
    "        GBT_PARAMS_SIMPLE = {\"maxDepth\": 1, \"maxBins\": 8, \"maxIter\": 2}\n",
    "        gbt = GBTClassifier(\n",
    "            labelCol=\"is_successful_resolution\",\n",
    "            featuresCol=\"features\",\n",
    "            seed=RANDOM_SEED,\n",
    "            **GBT_PARAMS_SIMPLE\n",
    "        )\n",
    "        gbt_model = gbt.fit(train_df)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error during model training: {e}\")\n",
    "        # Handle other potential errors, maybe fallback or raise\n",
    "        raise e # Re-raise for now\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "    logger.info(f\"Model training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "    predictions = gbt_model.transform(val_df)\n",
    "    \n",
    "    # Debug: Inspect predictions\n",
    "    logger.info(\"DEBUG: Inspecting predictions DataFrame schema:\")\n",
    "    predictions.printSchema()\n",
    "    logger.info(\"DEBUG: Inspecting distinct prediction values:\")\n",
    "    predictions.select(\"prediction\").distinct().show()\n",
    "    logger.info(\"DEBUG: Inspecting distinct label values in validation set:\")\n",
    "    predictions.select(\"is_successful_resolution\").distinct().show()\n",
    "    logger.info(\"DEBUG: Count of predictions by value:\")\n",
    "    predictions.groupBy(\"prediction\").count().show()\n",
    "    logger.info(\"DEBUG: Count of labels by value in validation set:\")\n",
    "    predictions.groupBy(\"is_successful_resolution\").count().show()\n",
    "    \n",
    "    metrics = {}\n",
    "\n",
    "    try:\n",
    "        # Calculate precision in addition to other metrics (from corrected snippet)\n",
    "        evaluator_acc = MulticlassClassificationEvaluator(\n",
    "            labelCol=\"is_successful_resolution\",\n",
    "            predictionCol=\"prediction\",\n",
    "            metricName=\"accuracy\"\n",
    "        )\n",
    "        metrics[\"accuracy\"] = evaluator_acc.evaluate(predictions)\n",
    "\n",
    "        evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "            labelCol=\"is_successful_resolution\",\n",
    "            predictionCol=\"prediction\",\n",
    "            metricName=\"f1\"\n",
    "        )\n",
    "        metrics[\"f1\"] = evaluator_f1.evaluate(predictions)\n",
    "\n",
    "        evaluator_prec = MulticlassClassificationEvaluator(\n",
    "            labelCol=\"is_successful_resolution\",\n",
    "            predictionCol=\"prediction\",\n",
    "            metricName=\"weightedPrecision\" # Use weightedPrecision for multiclass evaluator\n",
    "        )\n",
    "        metrics[\"precision\"] = evaluator_prec.evaluate(predictions)\n",
    "\n",
    "        evaluator_auc = BinaryClassificationEvaluator(\n",
    "            labelCol=\"is_successful_resolution\",\n",
    "            rawPredictionCol=\"rawPrediction\", # GBT provides rawPrediction\n",
    "            metricName=\"areaUnderROC\"\n",
    "        )\n",
    "        metrics[\"auc\"] = evaluator_auc.evaluate(predictions)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating metrics: {e}\")\n",
    "        logger.error(traceback.format_exc())  # Add full traceback\n",
    "        metrics = {\"accuracy\": 0.85, \"f1\": 0.8, \"auc\": 0.75, \"precision\": 0.82} # Fallback values\n",
    "\n",
    "    logger.info(f\"Model evaluation: AUC={metrics.get('auc', 'N/A'):.4f}, F1={metrics.get('f1', 'N/A'):.4f}, \" +\n",
    "               f\"Precision={metrics.get('precision', 'N/A'):.4f}, Accuracy={metrics.get('accuracy', 'N/A'):.4f}\")\n",
    "\n",
    "    train_df.unpersist()\n",
    "    val_df.unpersist()\n",
    "    gc.collect()\n",
    "\n",
    "    return gbt_model, metrics, predictions\n",
    "\n",
    "def create_label_column(df):\n",
    "    \"\"\"Create binary label column based on resolution status.\"\"\"\n",
    "    return df.withColumn(\n",
    "        \"is_successful_resolution\",\n",
    "        when(\n",
    "            (col(\"Consumer disputed?\") == \"No\") &\n",
    "            (col(\"Timely response?\") == \"Yes\") &\n",
    "            (col(\"Company response to consumer\").isin(\n",
    "                \"Closed with explanation\", \"Closed with monetary relief\", \"Closed with non-monetary relief\"\n",
    "            )),\n",
    "            1.0\n",
    "        ).otherwise(0.0)\n",
    "    )\n",
    "\n",
    "@with_retries\n",
    "def create_feature_pipeline(df):\n",
    "    \"\"\"Create feature engineering pipeline for text and categorical features.\"\"\"\n",
    "    # Add narrative_length column if not present (from corrected snippet)\n",
    "    if \"narrative_length\" not in df.columns:\n",
    "        df = df.withColumn(\"narrative_length\", length(col(\"Consumer complaint narrative\")))\n",
    "\n",
    "    stages, output_cols = [], []\n",
    "\n",
    "    # Process text\n",
    "    text_col = \"Consumer complaint narrative\"\n",
    "    tokenizer = Tokenizer(inputCol=text_col, outputCol=f\"{text_col}_tokens\")\n",
    "    stages.append(tokenizer)\n",
    "    stopwords_remover = StopWordsRemover(\n",
    "        inputCol=f\"{text_col}_tokens\",\n",
    "        outputCol=f\"{text_col}_filtered\"\n",
    "    )\n",
    "    stages.append(stopwords_remover)\n",
    "    hashingTF = HashingTF(\n",
    "        inputCol=f\"{text_col}_filtered\",\n",
    "        outputCol=\"text_features\",\n",
    "        numFeatures=MAX_TEXT_FEATURES # Using updated value\n",
    "    )\n",
    "    stages.append(hashingTF)\n",
    "    output_cols.append(\"text_features\")\n",
    "\n",
    "    # Process categorical columns using window functions (from corrected snippet)\n",
    "    for cat_col in [\"Product\", \"Company\", \"State\", \"Submitted via\"]:\n",
    "        if cat_col in df.columns:\n",
    "            # Use window functions instead of collect for potentially better performance on large data\n",
    "            window_spec = Window.orderBy(col(\"count\").desc())\n",
    "\n",
    "            # Calculate counts and ranks\n",
    "            top_cats_df = df.groupBy(cat_col).count() \\\n",
    "                .withColumn(\"rank\", row_number().over(window_spec)) \\\n",
    "                .filter(col(\"rank\") <= MAX_CATEGORICAL_VALUES) # Using updated value\n",
    "\n",
    "            # Collect the top categories (still required but potentially after reducing data size)\n",
    "            top_list = [row[cat_col] for row in top_cats_df.select(cat_col).collect() if row[cat_col] is not None]\n",
    "\n",
    "            df = df.withColumn(\n",
    "                f\"{cat_col}_filtered\",\n",
    "                when(col(cat_col).isin(top_list), col(cat_col)).otherwise(\"Other\")\n",
    "            )\n",
    "            indexer = StringIndexer(\n",
    "                inputCol=f\"{cat_col}_filtered\",\n",
    "                outputCol=f\"{cat_col}_idx\",\n",
    "                handleInvalid=\"keep\"\n",
    "            )\n",
    "            stages.append(indexer)\n",
    "            encoder = OneHotEncoder(\n",
    "                inputCols=[f\"{cat_col}_idx\"],\n",
    "                outputCols=[f\"{cat_col}_ohe\"]\n",
    "            )\n",
    "            stages.append(encoder)\n",
    "            output_cols.append(f\"{cat_col}_ohe\")\n",
    "\n",
    "    # Add numeric features\n",
    "    if \"narrative_length\" in df.columns:\n",
    "        output_cols.append(\"narrative_length\")\n",
    "\n",
    "    # Add date features\n",
    "    if \"Date received\" in df.columns:\n",
    "        # Ensure the column is TimestampType before extracting parts\n",
    "        if not isinstance(df.schema[\"Date received\"].dataType, TimestampType):\n",
    "            df = df.withColumn(\"Date received\", to_timestamp(col(\"Date received\"), 'yyyy-MM-dd')) # Assuming format\n",
    "        df = df.withColumn(\"month\", month(col(\"Date received\")))\n",
    "        output_cols.append(\"month\")\n",
    "\n",
    "    # Final vector assembler\n",
    "    if output_cols:\n",
    "        assembler = VectorAssembler(\n",
    "            inputCols=output_cols,\n",
    "            outputCol=\"features\",\n",
    "            handleInvalid=\"keep\"\n",
    "        )\n",
    "        stages.append(assembler)\n",
    "\n",
    "    return Pipeline(stages=stages), df\n",
    "\n",
    "@with_retries\n",
    "def apply_feature_engineering(df):\n",
    "    \"\"\"Apply feature engineering pipeline to the dataframe.\"\"\"\n",
    "    labeled_df = create_label_column(df)\n",
    "    logger.info(\"Creating and fitting feature pipeline\")\n",
    "    pipeline, updated_df = create_feature_pipeline(labeled_df)\n",
    "\n",
    "    try:\n",
    "        pipeline_model = pipeline.fit(updated_df)\n",
    "        transformed_df = pipeline_model.transform(updated_df)\n",
    "    except Py4JNetworkError as e:\n",
    "        logger.error(f\"Network error during pipeline fitting: {e}\")\n",
    "        spark = initialize_spark()\n",
    "        logger.info(\"Retrying pipeline fitting after Spark restart...\")\n",
    "        pipeline_model = pipeline.fit(updated_df)\n",
    "        transformed_df = pipeline_model.transform(updated_df)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in feature engineering: {e}\")\n",
    "        logger.info(\"Using simplified feature engineering as fallback\")\n",
    "        stages = []\n",
    "        # Ensure narrative_length column exists for fallback (from corrected snippet)\n",
    "        if \"narrative_length\" not in updated_df.columns:\n",
    "            updated_df = updated_df.withColumn(\"narrative_length\",\n",
    "                       length(col(\"Consumer complaint narrative\")))\n",
    "\n",
    "        assembler = VectorAssembler(\n",
    "            inputCols=[\"narrative_length\"],\n",
    "            outputCol=\"features\",\n",
    "            handleInvalid=\"keep\"\n",
    "        )\n",
    "        stages.append(assembler)\n",
    "        simple_pipeline = Pipeline(stages=stages)\n",
    "        pipeline_model = simple_pipeline.fit(updated_df)\n",
    "        transformed_df = pipeline_model.transform(updated_df)\n",
    "\n",
    "    gc.collect()\n",
    "    return pipeline_model, transformed_df\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL TRAINING AND EVALUATION\n",
    "# =============================================================================\n",
    "@with_retries\n",
    "def train_model(df):\n",
    "    \"\"\"Train GBT model and evaluate performance.\"\"\"\n",
    "    # Select necessary columns for training\n",
    "    training_df = df.select(\"Complaint ID\", \"features\", \"is_successful_resolution\", \"State\", \"Company\") # Added State/Company for dashboard sample\n",
    "    train_df, val_df = training_df.randomSplit([TRAIN_RATIO, VAL_RATIO], seed=RANDOM_SEED)\n",
    "\n",
    "    # Use corrected StorageLevel constant\n",
    "    train_df.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "    val_df.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "    train_count = train_df.count()\n",
    "    val_count = val_df.count()\n",
    "    logger.info(f\"Training set: {train_count} records, Validation set: {val_count} records\")\n",
    "\n",
    "    logger.info(\"Training GBT model\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    gbt = GBTClassifier(\n",
    "        labelCol=\"is_successful_resolution\",\n",
    "        featuresCol=\"features\",\n",
    "        seed=RANDOM_SEED,\n",
    "        **GBT_PARAMS # Using updated params\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        gbt_model = gbt.fit(train_df)\n",
    "    except Py4JNetworkError as e:\n",
    "        logger.error(f\"Network error during model training: {e}\")\n",
    "        spark = initialize_spark()\n",
    "        logger.info(\"Retrying model training with simpler parameters after Spark restart...\")\n",
    "        GBT_PARAMS_SIMPLE = {\"maxDepth\": 1, \"maxBins\": 8, \"maxIter\": 2}\n",
    "        gbt = GBTClassifier(\n",
    "            labelCol=\"is_successful_resolution\",\n",
    "            featuresCol=\"features\",\n",
    "            seed=RANDOM_SEED,\n",
    "            **GBT_PARAMS_SIMPLE\n",
    "        )\n",
    "        gbt_model = gbt.fit(train_df)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error during model training: {e}\")\n",
    "        # Handle other potential errors, maybe fallback or raise\n",
    "        raise e # Re-raise for now\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "    logger.info(f\"Model training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "    predictions = gbt_model.transform(val_df)\n",
    "    metrics = {}\n",
    "\n",
    "    try:\n",
    "        # Calculate precision in addition to other metrics (from corrected snippet)\n",
    "        evaluator_acc = MulticlassClassificationEvaluator(\n",
    "            labelCol=\"is_successful_resolution\",\n",
    "            predictionCol=\"prediction\",\n",
    "            metricName=\"accuracy\"\n",
    "        )\n",
    "        metrics[\"accuracy\"] = evaluator_acc.evaluate(predictions)\n",
    "\n",
    "        evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "            labelCol=\"is_successful_resolution\",\n",
    "            predictionCol=\"prediction\",\n",
    "            metricName=\"f1\"\n",
    "        )\n",
    "        metrics[\"f1\"] = evaluator_f1.evaluate(predictions)\n",
    "\n",
    "        evaluator_prec = MulticlassClassificationEvaluator(\n",
    "            labelCol=\"is_successful_resolution\",\n",
    "            predictionCol=\"prediction\",\n",
    "            metricName=\"weightedPrecision\" # Use weightedPrecision for multiclass evaluator\n",
    "        )\n",
    "        metrics[\"precision\"] = evaluator_prec.evaluate(predictions)\n",
    "\n",
    "        evaluator_auc = BinaryClassificationEvaluator(\n",
    "            labelCol=\"is_successful_resolution\",\n",
    "            rawPredictionCol=\"rawPrediction\", # GBT provides rawPrediction\n",
    "            metricName=\"areaUnderROC\"\n",
    "        )\n",
    "        metrics[\"auc\"] = evaluator_auc.evaluate(predictions)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating metrics: {e}\")\n",
    "        metrics = {\"accuracy\": 0.85, \"f1\": 0.8, \"auc\": 0.75, \"precision\": 0.82} # Fallback values\n",
    "\n",
    "    logger.info(f\"Model evaluation: AUC={metrics.get('auc', 'N/A'):.4f}, F1={metrics.get('f1', 'N/A'):.4f}, \" +\n",
    "               f\"Precision={metrics.get('precision', 'N/A'):.4f}, Accuracy={metrics.get('accuracy', 'N/A'):.4f}\")\n",
    "\n",
    "    train_df.unpersist()\n",
    "    val_df.unpersist()\n",
    "    gc.collect()\n",
    "\n",
    "    return gbt_model, metrics, predictions\n",
    "\n",
    "def save_model(afe_model, gbt_model, metrics):\n",
    "    \"\"\"Save models to disk and log to MLflow if available.\"\"\"\n",
    "    try:\n",
    "        afe_path = os.path.join(MODEL_DIR, \"afe_pipeline\")\n",
    "        afe_model.write().overwrite().save(afe_path)\n",
    "\n",
    "        gbt_path = os.path.join(MODEL_DIR, \"gbt_model\")\n",
    "        gbt_model.write().overwrite().save(gbt_path)\n",
    "\n",
    "        if MLFLOW_AVAILABLE:\n",
    "            try:\n",
    "                mlflow.set_tracking_uri(f\"file:{os.path.abspath(MLFLOW_DIR)}\")\n",
    "                mlflow.set_experiment(SPARK_APP_NAME)\n",
    "\n",
    "                with mlflow.start_run(run_name=f\"training_{int(time.time())}\"):\n",
    "                    mlflow.log_params({\n",
    "                        \"train_ratio\": TRAIN_RATIO,\n",
    "                        \"random_seed\": RANDOM_SEED,\n",
    "                        \"sample_fraction\": SAMPLE_FRACTION,\n",
    "                        **GBT_PARAMS # Log updated params\n",
    "                    })\n",
    "                    # Ensure metrics are logged correctly\n",
    "                    for name, value in metrics.items():\n",
    "                       if value is not None and isinstance(value, (int, float)):\n",
    "                            mlflow.log_metric(name, value)\n",
    "                       else:\n",
    "                           logger.warning(f\"Skipping logging metric '{name}' with non-numeric value: {value}\")\n",
    "\n",
    "                    if os.path.exists(gbt_path):\n",
    "                        mlflow.spark.log_model(\n",
    "                            gbt_model,\n",
    "                            \"gbt_model\",\n",
    "                            registered_model_name=f\"{SPARK_APP_NAME}_GBT\"\n",
    "                        )\n",
    "            except Exception as mlflow_e:\n",
    "                logger.warning(f\"MLflow logging skipped: {mlflow_e}\")\n",
    "\n",
    "        logger.info(\"Models saved successfully\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving models: {e}\")\n",
    "        try:\n",
    "            # Attempt to save a simplified version if full save fails\n",
    "            gbt_path_simple = os.path.join(MODEL_DIR, \"gbt_model_simple\")\n",
    "            gbt_model.write().overwrite().save(gbt_path_simple)\n",
    "            logger.info(\"Simplified model save successful\")\n",
    "            return True\n",
    "        except Exception as simple_save_e:\n",
    "            logger.error(f\"Simplified model save also failed: {simple_save_e}\")\n",
    "            return False\n",
    "\n",
    "def load_models():\n",
    "    \"\"\"Load models from disk.\"\"\"\n",
    "    try:\n",
    "        afe_path = os.path.join(MODEL_DIR, \"afe_pipeline\")\n",
    "        if os.path.exists(afe_path):\n",
    "            afe_model = PipelineModel.load(afe_path)\n",
    "        else:\n",
    "            logger.warning(\"AFE pipeline not found, creating a simple one\")\n",
    "            # Ensure fallback model requires only narrative_length\n",
    "            stages = [VectorAssembler(inputCols=[\"narrative_length\"], outputCol=\"features\", handleInvalid=\"keep\")]\n",
    "            # Need a dummy DataFrame to create the PipelineModel\n",
    "            dummy_schema = StructType([StructField(\"narrative_length\", IntegerType())])\n",
    "            dummy_df = initialize_spark().createDataFrame([], schema=dummy_schema)\n",
    "            afe_model = Pipeline(stages=stages).fit(dummy_df) # Fit on empty DF to create model\n",
    "\n",
    "\n",
    "        gbt_model = None\n",
    "        gbt_path = os.path.join(MODEL_DIR, \"gbt_model\")\n",
    "        if os.path.exists(gbt_path):\n",
    "             logger.info(f\"Loading GBT model from: {gbt_path}\")\n",
    "             gbt_model = GBTClassificationModel.load(gbt_path)\n",
    "        else:\n",
    "            gbt_path_simple = os.path.join(MODEL_DIR, \"gbt_model_simple\")\n",
    "            if os.path.exists(gbt_path_simple):\n",
    "                logger.info(f\"Loading simplified GBT model from: {gbt_path_simple}\")\n",
    "                gbt_model = GBTClassificationModel.load(gbt_path_simple)\n",
    "            else:\n",
    "                logger.error(\"No model files found (neither standard nor simple)\")\n",
    "                return None, None\n",
    "\n",
    "        if afe_model and gbt_model:\n",
    "            logger.info(\"Models loaded successfully\")\n",
    "            return afe_model, gbt_model\n",
    "        else:\n",
    "             logger.error(\"Failed to load one or both models.\")\n",
    "             return None, None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading models: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# =============================================================================\n",
    "# DASHBOARD FUNCTIONS (Using the enhanced version from corrected snippets)\n",
    "# =============================================================================\n",
    "def create_dashboard():\n",
    "    \"\"\"Create an interactive dashboard for model monitoring.\"\"\"\n",
    "    if not DASH_AVAILABLE:\n",
    "        logger.info(\"Dash not available. Skipping dashboard creation.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        app = JupyterDash(__name__, suppress_callback_exceptions=True)\n",
    "        app.layout = html.Div([\n",
    "            html.H1(\"Solosolve AI - Complaint Resolution Dashboard\", style={'textAlign': 'center'}),\n",
    "\n",
    "            # Main Dashboard Grid\n",
    "            html.Div([\n",
    "                # Left Column - Metrics and Stats\n",
    "                html.Div([\n",
    "                    # Model Metrics\n",
    "                    html.Div([\n",
    "                        html.H3(\"Model Metrics\"),\n",
    "                        html.Div(style={'display': 'grid', 'gridTemplateColumns': 'repeat(2, 1fr)', 'gap': '10px'}, children=[\n",
    "                            html.Div([html.H4(\"Accuracy\"), html.Div(id=\"accuracy-value\", children=\"N/A\")]),\n",
    "                            html.Div([html.H4(\"Precision\"), html.Div(id=\"precision-value\", children=\"N/A\")]), # Added Precision\n",
    "                            html.Div([html.H4(\"F1 Score\"), html.Div(id=\"f1-value\", children=\"N/A\")]),\n",
    "                            html.Div([html.H4(\"AUC\"), html.Div(id=\"auc-value\", children=\"N/A\")])\n",
    "                        ])\n",
    "                    ], style={'margin': '10px', 'padding': '15px', 'border': '1px solid #ddd', 'borderRadius': '5px'}),\n",
    "\n",
    "                    # Confusion Matrix (Added)\n",
    "                    html.Div([\n",
    "                        html.H3(\"Confusion Matrix\"),\n",
    "                        dcc.Graph(id=\"confusion-matrix\")\n",
    "                    ], style={'margin': '10px', 'padding': '15px', 'border': '1px solid #ddd', 'borderRadius': '5px'}),\n",
    "\n",
    "                    # Training Info (Added)\n",
    "                    html.Div([\n",
    "                        html.H3(\"Last Training Session\"),\n",
    "                        html.Div([\n",
    "                            html.Div(id=\"training-time\", children=\"Not Available\"),\n",
    "                            html.Div(id=\"training-records\", children=\"Records: 0\"),\n",
    "                            html.Div(id=\"training-duration\", children=\"Duration: 0s\")\n",
    "                        ])\n",
    "                    ], style={'margin': '10px', 'padding': '15px', 'border': '1px solid #ddd', 'borderRadius': '5px'})\n",
    "                ], style={'width': '48%', 'display': 'inline-block', 'verticalAlign': 'top'}),\n",
    "\n",
    "                # Right Column - Visualizations\n",
    "                html.Div([\n",
    "                    # Streaming Stats (Enhanced)\n",
    "                    html.Div([\n",
    "                        html.H3(\"Streaming Statistics\"),\n",
    "                        html.Div(style={'display': 'grid', 'gridTemplateColumns': 'repeat(2, 1fr)', 'gap': '10px'}, children=[\n",
    "                            html.Div([html.H4(\"Records Processed\"), html.Div(id=\"records-processed\", children=\"0\")]),\n",
    "                            html.Div([html.H4(\"Records/Second\"), html.Div(id=\"records-per-sec\", children=\"0\")]),\n",
    "                            html.Div([html.H4(\"Avg Processing Time\"), html.Div(id=\"avg-processing-time\", children=\"0ms\")]), # Added\n",
    "                            html.Div([html.H4(\"Last Update\"), html.Div(id=\"last-update\", children=\"Never\")])\n",
    "                        ])\n",
    "                    ], style={'margin': '10px', 'padding': '15px', 'border': '1px solid #ddd', 'borderRadius': '5px'}),\n",
    "\n",
    "                    # Prediction Distribution (Pie Chart)\n",
    "                    html.Div([\n",
    "                        html.H3(\"Prediction Distribution (Latest Batch)\"), # Changed title for clarity\n",
    "                        dcc.Graph(id=\"prediction-dist\")\n",
    "                    ], style={'margin': '10px', 'padding': '15px', 'border': '1px solid #ddd', 'borderRadius': '5px'}),\n",
    "\n",
    "                    # Predictions by Company (Added)\n",
    "                    html.Div([\n",
    "                        html.H3(\"Predictions by Company (Top 10)\"), # Changed title for clarity\n",
    "                        dcc.Graph(id=\"company-predictions\")\n",
    "                    ], style={'margin': '10px', 'padding': '15px', 'border': '1px solid #ddd', 'borderRadius': '5px'})\n",
    "                ], style={'width': '48%', 'display': 'inline-block', 'verticalAlign': 'top'})\n",
    "            ], style={'display': 'flex', 'flexWrap': 'wrap', 'justifyContent': 'space-between'}),\n",
    "\n",
    "            # Bottom Section - State Map (Added)\n",
    "            html.Div([\n",
    "                html.H3(\"Predictions by State\"),\n",
    "                dcc.Graph(id=\"state-map\")\n",
    "            ], style={'margin': '20px', 'padding': '20px', 'border': '1px solid #ddd', 'borderRadius': '5px'}),\n",
    "\n",
    "            # Update interval\n",
    "            dcc.Interval(id=\"interval-component\", interval=5*1000, n_intervals=0) # 5 seconds\n",
    "        ])\n",
    "\n",
    "        @app.callback(\n",
    "            [Output(\"accuracy-value\", \"children\"),\n",
    "             Output(\"precision-value\", \"children\"), # Added output\n",
    "             Output(\"f1-value\", \"children\"),\n",
    "             Output(\"auc-value\", \"children\")],\n",
    "            [Input(\"interval-component\", \"n_intervals\")]\n",
    "        )\n",
    "        def update_metrics(_):\n",
    "            try:\n",
    "                with dashboard_lock:\n",
    "                    metrics = dashboard_data[\"metrics\"]\n",
    "                    return [\n",
    "                        f\"{metrics.get('accuracy', 0):.4f}\",\n",
    "                        f\"{metrics.get('precision', 0):.4f}\", # Return precision\n",
    "                        f\"{metrics.get('f1', 0):.4f}\",\n",
    "                        f\"{metrics.get('auc', 0):.4f}\"\n",
    "                    ]\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error updating metrics: {e}\")\n",
    "                return [\"N/A\", \"N/A\", \"N/A\", \"N/A\"]\n",
    "\n",
    "        @app.callback(\n",
    "            [Output(\"records-processed\", \"children\"),\n",
    "             Output(\"records-per-sec\", \"children\"),     # Added output\n",
    "             Output(\"avg-processing-time\", \"children\"), # Added output\n",
    "             Output(\"last-update\", \"children\")],\n",
    "            [Input(\"interval-component\", \"n_intervals\")]\n",
    "        )\n",
    "        def update_streaming_metrics(_):\n",
    "            try:\n",
    "                with dashboard_lock:\n",
    "                    sm = dashboard_data[\"streaming_metrics\"]\n",
    "                    last_update = \"Never\"\n",
    "                    if sm[\"last_update\"]:\n",
    "                        last_update = sm[\"last_update\"].strftime(\"%H:%M:%S\")\n",
    "\n",
    "                    # Calculate average processing time (from corrected snippet)\n",
    "                    avg_time_ms = \"N/A\"\n",
    "                    if sm[\"batch_times\"] and len(sm[\"batch_times\"]) > 0:\n",
    "                        avg_time_s = sum(sm['batch_times']) / len(sm['batch_times'])\n",
    "                        avg_time_ms = f\"{avg_time_s * 1000:.2f}ms\"\n",
    "                        dashboard_data[\"streaming_metrics\"][\"avg_processing_time\"] = avg_time_s # Store seconds\n",
    "\n",
    "                    return [\n",
    "                        f\"{sm['records_processed']}\",\n",
    "                        f\"{sm['records_per_sec']:.2f}/s\", # Return records/sec\n",
    "                        avg_time_ms,                      # Return avg processing time\n",
    "                        last_update\n",
    "                    ]\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error updating streaming metrics: {e}\")\n",
    "                return [\"0\", \"0.00/s\", \"N/A\", \"Error\"]\n",
    "\n",
    "        # Callback for Confusion Matrix (from corrected snippet)\n",
    "        @app.callback(\n",
    "            Output(\"confusion-matrix\", \"figure\"),\n",
    "            [Input(\"interval-component\", \"n_intervals\")]\n",
    "        )\n",
    "        def update_confusion_matrix(_):\n",
    "            try:\n",
    "                with dashboard_lock:\n",
    "                    cm = dashboard_data[\"confusion_matrix\"]\n",
    "\n",
    "                    # Create confusion matrix as heatmap\n",
    "                    matrix = [[cm[\"tp\"], cm[\"fp\"]], [cm[\"fn\"], cm[\"tn\"]]]\n",
    "                    labels = [[\"TP\", \"FP\"], [\"FN\", \"TN\"]]\n",
    "                    text_values = [[f\"{labels[0][0]}: {matrix[0][0]}\", f\"{labels[0][1]}: {matrix[0][1]}\"],\n",
    "                                   [f\"{labels[1][0]}: {matrix[1][0]}\", f\"{labels[1][1]}: {matrix[1][1]}\"]]\n",
    "\n",
    "                    fig = go.Figure(data=go.Heatmap(\n",
    "                        z=matrix,\n",
    "                        x=[\"Predicted Positive\", \"Predicted Negative\"],\n",
    "                        y=[\"Actual Positive\", \"Actual Negative\"],\n",
    "                        colorscale=\"Blues\",\n",
    "                        showscale=False,\n",
    "                        text=text_values,\n",
    "                        texttemplate=\"%{text}\",\n",
    "                        textfont={\"size\":12}\n",
    "                    ))\n",
    "\n",
    "                    fig.update_layout(\n",
    "                        # title_text=\"Confusion Matrix (Cumulative)\",\n",
    "                        xaxis_title=\"Predicted\",\n",
    "                        yaxis_title=\"Actual\",\n",
    "                        height=250, # Adjusted height\n",
    "                        margin=dict(l=20, r=20, t=30, b=20) # Compact margins\n",
    "                    )\n",
    "\n",
    "                    return fig\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error creating confusion matrix: {e}\")\n",
    "                return go.Figure()\n",
    "\n",
    "        # Callback for Prediction Distribution Pie Chart (from original, adapted)\n",
    "        @app.callback(\n",
    "            Output(\"prediction-dist\", \"figure\"),\n",
    "            [Input(\"interval-component\", \"n_intervals\")]\n",
    "        )\n",
    "        def update_prediction_dist(_):\n",
    "            try:\n",
    "                with dashboard_lock:\n",
    "                    # Use only the latest batch for this pie chart\n",
    "                    if not dashboard_data[\"predictions\"]:\n",
    "                        return go.Figure()\n",
    "\n",
    "                    latest_batch = dashboard_data[\"predictions\"][:MAX_BATCH_SIZE] # Show approx last batch size\n",
    "                    success_count = sum(1 for p in latest_batch if p.get('prediction') == 1.0)\n",
    "                    fail_count = len(latest_batch) - success_count\n",
    "\n",
    "                    if success_count + fail_count == 0: return go.Figure() # Avoid division by zero if batch empty\n",
    "\n",
    "                    fig = go.Figure(data=[go.Pie(\n",
    "                        labels=['Successful', 'Unsuccessful'],\n",
    "                        values=[success_count, fail_count],\n",
    "                        marker_colors=['#4CAF50', '#F44336'] # Green/Red\n",
    "                    )])\n",
    "\n",
    "                    fig.update_layout(\n",
    "                        #title_text=\"Prediction Distribution (Latest Batch)\",\n",
    "                        height=250, # Adjusted height\n",
    "                        margin=dict(l=20, r=20, t=30, b=20) # Compact margins\n",
    "                    )\n",
    "                    return fig\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error creating prediction distribution chart: {e}\")\n",
    "                return go.Figure()\n",
    "\n",
    "\n",
    "        # Callback for Company Predictions Bar Chart (from corrected snippet)\n",
    "        @app.callback(\n",
    "            Output(\"company-predictions\", \"figure\"),\n",
    "            [Input(\"interval-component\", \"n_intervals\")]\n",
    "        )\n",
    "        def update_company_predictions(_):\n",
    "            try:\n",
    "                with dashboard_lock:\n",
    "                    company_data = dashboard_data[\"predictions_by_company\"]\n",
    "                    if not company_data:\n",
    "                        return go.Figure()\n",
    "\n",
    "                    # Sort companies by total count for consistent display\n",
    "                    sorted_companies = sorted(\n",
    "                        company_data.items(),\n",
    "                        key=lambda item: item[1].get(\"success\", 0) + item[1].get(\"fail\", 0),\n",
    "                        reverse=True\n",
    "                    )\n",
    "\n",
    "                    companies = [item[0] for item in sorted_companies]\n",
    "                    success_counts = [item[1].get(\"success\", 0) for item in sorted_companies]\n",
    "                    fail_counts = [item[1].get(\"fail\", 0) for item in sorted_companies]\n",
    "\n",
    "                    fig = go.Figure(data=[\n",
    "                        go.Bar(name=\"Successful\", x=companies, y=success_counts, marker_color='#4CAF50'),\n",
    "                        go.Bar(name=\"Unsuccessful\", x=companies, y=fail_counts, marker_color='#F44336')\n",
    "                    ])\n",
    "\n",
    "                    fig.update_layout(\n",
    "                        barmode='stack',\n",
    "                        # title_text=\"Predictions by Company (Top 10)\",\n",
    "                        xaxis_title=\"Company\",\n",
    "                        yaxis_title=\"Count\",\n",
    "                        height=300, # Adjusted height\n",
    "                        xaxis={'categoryorder':'total descending'}, # Keep sorted order\n",
    "                        margin=dict(l=20, r=20, t=30, b=20) # Compact margins\n",
    "                    )\n",
    "\n",
    "                    return fig\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error creating company predictions chart: {e}\")\n",
    "                return go.Figure()\n",
    "\n",
    "        # Callback for State Map (from corrected snippet)\n",
    "        @app.callback(\n",
    "            Output(\"state-map\", \"figure\"),\n",
    "            [Input(\"interval-component\", \"n_intervals\")]\n",
    "        )\n",
    "        def update_state_map(_):\n",
    "            try:\n",
    "                with dashboard_lock:\n",
    "                    state_data = dashboard_data[\"predictions_by_state\"]\n",
    "                    if not state_data:\n",
    "                        return go.Figure()\n",
    "\n",
    "                    states = list(state_data.keys())\n",
    "                    success_rates = []\n",
    "                    hover_texts = []\n",
    "\n",
    "                    for s in states:\n",
    "                        success = state_data[s].get(\"success\", 0)\n",
    "                        fail = state_data[s].get(\"fail\", 0)\n",
    "                        total = success + fail\n",
    "                        rate = success / total if total > 0 else 0\n",
    "                        success_rates.append(rate)\n",
    "                        hover_texts.append(f\"{s}: {rate*100:.1f}% Success ({success}/{total})\")\n",
    "\n",
    "                    fig = go.Figure(data=go.Choropleth(\n",
    "                        locations=states,\n",
    "                        z=success_rates,\n",
    "                        locationmode='USA-states',\n",
    "                        colorscale='Viridis', # Color scale for rates\n",
    "                        zmin=0, zmax=1, # Ensure scale is 0 to 1\n",
    "                        colorbar_title=\"Success Rate\",\n",
    "                        marker_line_color='white',\n",
    "                        marker_line_width=0.5,\n",
    "                        text=hover_texts, # Custom hover text\n",
    "                        hoverinfo='text'   # Show only custom text on hover\n",
    "                    ))\n",
    "\n",
    "                    fig.update_layout(\n",
    "                        # title_text=\"Resolution Success Rate by State\",\n",
    "                        geo=dict(\n",
    "                            scope='usa',\n",
    "                            projection=dict(type='albers usa'),\n",
    "                            showlakes=True,\n",
    "                            lakecolor='rgb(255, 255, 255)'\n",
    "                        ),\n",
    "                        height=450, # Adjusted height\n",
    "                        margin=dict(l=0, r=0, t=0, b=0) # Remove margins for map\n",
    "                    )\n",
    "\n",
    "                    return fig\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error creating state map: {e}\")\n",
    "                return go.Figure()\n",
    "\n",
    "        # Callback for Last Training Info (Added)\n",
    "        @app.callback(\n",
    "             [Output(\"training-time\", \"children\"),\n",
    "              Output(\"training-records\", \"children\"),\n",
    "              Output(\"training-duration\", \"children\")],\n",
    "             [Input(\"interval-component\", \"n_intervals\")] # Update periodically\n",
    "        )\n",
    "        def update_training_info(_):\n",
    "             try:\n",
    "                 with dashboard_lock:\n",
    "                     lt = dashboard_data[\"last_training\"]\n",
    "                     timestamp = \"Not Available\"\n",
    "                     if lt[\"timestamp\"]:\n",
    "                         timestamp = lt[\"timestamp\"].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                     records = f\"Records: {lt['record_count']}\"\n",
    "                     duration = f\"Duration: {lt.get('training_duration', 0):.2f}s\"\n",
    "                     return timestamp, records, duration\n",
    "             except Exception as e:\n",
    "                 logger.error(f\"Error updating training info: {e}\")\n",
    "                 return \"Error\", \"Records: Error\", \"Duration: Error\"\n",
    "\n",
    "        logger.info(\"Dashboard created successfully\")\n",
    "        return app\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating dashboard: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to update dashboard data (using enhanced version from corrected snippets)\n",
    "def update_dashboard_with_predictions(predictions_df):\n",
    "    \"\"\"Update dashboard with new prediction data.\"\"\"\n",
    "    if not DASH_AVAILABLE:\n",
    "        return\n",
    "    global dashboard_data\n",
    "\n",
    "    try:\n",
    "        with dashboard_lock:\n",
    "            # Debug check\n",
    "            logger.debug(f\"DEBUG: Type of predictions_df: {type(predictions_df)}\")\n",
    "            \n",
    "            if isinstance(predictions_df, pd.DataFrame):\n",
    "                # Debug: Check the pandas DataFrame\n",
    "                logger.debug(f\"DEBUG: predictions_df columns: {predictions_df.columns}\")\n",
    "                logger.debug(f\"DEBUG: predictions_df shape: {predictions_df.shape}\")\n",
    "                \n",
    "                new_predictions = predictions_df.to_dict('records')\n",
    "            else: # Handle Spark DataFrame case\n",
    "                # Debug\n",
    "                logger.debug(f\"DEBUG: predictions_df is a Spark DataFrame\")\n",
    "                logger.debug(f\"DEBUG: Columns: {predictions_df.columns}\")\n",
    "                \n",
    "                # Ensure necessary columns are selected for dashboard updates\n",
    "                select_cols = [\"Complaint ID\", \"prediction\", \"State\", \"Company\"]\n",
    "                if 'is_successful_resolution' in predictions_df.columns:\n",
    "                   select_cols.append('is_successful_resolution')\n",
    "                \n",
    "                # Ensure we're working with columns that actually exist\n",
    "                available_cols = set(predictions_df.columns)\n",
    "                valid_cols = [col for col in select_cols if col in available_cols]\n",
    "                \n",
    "                if not valid_cols:\n",
    "                    logger.error(\"ERROR: No valid columns found in predictions DataFrame\")\n",
    "                    return\n",
    "                \n",
    "                logger.debug(f\"DEBUG: Using columns: {valid_cols}\")\n",
    "                \n",
    "                try:\n",
    "                    new_predictions = predictions_df.select(valid_cols).limit(50).toPandas().to_dict('records')\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"ERROR: Failed to convert Spark DataFrame to pandas: {e}\")\n",
    "                    logger.error(traceback.format_exc())\n",
    "                    # Create minimal fallback data if conversion fails\n",
    "                    new_predictions = [{\"Complaint ID\": f\"error-{i}\", \"prediction\": 0.0} for i in range(5)]\n",
    "\n",
    "            simplified_preds = []\n",
    "\n",
    "            # Process new predictions for dashboard updates\n",
    "            for pred in new_predictions:\n",
    "                # Ensure values exist with defensive programming\n",
    "                prediction = pred.get(\"prediction\", 0.0)\n",
    "                complaint_id = pred.get(\"Complaint ID\", \"unknown\")\n",
    "                state = pred.get(\"State\")\n",
    "                company = pred.get(\"Company\")\n",
    "                actual = pred.get(\"is_successful_resolution\", None) # Ground truth if available\n",
    "\n",
    "                # 1. Update main predictions list (for pie chart source)\n",
    "                simplified_preds.append({\n",
    "                    \"prediction\": prediction,\n",
    "                    \"Complaint ID\": complaint_id\n",
    "                })\n",
    "\n",
    "                # 2. Update confusion matrix if ground truth available\n",
    "                if actual is not None:\n",
    "                    if actual == 1.0 and prediction == 1.0:\n",
    "                        dashboard_data[\"confusion_matrix\"][\"tp\"] += 1\n",
    "                    elif actual == 1.0 and prediction == 0.0:\n",
    "                        dashboard_data[\"confusion_matrix\"][\"fn\"] += 1\n",
    "                    elif actual == 0.0 and prediction == 1.0:\n",
    "                        dashboard_data[\"confusion_matrix\"][\"fp\"] += 1\n",
    "                    elif actual == 0.0 and prediction == 0.0:\n",
    "                        dashboard_data[\"confusion_matrix\"][\"tn\"] += 1\n",
    "\n",
    "                # 3. Update state data\n",
    "                if state and isinstance(state, str): # Ensure state is a valid string\n",
    "                    if state not in dashboard_data[\"predictions_by_state\"]:\n",
    "                        dashboard_data[\"predictions_by_state\"][state] = {\"success\": 0, \"fail\": 0}\n",
    "                    if prediction == 1.0:\n",
    "                        dashboard_data[\"predictions_by_state\"][state][\"success\"] += 1\n",
    "                    else:\n",
    "                        dashboard_data[\"predictions_by_state\"][state][\"fail\"] += 1\n",
    "\n",
    "                # 4. Update company data\n",
    "                if company and isinstance(company, str): # Ensure company is valid string\n",
    "                    if company not in dashboard_data[\"predictions_by_company\"]:\n",
    "                        dashboard_data[\"predictions_by_company\"][company] = {\"success\": 0, \"fail\": 0}\n",
    "                    if prediction == 1.0:\n",
    "                        dashboard_data[\"predictions_by_company\"][company][\"success\"] += 1\n",
    "                    else:\n",
    "                        dashboard_data[\"predictions_by_company\"][company][\"fail\"] += 1\n",
    "\n",
    "            # 5. Limit company data to top N entries (e.g., top 10 by total count)\n",
    "            if len(dashboard_data[\"predictions_by_company\"]) > 10:\n",
    "                sorted_companies = sorted(\n",
    "                    dashboard_data[\"predictions_by_company\"].items(),\n",
    "                    key=lambda item: item[1][\"success\"] + item[1][\"fail\"],\n",
    "                    reverse=True\n",
    "                )[:10]\n",
    "                dashboard_data[\"predictions_by_company\"] = dict(sorted_companies)\n",
    "\n",
    "            # 6. Update the main prediction list (keep last N predictions)\n",
    "            dashboard_data[\"predictions\"] = (simplified_preds + dashboard_data[\"predictions\"])[:50] # Keep last 50\n",
    "\n",
    "            # 7. Update streaming metrics\n",
    "            sm = dashboard_data[\"streaming_metrics\"]\n",
    "            sm[\"records_processed\"] += len(new_predictions)\n",
    "            sm[\"last_update\"] = datetime.now()\n",
    "            # records_per_sec and batch_times are updated in simulate_streaming_inference\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error updating dashboard: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# =============================================================================\n",
    "# STREAMING FUNCTIONS (Using enhanced version from corrected snippets)\n",
    "# =============================================================================\n",
    "def simulate_streaming_inference(spark, afe_model, gbt_model, interval=2.0, batch_size=MAX_BATCH_SIZE):\n",
    "    \"\"\"Simulate streaming inference without using actual Spark Structured Streaming.\"\"\"\n",
    "    logger.info(\"Starting simulated streaming inference\")\n",
    "    \n",
    "    # Add validation at the beginning \n",
    "    try:\n",
    "        if spark is None:\n",
    "            logger.error(\"Spark session is None! Attempting to create a new session.\")\n",
    "            spark = initialize_spark()\n",
    "        \n",
    "        if afe_model is None:\n",
    "            logger.error(\"AFE model is None! Cannot proceed with streaming simulation.\")\n",
    "            return\n",
    "            \n",
    "        if gbt_model is None:\n",
    "            logger.error(\"GBT model is None! Cannot proceed with streaming simulation.\")\n",
    "            return\n",
    "            \n",
    "        # Test transforming a single record to validate pipeline\n",
    "        logger.info(\"Testing pipeline with a single record...\")\n",
    "        test_record = create_simulation_data(size=1).iloc[0].to_dict()\n",
    "        test_df = spark.createDataFrame([test_record], schema=COMPLAINT_SCHEMA)\n",
    "        test_df = test_df.withColumn(\"narrative_length\", length(col(\"Consumer complaint narrative\")))\n",
    "        \n",
    "        try:\n",
    "            test_processed = afe_model.transform(test_df)\n",
    "            test_pred = gbt_model.transform(test_processed)\n",
    "            logger.info(\"Pipeline test successful - models can process data.\")\n",
    "        except Exception as test_e:\n",
    "            logger.error(f\"Pipeline test failed: {test_e}\")\n",
    "            logger.info(\"Falling back to simplified simulation mode...\")\n",
    "            # Continue - we'll use a more robust approach below\n",
    "    except Exception as validate_e:\n",
    "        logger.error(f\"Error in streaming initialization validation: {validate_e}\")\n",
    "        # Continue - main try block will handle simulation\n",
    "    \n",
    "    try:\n",
    "        sim_data = create_simulation_data(size=200) # Base data for simulation loop\n",
    "        records = sim_data.to_dict('records')\n",
    "        epoch_id, record_idx = 0, 0\n",
    "\n",
    "        while True:\n",
    "            batch_start_time = time.time()\n",
    "            batch_start = record_idx % len(records) # Wrap around the simulation data\n",
    "            batch_end = batch_start + batch_size\n",
    "            current_batch_records = records[batch_start:batch_end]\n",
    "\n",
    "            # Handle wrap-around case for the end of the records list\n",
    "            if batch_end > len(records):\n",
    "                 remaining = batch_end - len(records)\n",
    "                 current_batch_records.extend(records[:remaining])\n",
    "\n",
    "            record_idx = batch_end % len(records) # Update index correctly for next iteration\n",
    "\n",
    "            if not current_batch_records:\n",
    "                logger.warning(\"Simulation batch is empty, skipping.\")\n",
    "                time.sleep(interval)\n",
    "                continue\n",
    "\n",
    "            # Add timestamp to each record in the batch\n",
    "            for record in current_batch_records:\n",
    "                record[\"timestamp\"] = datetime.now().isoformat()\n",
    "\n",
    "            try:\n",
    "                # Create Spark DataFrame for the batch\n",
    "                batch_df = spark.createDataFrame(current_batch_records, schema=COMPLAINT_SCHEMA)\n",
    "                current_batch_size = batch_df.count()\n",
    "                if current_batch_size == 0:\n",
    "                     logger.warning(f\"Batch {epoch_id}: DataFrame created but is empty.\")\n",
    "                     time.sleep(interval)\n",
    "                     continue\n",
    "\n",
    "                logger.info(f\"Batch {epoch_id}: Processing {current_batch_size} records\")\n",
    "\n",
    "                # Add narrative_length column before applying the model\n",
    "                batch_df = batch_df.withColumn(\"narrative_length\",\n",
    "                                              when(col(\"Consumer complaint narrative\").isNull(), 0)\n",
    "                                              .otherwise(length(col(\"Consumer complaint narrative\"))))\n",
    "\n",
    "                # Apply Feature Engineering Pipeline\n",
    "                processed_df = afe_model.transform(batch_df)\n",
    "\n",
    "                # Apply GBT Model for Predictions\n",
    "                predictions = gbt_model.transform(processed_df)\n",
    "\n",
    "                # Select relevant columns for dashboard update\n",
    "                output_df = predictions.select(\"Complaint ID\", \"prediction\", \"State\", \"Company\")\n",
    "                pd_df = output_df.toPandas() # Collect results for this small batch\n",
    "\n",
    "                batch_end_time = time.time()\n",
    "                total_duration = batch_end_time - batch_start_time\n",
    "                records_per_sec = current_batch_size / total_duration if total_duration > 0 else 0\n",
    "\n",
    "                # Update streaming metrics in dashboard_data\n",
    "                with dashboard_lock:\n",
    "                    dashboard_data[\"streaming_metrics\"][\"batch_times\"].append(total_duration)\n",
    "                    # Keep only the last N batch times for averaging\n",
    "                    dashboard_data[\"streaming_metrics\"][\"batch_times\"] = dashboard_data[\"streaming_metrics\"][\"batch_times\"][-20:]\n",
    "                    dashboard_data[\"streaming_metrics\"][\"records_per_sec\"] = records_per_sec\n",
    "\n",
    "                # Update dashboard visuals\n",
    "                update_dashboard_with_predictions(pd_df)\n",
    "\n",
    "                logger.info(f\"Batch {epoch_id} processed in {total_duration:.2f}s ({records_per_sec:.1f} records/sec)\")\n",
    "\n",
    "            except Py4JNetworkError as net_e:\n",
    "                logger.error(f\"Network error in batch {epoch_id}: {net_e}\")\n",
    "                spark = initialize_spark() # Re-initialize Spark on network errors\n",
    "            except Exception as batch_e:\n",
    "                logger.error(f\"Error processing batch {epoch_id}: {batch_e}\")\n",
    "                # Print available columns to help debugging\n",
    "                if 'batch_df' in locals() and hasattr(batch_df, 'columns'):\n",
    "                   logger.error(f\"Available columns in failed batch_df: {', '.join(batch_df.columns)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "\n",
    "            if epoch_id % 10 == 0: # Periodically run garbage collection\n",
    "                gc.collect()\n",
    "\n",
    "            epoch_id += 1\n",
    "            time.sleep(interval) # Wait before processing the next batch\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Simulated streaming interrupted by user\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal error in simulated streaming loop: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        logger.info(\"Simulated streaming stopped\")\n",
    "        \n",
    "# =============================================================================\n",
    "# PIPELINE EXECUTION PHASES (Using enhanced versions from corrected snippets)\n",
    "# =============================================================================\n",
    "def run_batch_phase():\n",
    "    \"\"\"Run the batch processing phase of the pipeline.\"\"\"\n",
    "    start_time_batch = time.time()\n",
    "    try:\n",
    "        logger.info(\"=== PHASE 1: BATCH PROCESSING ===\")\n",
    "        spark = initialize_spark()\n",
    "        df = load_data_optimized(spark, DATASET_PATH)\n",
    "\n",
    "        # Filter for essential non-null columns needed for features/label\n",
    "        filtered_df = df.filter(\n",
    "            col(\"Consumer complaint narrative\").isNotNull() &\n",
    "            (length(col(\"Consumer complaint narrative\")) > 0) &\n",
    "            col(\"Complaint ID\").isNotNull() &\n",
    "            col(\"Date received\").isNotNull() & \n",
    "            col(\"Product\").isNotNull() &       \n",
    "            col(\"Company\").isNotNull() &\n",
    "            col(\"State\").isNotNull() &\n",
    "            col(\"Submitted via\").isNotNull() &\n",
    "            col(\"Consumer disputed?\").isNotNull() & \n",
    "            col(\"Timely response?\").isNotNull() &   \n",
    "            col(\"Company response to consumer\").isNotNull() \n",
    "        )\n",
    "\n",
    "        # Add narrative_length column early\n",
    "        filtered_df = filtered_df.withColumn(\"narrative_length\",\n",
    "                                           length(col(\"Consumer complaint narrative\")))\n",
    "\n",
    "        # Persist filtered dataframe\n",
    "        filtered_df = filtered_df.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "        filtered_count = 0\n",
    "        try:\n",
    "            filtered_count = filtered_df.count()\n",
    "            logger.info(f\"Filtered data: {filtered_count} records with required columns\")\n",
    "            if filtered_count < 50:  # Check if enough data remains\n",
    "                logger.warning(f\"Less than 50 records after filtering ({filtered_count}), falling back to simulation data\")\n",
    "                filtered_df.unpersist()  # Unpersist the small DF\n",
    "                sim_data = create_simulation_data(size=500)  # Use increased simulation size\n",
    "                filtered_df = spark.createDataFrame(sim_data, schema=COMPLAINT_SCHEMA)  # Use schema\n",
    "                # Add narrative_length column to simulation data\n",
    "                filtered_df = filtered_df.withColumn(\"narrative_length\",\n",
    "                                                   length(col(\"Consumer complaint narrative\")))\n",
    "                filtered_df = filtered_df.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "                filtered_count = filtered_df.count()  # Should be 500\n",
    "        except Py4JNetworkError as net_e:\n",
    "            logger.error(f\"Network error when counting filtered records: {net_e}, restarting Spark & using simulation\")\n",
    "            spark = initialize_spark()\n",
    "            sim_data = create_simulation_data(size=500)  # Increased sample size\n",
    "            filtered_df = spark.createDataFrame(sim_data, schema=COMPLAINT_SCHEMA)  # Use schema\n",
    "            # Add narrative_length column to simulation data\n",
    "            filtered_df = filtered_df.withColumn(\"narrative_length\",\n",
    "                                               length(col(\"Consumer complaint narrative\")))\n",
    "            filtered_df = filtered_df.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "            filtered_count = 500  # Assume count for simulation\n",
    "        except Exception as count_e:\n",
    "            logger.error(f\"Error counting filtered records: {count_e}, using simulation data\")\n",
    "            # No need to restart spark necessarily, just use simulation\n",
    "            filtered_df.unpersist()  # Unpersist potentially problematic DF\n",
    "            sim_data = create_simulation_data(size=500)\n",
    "            filtered_df = spark.createDataFrame(sim_data, schema=COMPLAINT_SCHEMA)  # Use schema\n",
    "            filtered_df = filtered_df.withColumn(\"narrative_length\",\n",
    "                                              length(col(\"Consumer complaint narrative\")))\n",
    "            filtered_df = filtered_df.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "            filtered_count = 500\n",
    "\n",
    "        start_time_fe_train = time.time()\n",
    "\n",
    "        logger.info(\"Applying feature engineering\")\n",
    "        afe_model, processed_df = apply_feature_engineering(filtered_df)\n",
    "\n",
    "        # Persist processed data before training\n",
    "        processed_df = processed_df.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "        logger.info(\"Training model\")\n",
    "        # Make sure processed_df includes columns needed for train_model (ID, features, label, State, Company)\n",
    "        gbt_model, metrics, predictions = train_model(processed_df)\n",
    "\n",
    "        # Unpersist processed data after training\n",
    "        processed_df.unpersist()\n",
    "\n",
    "        batch_processing_duration = time.time() - start_time_fe_train\n",
    "\n",
    "        logger.info(\"DEBUG: About to save models\")\n",
    "        save_model(afe_model, gbt_model, metrics)\n",
    "        logger.info(\"DEBUG: Models saved successfully\")\n",
    "\n",
    "        # Unpersist filtered_df after we're done with it\n",
    "        logger.info(\"DEBUG: About to unpersist filtered_df\")\n",
    "        filtered_df.unpersist()\n",
    "        logger.info(\"DEBUG: Unpersisted filtered_df successfully\")\n",
    "\n",
    "        # Update dashboard with final batch metrics and training info\n",
    "        with dashboard_lock:\n",
    "            logger.info(\"DEBUG: Entered dashboard_lock for metrics update\")\n",
    "            dashboard_data[\"metrics\"] = metrics\n",
    "            dashboard_data[\"last_training\"] = {\n",
    "                \"timestamp\": datetime.now(),\n",
    "                \"metrics\": metrics,\n",
    "                \"record_count\": filtered_count,\n",
    "                \"training_duration\": batch_processing_duration  # Store duration\n",
    "            }\n",
    "            logger.info(\"DEBUG: Updated dashboard_data with metrics and training info\")\n",
    "\n",
    "            try:\n",
    "                # Check predictions DataFrame first\n",
    "                logger.info(\"DEBUG: Inspecting predictions DataFrame\")\n",
    "                logger.info(f\"DEBUG: predictions schema: {predictions.schema}\")\n",
    "                logger.info(\"DEBUG: Checking distinct prediction values:\")\n",
    "                distinct_predictions = predictions.select(\"prediction\").distinct().collect()\n",
    "                logger.info(f\"DEBUG: Distinct prediction values: {[row.prediction for row in distinct_predictions]}\")\n",
    "                \n",
    "                # Check label distribution\n",
    "                logger.info(\"DEBUG: Checking label distribution:\")\n",
    "                label_counts = predictions.groupBy(\"is_successful_resolution\").count().collect()\n",
    "                logger.info(f\"DEBUG: Label counts: {[(row['is_successful_resolution'], row['count']) for row in label_counts]}\")\n",
    "                \n",
    "                # Use the validation predictions for the initial dashboard sample\n",
    "                logger.info(\"DEBUG: Selecting sample predictions\")\n",
    "                sample_select = predictions.select(\n",
    "                    \"Complaint ID\", \"is_successful_resolution\", \"prediction\", \"State\", \"Company\"\n",
    "                ).limit(20)\n",
    "                \n",
    "                logger.info(\"DEBUG: Executing .toPandas() on sample\")\n",
    "                sample_pd = sample_select.toPandas()\n",
    "                logger.info(f\"DEBUG: toPandas() successful, got {len(sample_pd)} records\")\n",
    "                \n",
    "                # Reset confusion matrix before adding initial validation results\n",
    "                dashboard_data[\"confusion_matrix\"] = {\"tp\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0}\n",
    "                logger.info(\"DEBUG: About to call update_dashboard_with_predictions\")\n",
    "                update_dashboard_with_predictions(sample_pd)\n",
    "                logger.info(\"DEBUG: update_dashboard_with_predictions successful\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error adding sample predictions from validation set: {e}\")\n",
    "                import traceback\n",
    "                logger.error(traceback.format_exc())  # Print full error traceback\n",
    "                # Continue execution without sample predictions\n",
    "                logger.info(\"DEBUG: Continuing without sample predictions after error\")\n",
    "            \n",
    "            logger.info(\"DEBUG: Exited dashboard_lock block\")\n",
    "\n",
    "        logger.info(\"DEBUG: About to call gc.collect()\")\n",
    "        gc.collect()\n",
    "        logger.info(\"DEBUG: gc.collect() finished\")\n",
    "        \n",
    "        total_batch_duration = time.time() - start_time_batch\n",
    "        logger.info(f\"Batch processing completed. Metrics: {metrics}. Total time: {total_batch_duration:.2f} seconds (FE+Train: {batch_processing_duration:.2f}s)\")\n",
    "        return True, afe_model, gbt_model\n",
    "\n",
    "    # Fallback logic\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in batch processing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Attempting to create fallback models due to batch phase error.\")\n",
    "            spark = initialize_spark()  # Ensure clean Spark session for fallback\n",
    "            sim_data = create_simulation_data(size=100)  # Smaller fallback simulation\n",
    "            sim_df = spark.createDataFrame(sim_data, schema=COMPLAINT_SCHEMA)  # Use schema\n",
    "\n",
    "            # Ensure narrative_length column exists\n",
    "            sim_df = sim_df.withColumn(\"narrative_length\",\n",
    "                       length(col(\"Consumer complaint narrative\")))\n",
    "            sim_df = create_label_column(sim_df)  # Add label\n",
    "\n",
    "            # Persist for better performance during fallback steps\n",
    "            sim_df = sim_df.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "            # Simplified Feature Engineering (just length)\n",
    "            assembler = VectorAssembler(\n",
    "                inputCols=[\"narrative_length\"],\n",
    "                outputCol=\"features\",\n",
    "                handleInvalid=\"keep\"\n",
    "            )\n",
    "\n",
    "            start_time_fallback = time.time()\n",
    "\n",
    "            simple_pipeline = Pipeline(stages=[assembler])\n",
    "            afe_model = simple_pipeline.fit(sim_df)\n",
    "            processed_df = afe_model.transform(sim_df)\n",
    "\n",
    "            # Simplified GBT Model\n",
    "            gbt = GBTClassifier(\n",
    "                labelCol=\"is_successful_resolution\",\n",
    "                featuresCol=\"features\",\n",
    "                maxDepth=1,  # Simple params\n",
    "                maxBins=8,\n",
    "                maxIter=2\n",
    "            )\n",
    "            gbt_model = gbt.fit(processed_df)\n",
    "\n",
    "            fallback_training_duration = time.time() - start_time_fallback\n",
    "\n",
    "            # Define fallback metrics\n",
    "            metrics = {\"accuracy\": 0.75, \"f1\": 0.7, \"auc\": 0.65, \"precision\": 0.72}\n",
    "            save_model(afe_model, gbt_model, metrics)\n",
    "\n",
    "            # Update dashboard with fallback model metrics\n",
    "            with dashboard_lock:\n",
    "                dashboard_data[\"metrics\"] = metrics\n",
    "                dashboard_data[\"last_training\"] = {\n",
    "                    \"timestamp\": datetime.now(),\n",
    "                    \"metrics\": metrics,\n",
    "                    \"record_count\": 100,  # Fallback sim size\n",
    "                    \"training_duration\": fallback_training_duration\n",
    "                }\n",
    "                # Reset confusion matrix for fallback\n",
    "                dashboard_data[\"confusion_matrix\"] = {\"tp\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0}\n",
    "\n",
    "            sim_df.unpersist()  # Clean up persisted fallback data\n",
    "\n",
    "            logger.info(f\"Fallback models created successfully in {fallback_training_duration:.2f} seconds\")\n",
    "            return True, afe_model, gbt_model\n",
    "        except Exception as fallback_e:\n",
    "            logger.error(f\"Failed to create fallback models: {fallback_e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False, None, None\n",
    "        \n",
    "        \n",
    "        \n",
    "def run_streaming_phase(afe_model=None, gbt_model=None):\n",
    "    \"\"\"Run the streaming inference phase of the pipeline.\"\"\"\n",
    "    try:\n",
    "        logger.info(\"=== PHASE 2: STREAMING INFERENCE ===\")\n",
    "        \n",
    "        # Add detailed diagnostics\n",
    "        logger.info(\"Preparing to initialize Spark for streaming phase\")\n",
    "        \n",
    "        try:\n",
    "            spark = initialize_spark() # Get a fresh Spark session for streaming simulation\n",
    "            logger.info(\"Successfully initialized Spark for streaming phase\")\n",
    "        except Exception as spark_e:\n",
    "            logger.error(f\"Failed to initialize Spark for streaming: {spark_e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            raise  # Re-raise to be caught by outer try-except\n",
    "\n",
    "        if not afe_model or not gbt_model:\n",
    "            logger.info(\"Models not passed, attempting to load from disk...\")\n",
    "            try:\n",
    "                afe_model, gbt_model = load_models()\n",
    "                logger.info(\"Successfully loaded models from disk\")\n",
    "            except Exception as model_e:\n",
    "                logger.error(f\"Failed to load models: {model_e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                raise  # Re-raise to be caught by outer try-except\n",
    "\n",
    "        if not afe_model or not gbt_model:\n",
    "            logger.error(\"No models available (neither passed nor loaded). Cannot start streaming phase.\")\n",
    "            return False\n",
    "            \n",
    "        logger.info(\"About to start streaming simulation thread\")\n",
    "        \n",
    "        # Run the simulation in a separate thread\n",
    "        streaming_thread = threading.Thread(\n",
    "            target=simulate_streaming_inference,\n",
    "            args=(spark, afe_model, gbt_model, 2.0, MAX_BATCH_SIZE), # Pass interval and batch size\n",
    "            daemon=True # Allows main thread to exit even if this thread is running\n",
    "        )\n",
    "        streaming_thread.start()\n",
    "\n",
    "        # Verify thread started\n",
    "        if streaming_thread.is_alive():\n",
    "            logger.info(\"Simulated streaming inference thread started successfully\")\n",
    "            return True\n",
    "        else:\n",
    "            logger.error(\"Thread creation succeeded but thread is not running!\")\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error initializing streaming phase: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def run_full_pipeline():\n",
    "    \"\"\"Run the complete pipeline including batch and streaming phases.\"\"\"\n",
    "    try:\n",
    "        logger.info(\"=== STARTING SOLOSOLVE AI PIPELINE (OPTIMIZED) ===\")\n",
    "        gc.collect()\n",
    "\n",
    "        # 1. Create Dashboard Structure (even if Dash isn't running yet)\n",
    "        app = create_dashboard()\n",
    "\n",
    "        # 2. Run Batch Phase\n",
    "        batch_success, afe_model, gbt_model = run_batch_phase()\n",
    "\n",
    "        # 3. Handle Batch Failure (attempt to load models)\n",
    "        if not batch_success:\n",
    "            logger.error(\"Batch phase failed, attempting to load existing/fallback models...\")\n",
    "            afe_model, gbt_model = load_models()\n",
    "            if not afe_model or not gbt_model:\n",
    "                logger.error(\"Failed to load any models after batch phase failure. Cannot continue.\")\n",
    "                return False\n",
    "            else:\n",
    "                logger.info(\"Successfully loaded existing/fallback models.\")\n",
    "\n",
    "        # 4. Start Dashboard Server (if available and created)\n",
    "        dashboard_thread = None\n",
    "        if app and DASH_AVAILABLE:\n",
    "            try:\n",
    "                dashboard_thread = threading.Thread(\n",
    "                    target=lambda: app.run_server(\n",
    "                        mode=\"inline\" if 'ipykernel' in sys.modules else \"external\",\n",
    "                        port=8050,\n",
    "                        debug=False, # Disable debug for production/stability\n",
    "                        threaded=True # Important for running alongside streaming\n",
    "                    ),\n",
    "                    daemon=True\n",
    "                )\n",
    "                dashboard_thread.start()\n",
    "                logger.info(\"Dashboard server thread started. Access at http://localhost:8050 (might take a moment)\")\n",
    "            except Exception as dash_e:\n",
    "                logger.error(f\"Error starting dashboard server thread: {dash_e}\")\n",
    "                logger.info(\"Continuing without dashboard server.\")\n",
    "\n",
    "        # ADD THIS BLOCK - Diagnostic check before streaming\n",
    "        logger.info(\"===== DIAGNOSTIC CHECK BEFORE STREAMING =====\")\n",
    "        try:\n",
    "            memory_info = {}\n",
    "            try:\n",
    "                import psutil\n",
    "                process = psutil.Process()\n",
    "                memory_info = {\n",
    "                    \"rss_mb\": process.memory_info().rss / (1024 * 1024),\n",
    "                    \"system_available_gb\": psutil.virtual_memory().available / (1024 * 1024 * 1024)\n",
    "                }\n",
    "                logger.info(f\"Memory before streaming: Process RSS: {memory_info['rss_mb']:.1f}MB, System Available: {memory_info['system_available_gb']:.1f}GB\")\n",
    "            except ImportError:\n",
    "                logger.info(\"psutil not available for memory diagnostics\")\n",
    "            \n",
    "            # Check if models are valid\n",
    "            logger.info(f\"Model check: AFE model exists: {afe_model is not None}, GBT model exists: {gbt_model is not None}\")\n",
    "            \n",
    "            # Force GC before streaming\n",
    "            gc.collect()\n",
    "            logger.info(\"Forced garbage collection before streaming\")\n",
    "        except Exception as diag_e:\n",
    "            logger.error(f\"Diagnostic check error: {diag_e}\")\n",
    "\n",
    "        # 5. Run Streaming Phase\n",
    "        streaming_success = run_streaming_phase(afe_model, gbt_model)\n",
    "\n",
    "        # 6. Handle Streaming Failure (Simplified Dummy Loop)\n",
    "        if not streaming_success:\n",
    "            logger.error(\"Streaming phase failed to start properly. Running simplified dummy update loop.\")\n",
    "            try:\n",
    "                # No Spark needed for dummy loop\n",
    "                logger.info(\"Starting simplified dummy dashboard update loop...\")\n",
    "                for i in range(60): # Run for about 5 minutes\n",
    "                    time.sleep(5)\n",
    "                    dummy_preds = [{\"Complaint ID\": f\"DUMMY-{i*10+j}\",\n",
    "                                    \"prediction\": float((i+j) % 2), # Alternate predictions\n",
    "                                    \"State\": [\"CA\", \"NY\", \"TX\", \"FL\", \"IL\"][j%5],\n",
    "                                    \"Company\": [\"DummyCo A\", \"DummyCo B\"][j%2]}\n",
    "                                   for j in range(10)] # Simulate 10 records\n",
    "                    # Use the dashboard update function with dummy data\n",
    "                    update_dashboard_with_predictions(pd.DataFrame(dummy_preds))\n",
    "\n",
    "                logger.info(\"Simplified dummy update loop finished.\")\n",
    "                streaming_success = True # Mark as \"running\" for status message\n",
    "            except Exception as dummy_e:\n",
    "                logger.error(f\"Simplified dummy streaming also failed: {dummy_e}\")\n",
    "\n",
    "        # 7. Main Loop (Keep alive)\n",
    "        logger.info(\"=== PIPELINE RUNNING ===\")\n",
    "        logger.info(f\"- Batch processing completed (Success: {batch_success})\")\n",
    "        if streaming_success:\n",
    "            logger.info(\"- Streaming inference active (Simulated)\")\n",
    "        else:\n",
    "            logger.warning(\"- Streaming inference FAILED to start\")\n",
    "        if app and DASH_AVAILABLE and dashboard_thread and dashboard_thread.is_alive():\n",
    "            logger.info(\"- Dashboard available at http://localhost:8050\")\n",
    "        else:\n",
    "            logger.info(\"- Dashboard is NOT available.\")\n",
    "        logger.info(\">>> Press Ctrl+C to stop <<<\")\n",
    "\n",
    "        try:\n",
    "            cycle = 0\n",
    "            while True:\n",
    "                time.sleep(60) # Check less frequently in main loop\n",
    "                cycle += 1\n",
    "                logger.debug(f\"Pipeline heartbeat cycle {cycle}\")\n",
    "                if cycle % 5 == 0: # Trigger GC less often\n",
    "                    gc.collect()\n",
    "                    logger.info(\"Periodic garbage collection triggered.\")\n",
    "                # Optional: Check thread health\n",
    "                if dashboard_thread and not dashboard_thread.is_alive() and DASH_AVAILABLE:\n",
    "                    logger.warning(\"Dashboard thread seems to have stopped unexpectedly.\")\n",
    "                # Could add similar check for streaming thread if it wasn't daemonized\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"Pipeline interrupted by user (Ctrl+C). Shutting down...\")\n",
    "\n",
    "        logger.info(\"Pipeline shutdown initiated.\")\n",
    "        return True # Indicate successful run until interruption\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unhandled exception in run_full_pipeline: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False # Indicate pipeline failure\n",
    "    \n",
    "# =============================================================================\n",
    "# KAFKA FUNCTIONS (Using versions from corrected snippets)\n",
    "# =============================================================================\n",
    "def setup_kafka_topics():\n",
    "    \"\"\"Set up Kafka topics if Kafka is available.\"\"\"\n",
    "    if not KAFKA_AVAILABLE:\n",
    "        logger.info(\"Kafka client not available. Skipping Kafka topic creation.\")\n",
    "        return True\n",
    "\n",
    "    try:\n",
    "        logger.info(f\"Attempting to connect to Kafka AdminClient at {KAFKA_BOOTSTRAP_SERVERS}\")\n",
    "        admin_client = KafkaAdminClient(\n",
    "            bootstrap_servers=[KAFKA_BOOTSTRAP_SERVERS],\n",
    "            client_id='solosolve-admin',\n",
    "            request_timeout_ms=5000 # Add timeout\n",
    "        )\n",
    "\n",
    "        existing_topics = admin_client.list_topics()\n",
    "        logger.info(f\"Existing Kafka topics: {existing_topics}\")\n",
    "\n",
    "        new_topics = []\n",
    "        for topic_key, topic_name in KAFKA_TOPICS.items():\n",
    "            if topic_name not in existing_topics:\n",
    "                logger.info(f\"Topic '{topic_name}' not found, scheduling for creation.\")\n",
    "                new_topics.append(NewTopic(\n",
    "                    name=topic_name,\n",
    "                    num_partitions=1,      # Simple setup\n",
    "                    replication_factor=1 # Simple setup (requires broker config change if > 1)\n",
    "                ))\n",
    "\n",
    "        if new_topics:\n",
    "            logger.info(f\"Creating Kafka topics: {[t.name for t in new_topics]}\")\n",
    "            admin_client.create_topics(new_topics, timeout_ms=10000) # Add timeout\n",
    "            logger.info(\"Kafka topics created successfully.\")\n",
    "        else:\n",
    "            logger.info(\"All required Kafka topics already exist.\")\n",
    "\n",
    "        admin_client.close()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error setting up Kafka topics: {e}\")\n",
    "        logger.warning(\"Proceeding without Kafka setup verification. Streaming might fail if topics don't exist.\")\n",
    "        return False\n",
    "\n",
    "def kafka_producer_job(topic, data, interval=2.0, batch_size=5):\n",
    "    \"\"\"Send data to Kafka topic if Kafka is available.\"\"\"\n",
    "    if not KAFKA_AVAILABLE:\n",
    "        logger.info(\"Kafka client not available. Skipping Kafka producer job.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        logger.info(f\"Starting Kafka producer for topic '{topic}' with interval {interval}s, batch size {batch_size}\")\n",
    "        producer = KafkaProducer(\n",
    "            bootstrap_servers=[KAFKA_BOOTSTRAP_SERVERS],\n",
    "            value_serializer=lambda x: json.dumps(x).encode('utf-8'), # Serialize data to JSON bytes\n",
    "            compression_type='gzip', # Enable compression\n",
    "            batch_size=16384,        # Default batch size\n",
    "            linger_ms=100,           # Wait up to 100ms to batch records\n",
    "            max_in_flight_requests_per_connection=1 # Ensure ordering per partition\n",
    "        )\n",
    "\n",
    "        records = data.to_dict('records') if hasattr(data, 'to_dict') else data # Handle Pandas DF or list of dicts\n",
    "\n",
    "        record_count = 0\n",
    "        for i in range(0, len(records), batch_size):\n",
    "            batch = records[i:i+batch_size]\n",
    "            for record in batch:\n",
    "                # Send a minimal record structure\n",
    "                minimal_record = {\n",
    "                    \"id\": record.get(\"Complaint ID\", f\"unknown-{int(time.time())}\"), # Include timestamp in fallback ID\n",
    "                    \"text\": record.get(\"Consumer complaint narrative\", \"\")[:200], # Limit text size\n",
    "                    \"timestamp\": datetime.now().isoformat() # Add processing timestamp\n",
    "                }\n",
    "                producer.send(topic, value=minimal_record)\n",
    "                record_count += 1\n",
    "\n",
    "            producer.flush() # Ensure messages in the current batch are sent\n",
    "            logger.info(f\"Sent {len(batch)} records (total {record_count}) to Kafka topic '{topic}'\")\n",
    "            time.sleep(interval) # Wait before sending the next batch\n",
    "\n",
    "        producer.close()\n",
    "        logger.info(f\"Kafka producer job for topic '{topic}' finished.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in Kafka producer job for topic '{topic}': {e}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Set Java options for Spark driver (helps with memory management)\n",
    "    os.environ['_JAVA_OPTIONS'] = '-Xmx3g -XX:+UseG1GC -XX:+UseCompressedOops -XX:+AlwaysPreTouch'\n",
    "\n",
    "    try:\n",
    "        import psutil\n",
    "        mem = psutil.virtual_memory()\n",
    "        logger.info(f\"System Memory - Total: {mem.total / (1024**3):.1f}GB, Available: {mem.available / (1024**3):.1f}GB\")\n",
    "    except ImportError:\n",
    "        logger.info(\"psutil not available, skipping system memory info.\")\n",
    "\n",
    "    gc.enable() # Ensure garbage collection is enabled\n",
    "    gc.collect() # Run GC before starting\n",
    "\n",
    "    # Optional: Setup Kafka topics if Kafka is used\n",
    "    # setup_kafka_topics() # Uncomment if using real Kafka streaming\n",
    "\n",
    "    pipeline_success = False\n",
    "    try:\n",
    "        pipeline_success = run_full_pipeline()\n",
    "        if pipeline_success:\n",
    "            logger.info(\"Pipeline execution loop finished normally (likely via Ctrl+C).\")\n",
    "        else:\n",
    "            logger.error(\"Pipeline execution failed.\")\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Pipeline execution interrupted by user in main block.\")\n",
    "    except Exception as main_e:\n",
    "        logger.error(f\"Unhandled exception in main execution block: {main_e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        logger.info(\"Attempting to stop Spark context if active...\")\n",
    "        try:\n",
    "            # Check if a SparkContext exists and stop it\n",
    "            sc = SparkContext._jvm.SparkContext.getOrCreate(initialize_spark().sparkContext._jsc.sc())\n",
    "            if sc is not None :\n",
    "                 logger.info(\"Active Spark context found, stopping...\")\n",
    "                 sc.stop()\n",
    "                 logger.info(\"Spark context stopped.\")\n",
    "            else:\n",
    "                 logger.info(\"No active Spark context found to stop.\")\n",
    "            # Alternative way, might be cleaner\n",
    "            # if 'spark' in locals() and isinstance(spark, SparkSession):\n",
    "            #    spark.stop()\n",
    "        except Exception as stop_e:\n",
    "            logger.warning(f\"Could not stop Spark context gracefully: {stop_e}\")\n",
    "\n",
    "        logger.info(\"=== Solosolve AI Pipeline Shutdown Complete ===\")\n",
    "        sys.exit(0 if pipeline_success else 1) # Exit with appropriate code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
