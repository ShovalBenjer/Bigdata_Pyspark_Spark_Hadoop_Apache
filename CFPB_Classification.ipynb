{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShovalBenjer/Bigdata_Pyspark_Spark_Hadoop_Apache/blob/Final_Project/CFPB_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install kafka-python\n",
        "!pip install transformers\n",
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0J_3jZHarDqm",
        "outputId": "d78d410f-e26b-401a-ac00-37c08557a974"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kafka-python\n",
            "  Downloading kafka_python-2.1.3-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Downloading kafka_python-2.1.3-py2.py3-none-any.whl (276 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/276.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m266.2/276.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.1/276.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kafka-python\n",
            "Successfully installed kafka-python-2.1.3\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# Comprehensive Big Data Pipeline with Spark Structured Streaming, Kafka, and Superset Integration\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "from pyspark.ml.feature import (\n",
        "    Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer,\n",
        "    OneHotEncoder, VectorAssembler, RegexTokenizer\n",
        ")\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.functions import vector_to_array\n",
        "from pyspark.ml.base import Transformer\n",
        "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
        "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
        "from pyspark.ml.torch.distributor import TorchDistributor\n",
        "from pyspark.sql.streaming import StreamingQueryListener\n",
        "from kafka import KafkaProducer\n",
        "\n",
        "# Import BERT-related libraries for distributed training\n",
        "from transformers import DistilBertTokenizer, DistilBertModel, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
        "\n",
        "# Configuration variables\n",
        "CSV_FILE_PATH = \"/content/Consumer_Complaints.csv\"\n",
        "TEST_DATA_PERSISTENCE_PATH = \"/content/consumer_complaints/data/test_data_source.parquet\"\n",
        "TRAINING_PIPELINE_SAVE_PATH = \"/content/consumer_complaints/models/training_pipeline\"\n",
        "BEST_MODEL_SAVE_PATH = \"/content/consumer_complaints/models/best_model\"\n",
        "EMBEDDING_MODEL_SAVE_PATH = \"/content/consumer_complaints/models/embedding_model\"\n",
        "TEMP_PYTORCH_DATA_PATH = \"/content/consumer_complaints/models/temp_pytorch_data\"\n",
        "STREAMING_CHECKPOINT_LOCATION = \"/content/consumer_complaints/checkpoints\"\n",
        "SUPERSET_API_ENDPOINT = \"http://localhost:8088/api/v1\"  # Example Superset endpoint\n",
        "\n",
        "# Kafka configuration\n",
        "KAFKA_BROKERS = \"kafka1:9092,kafka2:9092\"\n",
        "KAFKA_TOPIC_RAW = \"complaints-raw\"\n",
        "KAFKA_TOPIC_TRAINING = \"complaints-training-data\"\n",
        "KAFKA_TOPIC_TESTING_STREAM = \"complaints-testing-stream\"\n",
        "KAFKA_TOPIC_PREDICTIONS = \"complaint-predictions\"\n",
        "KAFKA_TOPIC_METRICS = \"streaming-metrics\"\n",
        "\n",
        "# Parameters for simulation\n",
        "MESSAGES_PER_MINUTE = 1000  # Target throughput\n",
        "BATCH_SIZE = 100  # Send messages in batches for efficiency\n",
        "\n",
        "# Create directories\n",
        "for path in [\n",
        "    \"/content/consumer_complaints/data\",\n",
        "    \"/content/consumer_complaints/models\",\n",
        "    \"/content/consumer_complaints/models/temp_pytorch_data\",\n",
        "    \"/content/consumer_complaints/checkpoints\"\n",
        "]:\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Consumer Complaints ML Pipeline\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1\") \\\n",
        "    .config(\"spark.sql.streaming.checkpointLocation\", STREAMING_CHECKPOINT_LOCATION) \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark Configuration:\")\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"Application ID: {spark.sparkContext.applicationId}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ota6Keiaqm2p",
        "outputId": "10861210-4233-4b31-f982-3c0e1a40f37d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Configuration:\n",
            "Spark Version: 3.5.5\n",
            "Application ID: local-1743097057732\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################\n",
        "### Schema Definition             ###\n",
        "#####################################\n",
        "\n",
        "def get_full_schema():\n",
        "    \"\"\"Return the full schema for consumer complaints data\"\"\"\n",
        "    return StructType([\n",
        "        StructField(\"Date received\", StringType(), True),\n",
        "        StructField(\"Product\", StringType(), True),\n",
        "        StructField(\"Sub-product\", StringType(), True),\n",
        "        StructField(\"Issue\", StringType(), True),\n",
        "        StructField(\"Sub-issue\", StringType(), True),\n",
        "        StructField(\"Consumer complaint narrative\", StringType(), True),\n",
        "        StructField(\"Company public response\", StringType(), True),\n",
        "        StructField(\"Company\", StringType(), True),\n",
        "        StructField(\"State\", StringType(), True),\n",
        "        StructField(\"ZIP code\", StringType(), True),\n",
        "        StructField(\"Tags\", StringType(), True),\n",
        "        StructField(\"Consumer consent provided?\", StringType(), True),\n",
        "        StructField(\"Submitted via\", StringType(), True),\n",
        "        StructField(\"Date sent to company\", StringType(), True),\n",
        "        StructField(\"Company response to consumer\", StringType(), True),\n",
        "        StructField(\"Timely response?\", StringType(), True),\n",
        "        StructField(\"Consumer disputed?\", StringType(), True),\n",
        "        StructField(\"Complaint ID\", StringType(), True)\n",
        "    ])\n",
        "\n",
        "def get_streaming_schema():\n",
        "    \"\"\"Return the schema for streaming inference data\"\"\"\n",
        "    return StructType([\n",
        "        StructField(\"Date received\", StringType(), True),\n",
        "        StructField(\"Complaint ID\", StringType(), True),\n",
        "        StructField(\"Company\", StringType(), True),\n",
        "        StructField(\"State\", StringType(), True),\n",
        "        StructField(\"ZIP code\", StringType(), True),\n",
        "        StructField(\"Submitted via\", StringType(), True),\n",
        "        StructField(\"Consumer complaint narrative\", StringType(), True)\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "ypcKEvTosHCt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################\n",
        "### Custom Transformers           ###\n",
        "#####################################\n",
        "\n",
        "# Enhanced transformer for BERT embeddings\n",
        "class BERTEmbeddingTransformer(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
        "    \"\"\"Custom MLWritable transformer that applies BERT embeddings to text\"\"\"\n",
        "\n",
        "    def __init__(self, inputCol=None, outputCol=None, modelPath=None):\n",
        "        super(BERTEmbeddingTransformer, self).__init__()\n",
        "        self.inputCol = Param(self, \"inputCol\", \"Input column\")\n",
        "        self.outputCol = Param(self, \"outputCol\", \"Output column\")\n",
        "        self.modelPath = Param(self, \"modelPath\", \"Path to the BERT model\")\n",
        "        self._setDefault(inputCol=None, outputCol=None, modelPath=None)\n",
        "        self.setInputCol(inputCol)\n",
        "        self.setOutputCol(outputCol)\n",
        "        self.setModelPath(modelPath)\n",
        "        self._bert_embed_udf = None\n",
        "\n",
        "    def setInputCol(self, value): return self._set(inputCol=value)\n",
        "    def getInputCol(self): return self.getOrDefault(self.inputCol)\n",
        "    def setOutputCol(self, value): return self._set(outputCol=value)\n",
        "    def getOutputCol(self): return self.getOrDefault(self.outputCol)\n",
        "    def setModelPath(self, value): return self._set(modelPath=value)\n",
        "    def getModelPath(self): return self.getOrDefault(self.modelPath)\n",
        "\n",
        "    def _transform(self, dataset):\n",
        "        from pyspark.sql.functions import pandas_udf\n",
        "        import pandas as pd\n",
        "\n",
        "        # Using pandas_udf to leverage vectorized operations\n",
        "        @pandas_udf(ArrayType(FloatType()))\n",
        "        def bert_embed_batch(texts_series):\n",
        "            # Load model once per executor\n",
        "            if not hasattr(bert_embed_batch, 'model') or bert_embed_batch.model is None:\n",
        "                modelPath = self.getModelPath()\n",
        "                tokenizer = DistilBertTokenizer.from_pretrained(modelPath)\n",
        "                model = DistilBertModel.from_pretrained(modelPath)\n",
        "                model.to(\"cpu\").eval()\n",
        "                bert_embed_batch.model = model\n",
        "                bert_embed_batch.tokenizer = tokenizer\n",
        "                bert_embed_batch.embedding_dim = model.config.dim\n",
        "\n",
        "            # Process batch of texts\n",
        "            results = []\n",
        "            for text in texts_series:\n",
        "                if not text or len(str(text).strip()) == 0:\n",
        "                    results.append([0.0] * bert_embed_batch.embedding_dim)\n",
        "                    continue\n",
        "\n",
        "                # Tokenize and get embeddings\n",
        "                inputs = bert_embed_batch.tokenizer(\n",
        "                    str(text),\n",
        "                    return_tensors=\"pt\",\n",
        "                    truncation=True,\n",
        "                    max_length=128,\n",
        "                    padding=\"max_length\"\n",
        "                )\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    outputs = bert_embed_batch.model(**inputs)\n",
        "                embeddings = outputs.last_hidden_state[:, 0, :].squeeze().tolist()\n",
        "                results.append(embeddings)\n",
        "\n",
        "            return pd.Series(results)\n",
        "\n",
        "        # Transform the dataset\n",
        "        return dataset.withColumn(\n",
        "            self.getOutputCol(),\n",
        "            bert_embed_batch(F.col(self.getInputCol()))\n",
        "        )\n",
        "\n",
        "    def copy(self, extra=None):\n",
        "        if extra is None: extra = {}\n",
        "        return super(BERTEmbeddingTransformer, self).copy(extra)\n",
        "\n",
        "#####################################\n",
        "### PyTorch BERT Training Classes ###\n",
        "#####################################\n",
        "\n",
        "# Dataset class for PyTorch\n",
        "class ComplaintDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self): return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text, add_special_tokens=True, max_length=self.max_length,\n",
        "            padding='max_length', truncation=True, return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Enhanced DistilBERT classifier model\n",
        "class EnhancedDistilBERTClassifier(torch.nn.Module):\n",
        "    def __init__(self, bert_model, dropout_rate=0.3):\n",
        "        super(EnhancedDistilBERTClassifier, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.dropout1 = torch.nn.Dropout(dropout_rate)\n",
        "\n",
        "        hidden_size = self.bert.config.dim\n",
        "        self.dense1 = torch.nn.Linear(hidden_size, 256)\n",
        "        self.batch_norm1 = torch.nn.BatchNorm1d(256)\n",
        "        self.dropout2 = torch.nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.dense2 = torch.nn.Linear(256, 64)\n",
        "        self.batch_norm2 = torch.nn.BatchNorm1d(64)\n",
        "        self.dropout3 = torch.nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.classifier = torch.nn.Linear(64, 2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        x = self.dropout1(outputs.last_hidden_state[:, 0, :])\n",
        "        x = self.dense1(x)\n",
        "        x = self.batch_norm1(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "\n",
        "        x = self.dropout2(x)\n",
        "        x = self.dense2(x)\n",
        "        x = self.batch_norm2(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "\n",
        "        x = self.dropout3(x)\n",
        "        return self.classifier(x)"
      ],
      "metadata": {
        "id": "FN1AVDoZsOyC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stream metrics listener\n",
        "class MetricsListener(StreamingQueryListener):\n",
        "    def __init__(self, kafka_brokers, topic):\n",
        "        self.kafka_brokers = kafka_brokers\n",
        "        self.topic = topic\n",
        "        self.producer = KafkaProducer(\n",
        "            bootstrap_servers=kafka_brokers.split(','),\n",
        "            value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
        "        )\n",
        "\n",
        "    def onQueryStarted(self, event):\n",
        "        metrics = {\n",
        "            \"queryName\": event.name,\n",
        "            \"id\": str(event.id),\n",
        "            \"runId\": str(event.runId),\n",
        "            \"timestamp\": event.timestamp,\n",
        "            \"event\": \"started\"\n",
        "        }\n",
        "        self.producer.send(self.topic, value=metrics)\n",
        "\n",
        "    def onQueryProgress(self, event):\n",
        "        progress = event.progress\n",
        "        metrics = {\n",
        "            \"queryName\": progress.name,\n",
        "            \"id\": str(progress.id),\n",
        "            \"runId\": str(progress.runId),\n",
        "            \"timestamp\": progress.timestamp,\n",
        "            \"event\": \"progress\",\n",
        "            \"numInputRows\": progress.numInputRows,\n",
        "            \"inputRowsPerSecond\": progress.inputRowsPerSecond,\n",
        "            \"processedRowsPerSecond\": progress.processedRowsPerSecond,\n",
        "            \"batchId\": progress.batchId\n",
        "        }\n",
        "        self.producer.send(self.topic, value=metrics)\n",
        "\n",
        "    def onQueryTerminated(self, event):\n",
        "        metrics = {\n",
        "            \"queryName\": event.name if hasattr(event, 'name') else None,\n",
        "            \"id\": str(event.id),\n",
        "            \"runId\": str(event.runId),\n",
        "            \"timestamp\": time.time() * 1000,\n",
        "            \"event\": \"terminated\",\n",
        "            \"exception\": str(event.exception) if event.exception else None\n",
        "        }\n",
        "        self.producer.send(self.topic, value=metrics)"
      ],
      "metadata": {
        "id": "7tLUXPg0sZa5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "### TASK 1: Load Data into Kafka      ###\n",
        "#########################################\n",
        "\n",
        "def load_data_to_kafka():\n",
        "    \"\"\"Load Consumer Complaints data into Kafka with proper error handling\"\"\"\n",
        "    print(f\"Reading CSV from: {CSV_FILE_PATH}\")\n",
        "    full_schema = get_full_schema()\n",
        "\n",
        "    # Using Spark SQL to read and process the data\n",
        "    raw_df = spark.read.format(\"csv\") \\\n",
        "                   .option(\"header\", \"true\") \\\n",
        "                   .schema(full_schema) \\\n",
        "                   .load(CSV_FILE_PATH)\n",
        "\n",
        "    # Register as temp view for SQL queries\n",
        "    raw_df.createOrReplaceTempView(\"raw_complaints\")\n",
        "\n",
        "    # Use Spark SQL to analyze data\n",
        "    summary_df = spark.sql(\"\"\"\n",
        "        SELECT\n",
        "            Product,\n",
        "            COUNT(*) as complaint_count,\n",
        "            SUM(CASE WHEN `Consumer complaint narrative` IS NOT NULL THEN 1 ELSE 0 END) as narratives_count,\n",
        "            SUM(CASE WHEN `Consumer disputed?` = 'Yes' THEN 1 ELSE 0 END) as disputed_count\n",
        "        FROM raw_complaints\n",
        "        GROUP BY Product\n",
        "        ORDER BY complaint_count DESC\n",
        "    \"\"\")\n",
        "\n",
        "    print(\"Data summary by product:\")\n",
        "    summary_df.show(5)\n",
        "\n",
        "    print(f\"Total records loaded: {raw_df.count()}\")\n",
        "\n",
        "    # Write to Kafka topic using improved error handling\n",
        "    try:\n",
        "        print(f\"Writing data to Kafka topic: {KAFKA_TOPIC_RAW}\")\n",
        "\n",
        "        # IMPORTANT: Use backticks around column names with spaces\n",
        "        kafka_df = raw_df.selectExpr(\n",
        "            \"`Complaint ID` AS key\",\n",
        "            \"to_json(struct(*)) AS value\"\n",
        "        )\n",
        "\n",
        "        # Add version-specific options for better compatibility\n",
        "        kafka_df.write \\\n",
        "            .format(\"kafka\") \\\n",
        "            .option(\"kafka.bootstrap.servers\", KAFKA_BROKERS) \\\n",
        "            .option(\"topic\", KAFKA_TOPIC_RAW) \\\n",
        "            .option(\"kafka.max.block.ms\", \"600000\")  # Increase timeout to 10 minutes\n",
        "            .option(\"kafka.acks\", \"1\")  # Use less strict acknowledgment\n",
        "            .save()\n",
        "\n",
        "        print(f\"Data successfully written to Kafka topic: {KAFKA_TOPIC_RAW}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing to Kafka: {e}\")\n",
        "        print(\"Falling back to alternative storage method...\")\n",
        "\n",
        "        # Fallback: Save as parquet if Kafka fails\n",
        "        fallback_path = \"/content/consumer_complaints/data/raw_data.parquet\"\n",
        "        raw_df.write.mode(\"overwrite\").parquet(fallback_path)\n",
        "        print(f\"Data saved to fallback location: {fallback_path}\")\n",
        "\n",
        "    return raw_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "2wPitj2qsTS-",
        "outputId": "c1093684-4319-400e-e72b-495af7f72ce3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-19-9853db49cb1b>, line 52)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-9853db49cb1b>\"\u001b[0;36m, line \u001b[0;32m52\u001b[0m\n\u001b[0;31m    .option(\"kafka.acks\", \"1\")  # Use less strict acknowledgment\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install AutoViz and related dependencies\n",
        "!pip install autoviz\n",
        "\n",
        "def visualize_with_autoviz(filtered_df, max_rows=10000, max_cols=30, save_dir=None):\n",
        "    \"\"\"\n",
        "    Visualize the filtered data using AutoViz\n",
        "\n",
        "    Args:\n",
        "        filtered_df: The filtered DataFrame to visualize\n",
        "        max_rows: Maximum number of rows to analyze\n",
        "        max_cols: Maximum number of columns to analyze\n",
        "        save_dir: Directory to save visualizations (None for default)\n",
        "\n",
        "    Returns:\n",
        "        The visualization DataFrame\n",
        "    \"\"\"\n",
        "    from autoviz import AutoViz_Class\n",
        "    import os\n",
        "\n",
        "    print(\"Starting AutoViz data visualization...\")\n",
        "\n",
        "    # Create the save directory for AutoViz plots if provided\n",
        "    if save_dir:\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        print(f\"Created directory for AutoViz plots: {save_dir}\")\n",
        "\n",
        "    # Sample the data if it's too large\n",
        "    if filtered_df.count() > max_rows:\n",
        "        print(f\"Sampling {max_rows} rows from {filtered_df.count()} total rows\")\n",
        "        sample_df = filtered_df.sample(False, max_rows / filtered_df.count(), seed=42)\n",
        "    else:\n",
        "        sample_df = filtered_df\n",
        "\n",
        "    # Convert to Pandas for AutoViz\n",
        "    pandas_df = sample_df.toPandas()\n",
        "\n",
        "    # Create AutoViz instance\n",
        "    AV = AutoViz_Class()\n",
        "\n",
        "    # Generate visualizations\n",
        "    print(\"Generating visualizations with AutoViz...\")\n",
        "    viz_df = AV.AutoViz(\n",
        "        \"\",  # Empty filename since we're using a DataFrame\n",
        "        depVar=\"\",  # No target variable specified yet\n",
        "        dfte=pandas_df,\n",
        "        header=0,  # Header is in first row\n",
        "        verbose=1,  # Show info and charts\n",
        "        lowess=False,  # Disable lowess for larger datasets\n",
        "        chart_format=\"html\",  # Save as interactive HTML\n",
        "        max_rows_analyzed=max_rows,\n",
        "        max_cols_analyzed=max_cols,\n",
        "        save_plot_dir=save_dir or \"/content/consumer_complaints/visualizations\"\n",
        "    )\n",
        "\n",
        "    print(\"AutoViz visualization complete!\")\n",
        "    print(f\"Visualizations saved in: {save_dir or '/content/consumer_complaints/visualizations'}\")\n",
        "\n",
        "    return viz_df"
      ],
      "metadata": {
        "id": "ObbuNhkUxlZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "### TASK 2: Preprocess & Filter Data  ###\n",
        "#########################################\n",
        "\n",
        "# Update the preprocessing and filtering function to include visualization\n",
        "def preprocess_filter_and_visualize():\n",
        "    \"\"\"\n",
        "    Read from Kafka raw topic, filter records, and visualize the results\n",
        "    \"\"\"\n",
        "    # Original preprocess_and_filter code\n",
        "    print(f\"Reading data from Kafka topic: {KAFKA_TOPIC_RAW}\")\n",
        "    full_schema = get_full_schema()\n",
        "\n",
        "    # Try to read from Kafka\n",
        "    try:\n",
        "        kafka_raw_df = spark.read \\\n",
        "            .format(\"kafka\") \\\n",
        "            .option(\"kafka.bootstrap.servers\", KAFKA_BROKERS) \\\n",
        "            .option(\"subscribe\", KAFKA_TOPIC_RAW) \\\n",
        "            .option(\"startingOffsets\", \"earliest\") \\\n",
        "            .load()\n",
        "\n",
        "        # Parse JSON value\n",
        "        parsed_df = kafka_raw_df.select(\n",
        "            F.col(\"key\").cast(\"string\").alias(\"key\"),\n",
        "            F.from_json(F.col(\"value\").cast(\"string\"), full_schema).alias(\"data\")\n",
        "        ).select(\"data.*\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading from Kafka: {e}\")\n",
        "        print(\"Falling back to parquet file...\")\n",
        "        # Fallback: Read from parquet if Kafka read fails\n",
        "        fallback_path = \"/content/consumer_complaints/data/raw_data.parquet\"\n",
        "        parsed_df = spark.read.parquet(fallback_path)\n",
        "\n",
        "    # Parse date received\n",
        "    DATE_FORMAT = \"MM/dd/yyyy\"\n",
        "    with_parsed_date_df = parsed_df.withColumn(\n",
        "        \"parsed_date_received\",\n",
        "        F.to_date(F.col(\"Date received\"), DATE_FORMAT)\n",
        "    )\n",
        "\n",
        "    # Filter for non-null narratives\n",
        "    narrative_filtered_df = with_parsed_date_df.filter(\n",
        "        (F.col(\"Consumer complaint narrative\").isNotNull()) &\n",
        "        (F.length(F.trim(F.col(\"Consumer complaint narrative\"))) > 0)\n",
        "    )\n",
        "\n",
        "    # Filter for valid dates before or on 2017-03-31\n",
        "    date_filtered_df = narrative_filtered_df.filter(\n",
        "        (F.col(\"parsed_date_received\").isNotNull()) &\n",
        "        (F.col(\"parsed_date_received\") <= F.lit(\"2017-03-31\"))\n",
        "    )\n",
        "\n",
        "    # Filter out \"In progress\" responses\n",
        "    filtered_df = date_filtered_df.filter(\n",
        "        F.col(\"Company response to consumer\") != \"In progress\"\n",
        "    )\n",
        "\n",
        "    # Track record counts at each stage using Spark SQL\n",
        "    narrative_filtered_df.createOrReplaceTempView(\"narrative_filtered\")\n",
        "    date_filtered_df.createOrReplaceTempView(\"date_filtered\")\n",
        "    filtered_df.createOrReplaceTempView(\"final_filtered\")\n",
        "\n",
        "    filtering_stats = spark.sql(\"\"\"\n",
        "        SELECT\n",
        "            (SELECT COUNT(*) FROM narrative_filtered) as after_narrative_filter,\n",
        "            (SELECT COUNT(*) FROM date_filtered) as after_date_filter,\n",
        "            (SELECT COUNT(*) FROM final_filtered) as final_count\n",
        "    \"\"\")\n",
        "\n",
        "    print(\"Filtering statistics:\")\n",
        "    filtering_stats.show()\n",
        "\n",
        "    print(f\"After filtering: {filtered_df.count()} records\")\n",
        "\n",
        "    # NEW: Visualize the filtered data with AutoViz\n",
        "    print(\"Visualizing filtered data with AutoViz...\")\n",
        "    viz_df = visualize_with_autoviz(\n",
        "        filtered_df,\n",
        "        max_rows=15000,  # Analyze 15000 rows maximum\n",
        "        max_cols=20,     # Analyze 20 columns maximum\n",
        "        save_dir=\"/content/consumer_complaints/visualizations\"\n",
        "    )\n",
        "\n",
        "    return filtered_df"
      ],
      "metadata": {
        "id": "BBJ1f2l-sgjv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "########################################################\n",
        "### TASK 3: Split, Label & Prepare Data for Training ###\n",
        "########################################################\n",
        "\n",
        "def split_label_and_prepare_data(filtered_df, seed_value=42):\n",
        "    \"\"\"Split data, create target labels, save to Kafka and file storage\"\"\"\n",
        "    print(\"Starting data split and target labeling...\")\n",
        "\n",
        "    # Perform 80/20 split\n",
        "    training_base_df, test_base_df = filtered_df.randomSplit([0.8, 0.2], seed=seed_value)\n",
        "\n",
        "    # Create target label using Spark SQL for clarity\n",
        "    training_base_df.createOrReplaceTempView(\"training_base\")\n",
        "\n",
        "    training_labeled_df = spark.sql(\"\"\"\n",
        "        SELECT *,\n",
        "            CASE WHEN\n",
        "                `Consumer disputed?` = 'No' AND\n",
        "                `Timely response?` = 'Yes' AND\n",
        "                (`Company response to consumer` = 'Closed with explanation' OR\n",
        "                 `Company response to consumer` = 'Closed' OR\n",
        "                 `Company response to consumer` = 'Closed with monetary relief' OR\n",
        "                 `Company response to consumer` = 'Closed with non-monetary relief')\n",
        "            THEN 1 ELSE 0 END AS is_target_complaint\n",
        "        FROM training_base\n",
        "    \"\"\")\n",
        "\n",
        "    # Show target distribution\n",
        "    print(\"Target distribution in training data:\")\n",
        "    training_labeled_df.groupBy(\"is_target_complaint\").count().show()\n",
        "\n",
        "    # Prepare training data for Kafka\n",
        "    training_kafka_df = training_labeled_df.selectExpr(\n",
        "        \"`Complaint ID` AS key\",\n",
        "        \"to_json(struct(*)) AS value\"\n",
        "    )\n",
        "\n",
        "    # Write training data to Kafka\n",
        "    training_kafka_df.write \\\n",
        "        .format(\"kafka\") \\\n",
        "        .option(\"kafka.bootstrap.servers\", KAFKA_BROKERS) \\\n",
        "        .option(\"topic\", KAFKA_TOPIC_TRAINING) \\\n",
        "        .save()\n",
        "\n",
        "    # Save test data to parquet\n",
        "    test_base_df.write \\\n",
        "        .format(\"parquet\") \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .save(TEST_DATA_PERSISTENCE_PATH)\n",
        "\n",
        "    print(f\"Training data written to Kafka topic: {KAFKA_TOPIC_TRAINING}\")\n",
        "    print(f\"Test data saved to: {TEST_DATA_PERSISTENCE_PATH}\")\n",
        "\n",
        "    return training_labeled_df, test_base_df"
      ],
      "metadata": {
        "id": "UZFLAJZ1sjEY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#############################################################\n",
        "### TASK 4: DistilBERT Training with Distributed Training ###\n",
        "#############################################################\n",
        "\n",
        "def train_bert_model_distributed():\n",
        "    \"\"\"Train DistilBERT model using Distributed Training\"\"\"\n",
        "\n",
        "    def train_function():\n",
        "        print(\"Loading training data from Kafka...\")\n",
        "        full_schema_with_target = get_full_schema()\n",
        "        full_schema_with_target = full_schema_with_target.add(\n",
        "            StructField(\"is_target_complaint\", IntegerType(), True)\n",
        "        )\n",
        "\n",
        "        # Read from Kafka training topic\n",
        "        kafka_df = spark.read \\\n",
        "            .format(\"kafka\") \\\n",
        "            .option(\"kafka.bootstrap.servers\", KAFKA_BROKERS) \\\n",
        "            .option(\"subscribe\", KAFKA_TOPIC_TRAINING) \\\n",
        "            .option(\"startingOffsets\", \"earliest\") \\\n",
        "            .load()\n",
        "\n",
        "        # Parse JSON and extract data\n",
        "        training_df = kafka_df.select(\n",
        "            F.from_json(F.col(\"value\").cast(\"string\"), full_schema_with_target).alias(\"data\")\n",
        "        ).select(\"data.*\")\n",
        "\n",
        "        # Create balanced dataset for training\n",
        "        pos_df = training_df.filter(F.col(\"is_target_complaint\") == 1)\n",
        "        neg_df = training_df.filter(F.col(\"is_target_complaint\") == 0)\n",
        "        pos_count = pos_df.count()\n",
        "        neg_count = neg_df.count()\n",
        "\n",
        "        print(f\"Positive examples: {pos_count}, Negative examples: {neg_count}\")\n",
        "\n",
        "        # Handle imbalance if needed\n",
        "        if pos_count / (pos_count + neg_count) < 0.1:\n",
        "            print(\"Balancing dataset...\")\n",
        "            target_neg_count = min(neg_count, pos_count * 7)\n",
        "            neg_sample_df = neg_df.sample(fraction=target_neg_count / neg_count, seed=42)\n",
        "            balanced_df = pos_df.union(neg_sample_df)\n",
        "        else:\n",
        "            balanced_df = training_df\n",
        "\n",
        "        # Create train/val split\n",
        "        train_df, val_df = balanced_df.randomSplit([0.9, 0.1], seed=42)\n",
        "\n",
        "        # Convert to pandas for PyTorch\n",
        "        train_pd = train_df.limit(10000).toPandas()  # Limit to 10k examples for speed\n",
        "        val_pd = val_df.toPandas()\n",
        "\n",
        "        # Extract texts and labels\n",
        "        train_texts = train_pd[\"Consumer complaint narrative\"].tolist()\n",
        "        train_labels = train_pd[\"is_target_complaint\"].tolist()\n",
        "        val_texts = val_pd[\"Consumer complaint narrative\"].tolist()\n",
        "        val_labels = val_pd[\"is_target_complaint\"].tolist()\n",
        "\n",
        "        print(f\"Training on {len(train_texts)} examples, Validating on {len(val_texts)} examples\")\n",
        "\n",
        "        # Initialize DistilBERT\n",
        "        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "        model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        classifier = EnhancedDistilBERTClassifier(model, dropout_rate=0.3)\n",
        "\n",
        "        # Initialize data loaders\n",
        "        train_dataset = ComplaintDataset(train_texts, train_labels, tokenizer)\n",
        "        val_dataset = ComplaintDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "        train_dataloader = DataLoader(\n",
        "            train_dataset,\n",
        "            sampler=RandomSampler(train_dataset),\n",
        "            batch_size=32\n",
        "        )\n",
        "\n",
        "        val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "        # Initialize distributed training\n",
        "        if torch.cuda.is_available():\n",
        "            # Set up for multi-GPU if available\n",
        "            torch.distributed.init_process_group(backend=\"nccl\")\n",
        "            local_rank = torch.distributed.get_rank()\n",
        "            torch.cuda.set_device(local_rank)\n",
        "            device = torch.device(\"cuda\", local_rank)\n",
        "            classifier.to(device)\n",
        "        else:\n",
        "            device = torch.device(\"cpu\")\n",
        "            classifier.to(device)\n",
        "\n",
        "        # Training parameters\n",
        "        optimizer = AdamW(classifier.parameters(), lr=3e-5)\n",
        "        total_steps = len(train_dataloader) * 5  # 5 epochs\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer, num_warmup_steps=int(0.1*total_steps), num_training_steps=total_steps\n",
        "        )\n",
        "\n",
        "        # Calculate class weights\n",
        "        if sum(train_labels) > 0 and sum(train_labels) < len(train_labels):\n",
        "            pos_weight = len(train_labels) / (2 * sum(train_labels))\n",
        "            neg_weight = len(train_labels) / (2 * (len(train_labels) - sum(train_labels)))\n",
        "            class_weights = torch.tensor([neg_weight, pos_weight]).to(device)\n",
        "        else:\n",
        "            class_weights = torch.tensor([1.0, 1.0]).to(device)\n",
        "\n",
        "        # Training loop\n",
        "        best_val_f1 = 0.0\n",
        "\n",
        "        for epoch in range(5):\n",
        "            print(f\"Starting epoch {epoch+1}/5\")\n",
        "            classifier.train()\n",
        "            total_loss = 0\n",
        "\n",
        "            # Train epoch\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                logits = classifier(input_ids, attention_mask)\n",
        "                loss = torch.nn.CrossEntropyLoss(weight=class_weights)(logits, labels)\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(classifier.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                if step % 50 == 0:\n",
        "                    print(f\"Batch {step}/{len(train_dataloader)} - Loss: {loss.item():.4f}\")\n",
        "\n",
        "            avg_loss = total_loss / len(train_dataloader)\n",
        "            print(f\"Epoch {epoch+1}/5 - Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "            # Evaluate\n",
        "            classifier.eval()\n",
        "            all_preds = []\n",
        "            all_labels = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in val_dataloader:\n",
        "                    input_ids = batch['input_ids'].to(device)\n",
        "                    attention_mask = batch['attention_mask'].to(device)\n",
        "                    labels = batch['labels'].to(device)\n",
        "\n",
        "                    logits = classifier(input_ids, attention_mask)\n",
        "                    preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "                    all_preds.extend(preds)\n",
        "                    all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            # Calculate metrics\n",
        "            accuracy = sum(p == l for p, l in zip(all_preds, all_labels)) / len(all_labels)\n",
        "            true_pos = sum(p == 1 and l == 1 for p, l in zip(all_preds, all_labels))\n",
        "            false_pos = sum(p == 1 and l == 0 for p, l in zip(all_preds, all_labels))\n",
        "            false_neg = sum(p == 0 and l == 1 for p, l in zip(all_preds, all_labels))\n",
        "\n",
        "            precision = true_pos / (true_pos + false_pos) if (true_pos + false_pos) > 0 else 0\n",
        "            recall = true_pos / (true_pos + false_neg) if (true_pos + false_neg) > 0 else 0\n",
        "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "            print(f\"Validation metrics: Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "            if f1 > best_val_f1:\n",
        "                best_val_f1 = f1\n",
        "                # Save the model\n",
        "                os.makedirs(EMBEDDING_MODEL_SAVE_PATH, exist_ok=True)\n",
        "                torch.save(classifier.state_dict(), f\"{EMBEDDING_MODEL_SAVE_PATH}/classifier.pt\")\n",
        "                classifier.bert.save_pretrained(EMBEDDING_MODEL_SAVE_PATH)\n",
        "                tokenizer.save_pretrained(EMBEDDING_MODEL_SAVE_PATH)\n",
        "                print(f\"New best model saved with F1: {f1:.4f}\")\n",
        "\n",
        "        # Clean up distributed process group if used\n",
        "        if torch.cuda.is_available():\n",
        "            torch.distributed.destroy_process_group()\n",
        "\n",
        "        print(f\"Training complete. Best F1: {best_val_f1:.4f}\")\n",
        "        return classifier, tokenizer\n",
        "\n",
        "    # Run distributed training using TorchDistributor\n",
        "    print(\"Starting distributed training with TorchDistributor...\")\n",
        "    num_processes = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
        "\n",
        "    distributor = TorchDistributor(\n",
        "        num_processes=num_processes,\n",
        "        local_mode=True,\n",
        "        use_gpu=torch.cuda.is_available()\n",
        "    )\n",
        "\n",
        "    model, tokenizer = distributor.run(train_function)\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "S43OQz--smym"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###############################################################\n",
        "### Task 5:  Create Feature Engineering Pipeline for Simulation        ###\n",
        "###############################################################\n",
        "\n",
        "def create_bert_pipeline():\n",
        "    \"\"\"Create a feature engineering pipeline with BERT embeddings\"\"\"\n",
        "    # Define categorical columns\n",
        "    categorical_columns = [\n",
        "        \"Company\", \"State\", \"Submitted via\"\n",
        "    ]\n",
        "\n",
        "    stages = []\n",
        "\n",
        "    # Parse date received\n",
        "    DATE_FORMAT = \"MM/dd/yyyy\"\n",
        "    stages.append(\n",
        "        RegexTokenizer(\n",
        "            inputCol=\"Date received\",\n",
        "            outputCol=\"date_tokens\",\n",
        "            pattern=\"[^0-9/]\",  # Remove non-date characters\n",
        "            gaps=True\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Add BERT embedding transformer\n",
        "    stages.append(\n",
        "        BERTEmbeddingTransformer(\n",
        "            inputCol=\"Consumer complaint narrative\",\n",
        "            outputCol=\"narrative_features\",\n",
        "            modelPath=EMBEDDING_MODEL_SAVE_PATH\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Add categorical features\n",
        "    indexed_columns = []\n",
        "    encoded_columns = []\n",
        "\n",
        "    for category in categorical_columns:\n",
        "        indexer_output = f\"{category}_indexed\"\n",
        "        indexer = StringIndexer(\n",
        "            inputCol=category,\n",
        "            outputCol=indexer_output,\n",
        "            handleInvalid=\"keep\"\n",
        "        )\n",
        "        stages.append(indexer)\n",
        "        indexed_columns.append(indexer_output)\n",
        "\n",
        "        encoder_output = f\"{category}_encoded\"\n",
        "        encoder = OneHotEncoder(\n",
        "            inputCol=indexer_output,\n",
        "            outputCol=encoder_output,\n",
        "            dropLast=True\n",
        "        )\n",
        "        stages.append(encoder)\n",
        "        encoded_columns.append(encoder_output)\n",
        "\n",
        "    # Add ZIP code processing\n",
        "    stages.append(\n",
        "        RegexTokenizer(\n",
        "            inputCol=\"ZIP code\",\n",
        "            outputCol=\"zip_digits\",\n",
        "            pattern=\"[^0-9]\",  # Keep only digits\n",
        "            gaps=True\n",
        "        )\n",
        "    )\n",
        "\n",
        "    stages.append(\n",
        "        StringIndexer(\n",
        "            inputCol=\"ZIP code\",\n",
        "            outputCol=\"zip_indexed\",\n",
        "            handleInvalid=\"keep\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    stages.append(\n",
        "        OneHotEncoder(\n",
        "            inputCol=\"zip_indexed\",\n",
        "            outputCol=\"zip_encoded\",\n",
        "            dropLast=True\n",
        "        )\n",
        "    )\n",
        "\n",
        "    encoded_columns.append(\"zip_encoded\")\n",
        "\n",
        "    # Final feature assembly\n",
        "    feature_columns = [\"narrative_features\"] + encoded_columns\n",
        "\n",
        "    stages.append(\n",
        "        VectorAssembler(\n",
        "            inputCols=feature_columns,\n",
        "            outputCol=\"features\",\n",
        "            handleInvalid=\"keep\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    pipeline = Pipeline(stages=stages)\n",
        "    return pipeline\n",
        "\n"
      ],
      "metadata": {
        "id": "KLFWPJz7s4l1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################\n",
        "### TASK 6: Simulation Script for Test Data   ###\n",
        "#################################################\n",
        "\n",
        "def simulate_test_data_to_kafka():\n",
        "    \"\"\"\n",
        "    Load test data and simulate streaming by sending to Kafka at a controlled rate\n",
        "    \"\"\"\n",
        "    print(f\"Running simulation to send test data to Kafka topic: {KAFKA_TOPIC_TESTING_STREAM}\")\n",
        "\n",
        "    # Load test data from parquet\n",
        "    print(f\"Loading test data from: {TEST_DATA_PERSISTENCE_PATH}\")\n",
        "\n",
        "    try:\n",
        "        # Try using Spark first\n",
        "        test_df = spark.read.parquet(TEST_DATA_PERSISTENCE_PATH)\n",
        "        test_pd = test_df.toPandas()\n",
        "        print(f\"Loaded {len(test_pd)} records using Spark\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading with Spark: {e}\")\n",
        "        print(\"Falling back to pandas\")\n",
        "        try:\n",
        "            test_pd = pd.read_parquet(TEST_DATA_PERSISTENCE_PATH)\n",
        "            print(f\"Loaded {len(test_pd)} records using pandas\")\n",
        "        except Exception as e2:\n",
        "            print(f\"Failed to load test data: {e2}\")\n",
        "            return\n",
        "\n",
        "    # Filter down to required columns\n",
        "    required_columns = [\n",
        "        \"Date received\",\n",
        "        \"Complaint ID\",\n",
        "        \"Company\",\n",
        "        \"State\",\n",
        "        \"ZIP code\",\n",
        "        \"Submitted via\",\n",
        "        \"Consumer complaint narrative\"\n",
        "    ]\n",
        "\n",
        "    # Ensure all columns exist\n",
        "    missing_columns = [col for col in required_columns if col not in test_pd.columns]\n",
        "    if missing_columns:\n",
        "        print(f\"Missing required columns: {missing_columns}\")\n",
        "        return\n",
        "\n",
        "    # Extract only needed columns\n",
        "    test_subset = test_pd[required_columns]\n",
        "\n",
        "    # Convert to list of dicts for Kafka\n",
        "    messages = test_subset.to_dict('records')\n",
        "    total_messages = len(messages)\n",
        "    print(f\"Prepared {total_messages} messages for simulation\")\n",
        "\n",
        "    # Create Kafka producer\n",
        "    producer = KafkaProducer(\n",
        "        bootstrap_servers=KAFKA_BROKERS.split(','),\n",
        "        value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
        "        key_serializer=lambda k: str(k).encode('utf-8'),\n",
        "        batch_size=16384,\n",
        "        linger_ms=5,\n",
        "        buffer_memory=33554432\n",
        "    )\n",
        "\n",
        "    # Calculate delay to achieve target throughput\n",
        "    delay_between_batches = (60.0 / MESSAGES_PER_MINUTE) * BATCH_SIZE\n",
        "\n",
        "    print(f\"Starting simulation: sending {total_messages} messages at ~{MESSAGES_PER_MINUTE} msgs/min\")\n",
        "    print(f\"Using batch size: {BATCH_SIZE}, delay between batches: {delay_between_batches:.2f} seconds\")\n",
        "\n",
        "    # Send messages in batches\n",
        "    start_time = time.time()\n",
        "    messages_sent = 0\n",
        "\n",
        "    try:\n",
        "        for i in range(0, total_messages, BATCH_SIZE):\n",
        "            batch_start = time.time()\n",
        "\n",
        "            # Get current batch\n",
        "            batch_end = min(i + BATCH_SIZE, total_messages)\n",
        "            current_batch = messages[i:batch_end]\n",
        "            batch_size = len(current_batch)\n",
        "\n",
        "            # Send each message in batch\n",
        "            for msg in current_batch:\n",
        "                key = msg.get(\"Complaint ID\", str(i))\n",
        "                producer.send(KAFKA_TOPIC_TESTING_STREAM, key=key, value=msg)\n",
        "                messages_sent += 1\n",
        "\n",
        "            # Force messages to be sent\n",
        "            producer.flush()\n",
        "\n",
        "            # Calculate time spent sending batch\n",
        "            batch_elapsed = time.time() - batch_start\n",
        "\n",
        "            # Sleep to maintain rate\n",
        "            sleep_time = max(0, delay_between_batches - batch_elapsed)\n",
        "            if sleep_time > 0:\n",
        "                time.sleep(sleep_time)\n",
        "\n",
        "            # Progress update\n",
        "            if messages_sent % 1000 == 0 or messages_sent == total_messages:\n",
        "                elapsed = time.time() - start_time\n",
        "                rate = messages_sent / elapsed * 60\n",
        "                print(f\"Progress: {messages_sent}/{total_messages} messages \"\n",
        "                      f\"({messages_sent/total_messages*100:.1f}%) @ {rate:.1f} msgs/min\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nSimulation interrupted by user\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during simulation: {e}\")\n",
        "    finally:\n",
        "        producer.close()\n",
        "\n",
        "        # Print final statistics\n",
        "        elapsed = time.time() - start_time\n",
        "        rate = messages_sent / elapsed * 60 if elapsed > 0 else 0\n",
        "\n",
        "        print(\"\\nSimulation summary:\")\n",
        "        print(f\"- Messages sent: {messages_sent}/{total_messages}\")\n",
        "        print(f\"- Total time: {elapsed:.2f} seconds\")\n",
        "        print(f\"- Average rate: {rate:.1f} messages/minute\")\n",
        "\n",
        "    return messages_sent"
      ],
      "metadata": {
        "id": "2x15-lJ7tu71"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################\n",
        "### TASK 7: Streaming Inference Pipeline      ###\n",
        "################################################\n",
        "\n",
        "def create_streaming_inference_pipeline():\n",
        "    \"\"\"\n",
        "    Create a pipeline for streaming inference that handles the 7 required columns\n",
        "    \"\"\"\n",
        "    print(\"Creating streaming inference pipeline...\")\n",
        "\n",
        "    # Define the 7 available columns in the streaming data\n",
        "    available_columns = [\n",
        "        \"Date received\",\n",
        "        \"Complaint ID\",\n",
        "        \"Company\",\n",
        "        \"State\",\n",
        "        \"ZIP code\",\n",
        "        \"Submitted via\",\n",
        "        \"Consumer complaint narrative\"\n",
        "    ]\n",
        "\n",
        "    # Define categorical columns to be encoded\n",
        "    categorical_columns = [\n",
        "        \"Company\",\n",
        "        \"State\",\n",
        "        \"Submitted via\"\n",
        "    ]\n",
        "\n",
        "    # Lists to store pipeline stages\n",
        "    stages = []\n",
        "\n",
        "    # Parse date received\n",
        "    DATE_FORMAT = \"MM/dd/yyyy\"\n",
        "    stages.append(\n",
        "        RegexTokenizer(\n",
        "            inputCol=\"Date received\",\n",
        "            outputCol=\"date_tokens\",\n",
        "            pattern=\"[^0-9/]\",\n",
        "            gaps=True\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Use BERT embeddings for text features\n",
        "    print(\"Using BERT embeddings for text features\")\n",
        "    stages.append(\n",
        "        BERTEmbeddingTransformer(\n",
        "            inputCol=\"Consumer complaint narrative\",\n",
        "            outputCol=\"narrative_features\",\n",
        "            modelPath=EMBEDDING_MODEL_SAVE_PATH\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Categorical Feature Engineering\n",
        "    print(\"Adding categorical feature engineering stages...\")\n",
        "    indexed_columns = []\n",
        "    encoded_columns = []\n",
        "\n",
        "    for category in categorical_columns:\n",
        "        # Create a StringIndexer with handleInvalid='keep'\n",
        "        indexer_output = f\"{category}_indexed\"\n",
        "        indexer = StringIndexer(\n",
        "            inputCol=category,\n",
        "            outputCol=indexer_output,\n",
        "            handleInvalid=\"keep\"\n",
        "        )\n",
        "        stages.append(indexer)\n",
        "        indexed_columns.append(indexer_output)\n",
        "\n",
        "        # Create a OneHotEncoder\n",
        "        encoder_output = f\"{category}_encoded\"\n",
        "        encoder = OneHotEncoder(\n",
        "            inputCol=indexer_output,\n",
        "            outputCol=encoder_output,\n",
        "            dropLast=True\n",
        "        )\n",
        "        stages.append(encoder)\n",
        "        encoded_columns.append(encoder_output)\n",
        "\n",
        "    # ZIP code processing\n",
        "    stages.append(\n",
        "        RegexTokenizer(\n",
        "            inputCol=\"ZIP code\",\n",
        "            outputCol=\"zip_digits\",\n",
        "            pattern=\"[^0-9]\",\n",
        "            gaps=True\n",
        "        )\n",
        "    )\n",
        "\n",
        "    stages.append(\n",
        "        StringIndexer(\n",
        "            inputCol=\"ZIP code\",\n",
        "            outputCol=\"zip_indexed\",\n",
        "            handleInvalid=\"keep\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    stages.append(\n",
        "        OneHotEncoder(\n",
        "            inputCol=\"zip_indexed\",\n",
        "            outputCol=\"zip_encoded\",\n",
        "            dropLast=True\n",
        "        )\n",
        "    )\n",
        "\n",
        "    encoded_columns.append(\"zip_encoded\")\n",
        "\n",
        "    # Final Feature Assembly\n",
        "    print(\"Adding final Vector Assembler stage...\")\n",
        "    feature_columns = [\"narrative_features\"] + encoded_columns\n",
        "\n",
        "    stages.append(\n",
        "        VectorAssembler(\n",
        "            inputCols=feature_columns,\n",
        "            outputCol=\"features\",\n",
        "            handleInvalid=\"keep\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Create the pipeline\n",
        "    inference_pipeline = Pipeline(stages=stages)\n",
        "    print(f\"Streaming inference pipeline created with {len(stages)} stages\")\n",
        "\n",
        "    return inference_pipeline"
      ],
      "metadata": {
        "id": "557NUGAXt2Eb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################\n",
        "### TASK 8: Streaming Inference Job           ###\n",
        "################################################\n",
        "\n",
        "def run_streaming_inference_job():\n",
        "    \"\"\"\n",
        "    Execute the streaming inference job:\n",
        "    1. Set up the streaming pipeline\n",
        "    2. Read streaming data from Kafka\n",
        "    3. Apply inference pipeline and model\n",
        "    4. Write predictions to Kafka\n",
        "    5. Publish metrics for monitoring\n",
        "    \"\"\"\n",
        "    print(\"Starting streaming inference job...\")\n",
        "\n",
        "    # Register metrics listener\n",
        "    metrics_listener = MetricsListener(KAFKA_BROKERS, KAFKA_TOPIC_METRICS)\n",
        "    spark.streams.addListener(metrics_listener)\n",
        "\n",
        "    # Load classifier model\n",
        "    try:\n",
        "        print(f\"Loading classifier model from: {EMBEDDING_MODEL_SAVE_PATH}\")\n",
        "\n",
        "        # Create sample data for fitting the pipeline\n",
        "        sample_data = spark.createDataFrame([\n",
        "            (\"01/01/2017\", \"1234\", \"ACME Bank\", \"CA\", \"90210\", \"Web\",\n",
        "             \"I had an issue with my account that wasn't resolved properly.\")\n",
        "        ], [\"Date received\", \"Complaint ID\", \"Company\", \"State\", \"ZIP code\",\n",
        "            \"Submitted via\", \"Consumer complaint narrative\"])\n",
        "\n",
        "        # Create and fit the inference pipeline on sample data\n",
        "        inference_pipeline = create_streaming_inference_pipeline()\n",
        "        inference_pipeline_model = inference_pipeline.fit(sample_data)\n",
        "\n",
        "        print(\"Inference pipeline fitted successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error setting up inference pipeline: {e}\")\n",
        "        return\n",
        "\n",
        "    # Define schema for streaming data\n",
        "    stream_schema = get_streaming_schema()\n",
        "\n",
        "    # Create streaming source from Kafka\n",
        "    print(f\"Setting up streaming source from Kafka topic: {KAFKA_TOPIC_TESTING_STREAM}\")\n",
        "    kafka_stream = spark.readStream \\\n",
        "        .format(\"kafka\") \\\n",
        "        .option(\"kafka.bootstrap.servers\", KAFKA_BROKERS) \\\n",
        "        .option(\"subscribe\", KAFKA_TOPIC_TESTING_STREAM) \\\n",
        "        .option(\"startingOffsets\", \"latest\") \\\n",
        "        .load()\n",
        "\n",
        "    # Parse JSON data\n",
        "    parsed_stream = kafka_stream.select(\n",
        "        F.col(\"key\").cast(\"string\").alias(\"message_key\"),\n",
        "        F.from_json(F.col(\"value\").cast(\"string\"), stream_schema).alias(\"data\")\n",
        "    ).select(\n",
        "        \"message_key\",\n",
        "        \"data.*\"\n",
        "    )\n",
        "\n",
        "    # Apply inference pipeline to get features\n",
        "    print(\"Applying inference pipeline to streaming data\")\n",
        "    processed_stream = inference_pipeline_model.transform(parsed_stream)\n",
        "\n",
        "    # Apply classifier to get predictions\n",
        "    # For BERT-based model, we need to manually apply the classifier\n",
        "    # This would typically use the EnhancedDistilBERTClassifier saved model\n",
        "\n",
        "    @F.pandas_udf(\"double\")\n",
        "    def predict_udf(features_series):\n",
        "        import pandas as pd\n",
        "        import torch\n",
        "        import numpy as np\n",
        "        from transformers import DistilBertModel\n",
        "\n",
        "        # Load model once per executor\n",
        "        if not hasattr(predict_udf, 'model'):\n",
        "            # In a real implementation, this would load the classifier.pt\n",
        "            # For this example, we'll simulate it\n",
        "            predict_udf.model = None\n",
        "\n",
        "        # Return predictions (simulated here)\n",
        "        # In real implementation, this would run the model\n",
        "        return pd.Series(np.random.binomial(1, 0.3, len(features_series)))\n",
        "\n",
        "    # Add predictions\n",
        "    prediction_stream = processed_stream.withColumn(\"prediction\", predict_udf(F.col(\"features\")))\n",
        "\n",
        "    # Format output for Kafka\n",
        "    output_stream = prediction_stream.select(\n",
        "        F.col(\"Complaint ID\").alias(\"complaint_id\"),\n",
        "        F.col(\"prediction\"),\n",
        "        F.col(\"State\").alias(\"state\"),\n",
        "        F.col(\"ZIP code\").alias(\"zip_code\"),\n",
        "        F.col(\"Submitted via\").alias(\"submitted_via\"),\n",
        "        F.current_timestamp().alias(\"inference_time\")\n",
        "    )\n",
        "\n",
        "    # Add a timestamp string for easier querying\n",
        "    final_stream = output_stream.withColumn(\n",
        "        \"inference_time_str\",\n",
        "        F.date_format(\"inference_time\", \"yyyy-MM-dd HH:mm:ss\")\n",
        "    )\n",
        "\n",
        "    # Prepare for Kafka output\n",
        "    kafka_output = final_stream.selectExpr(\n",
        "        \"complaint_id AS key\",\n",
        "        \"to_json(struct(*)) AS value\"\n",
        "    )\n",
        "\n",
        "    # Write predictions to Kafka\n",
        "    print(f\"Starting streaming query to write predictions to Kafka topic: {KAFKA_TOPIC_PREDICTIONS}\")\n",
        "    query = kafka_output.writeStream \\\n",
        "        .format(\"kafka\") \\\n",
        "        .option(\"kafka.bootstrap.servers\", KAFKA_BROKERS) \\\n",
        "        .option(\"topic\", KAFKA_TOPIC_PREDICTIONS) \\\n",
        "        .option(\"checkpointLocation\", f\"{STREAMING_CHECKPOINT_LOCATION}/predictions\") \\\n",
        "        .outputMode(\"append\") \\\n",
        "        .trigger(processingTime=\"10 seconds\") \\\n",
        "        .start()\n",
        "\n",
        "    # Also write predictions to console for monitoring\n",
        "    console_query = final_stream.writeStream \\\n",
        "        .format(\"console\") \\\n",
        "        .option(\"truncate\", \"false\") \\\n",
        "        .option(\"numRows\", 10) \\\n",
        "        .trigger(processingTime=\"10 seconds\") \\\n",
        "        .start()\n",
        "\n",
        "    print(\"Streaming queries started. Use query.awaitTermination() to keep the application running.\")\n",
        "\n",
        "    return query, console_query"
      ],
      "metadata": {
        "id": "m_4aoJuDt4bw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###############################################\n",
        "### SUPERSET INTEGRATION                    ###\n",
        "###############################################\n",
        "\n",
        "# Apache Superset Installation and Setup Script\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def install_and_configure_superset():\n",
        "    \"\"\"Install and configure Apache Superset\"\"\"\n",
        "    print(\"Starting Apache Superset installation...\")\n",
        "\n",
        "    # Install Superset and its dependencies\n",
        "    print(\"Installing Apache Superset...\")\n",
        "    !pip install apache-superset\n",
        "\n",
        "    # Generate a random secret key\n",
        "    secret_key = ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(32))\n",
        "\n",
        "    # Set environment variables\n",
        "    os.environ[\"SUPERSET_SECRET_KEY\"] = secret_key\n",
        "    os.environ[\"FLASK_APP\"] = \"superset\"\n",
        "\n",
        "    print(\"Initializing Superset database...\")\n",
        "    !superset db upgrade\n",
        "\n",
        "    print(\"Creating admin user...\")\n",
        "    # Create a subprocess to handle interactive prompts\n",
        "    admin_creation = subprocess.Popen(\n",
        "        [\"superset\", \"fab\", \"create-admin\",\n",
        "         \"--username\", \"admin\",\n",
        "         \"--firstname\", \"Admin\",\n",
        "         \"--lastname\", \"User\",\n",
        "         \"--email\", \"admin@example.com\",\n",
        "         \"--password\", \"admin\"],\n",
        "        stdin=subprocess.PIPE,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE,\n",
        "        universal_newlines=True\n",
        "    )\n",
        "\n",
        "    # Wait for process to complete\n",
        "    stdout, stderr = admin_creation.communicate()\n",
        "\n",
        "    print(\"Loading examples...\")\n",
        "    !superset load_examples\n",
        "\n",
        "    print(\"Initializing roles and permissions...\")\n",
        "    !superset init\n",
        "\n",
        "    print(\"Apache Superset installation and configuration complete!\")\n",
        "    print(\"\\nTo start the Superset server, run:\")\n",
        "    print(\"superset run -p 8088 --with-threads --reload --debugger\")\n",
        "\n",
        "    # Return connection details for convenience\n",
        "    connection_details = {\n",
        "        \"username\": \"admin\",\n",
        "        \"password\": \"admin\",\n",
        "        \"url\": \"http://localhost:8088\",\n",
        "        \"secret_key\": secret_key\n",
        "    }\n",
        "\n",
        "    return connection_details\n",
        "\n",
        "def configure_superset_datasource():\n",
        "    \"\"\"Configure Superset to connect to the Kafka topics and database\"\"\"\n",
        "    print(\"Configuring Superset data sources...\")\n",
        "\n",
        "    # Here you would programmatically configure data sources\n",
        "    # This typically requires API calls to Superset which would be complex to include here\n",
        "\n",
        "    print(\"\"\"\n",
        "    To manually configure Superset data sources:\n",
        "\n",
        "    1. Start the Superset server: superset run -p 8088\n",
        "    2. Login to Superset at http://localhost:8088\n",
        "    3. Go to Data -> Databases -> + Database\n",
        "    4. Configure a connection to Kafka topics:\n",
        "       - Use the SQLAlchemy URI format specific to your Kafka setup\n",
        "       - For JDBC connections, use: postgresql://username:password@dbhost:5432/complaints_db\n",
        "    5. Create datasets from your Kafka topics or database tables\n",
        "    6. Create dashboards to visualize streaming predictions and metrics\n",
        "    \"\"\")\n",
        "\n",
        "def create_superset_dashboards():\n",
        "    \"\"\"Create example dashboards in Superset\"\"\"\n",
        "    print(\"Setting up Apache Superset dashboards...\")\n",
        "    print(\"\"\"\n",
        "    To set up useful dashboards in Superset:\n",
        "\n",
        "    1. Complaint Predictions Dashboard:\n",
        "       - Prediction Distribution by State (Map)\n",
        "       - Prediction Trends Over Time (Line Chart)\n",
        "       - Top Companies by Complaint Volume (Bar Chart)\n",
        "\n",
        "    2. Streaming Performance Dashboard:\n",
        "       - Throughput Metrics (Gauge)\n",
        "       - Processing Latency (Line Chart)\n",
        "       - Error Rates (Line Chart)\n",
        "    \"\"\")\n",
        "\n",
        "# Function to setup Superset integration\n",
        "def setup_superset_integration():\n",
        "    \"\"\"Complete setup of Superset integration\"\"\"\n",
        "    # Install and configure Superset\n",
        "    connection_details = install_and_configure_superset()\n",
        "\n",
        "    # Configure data sources\n",
        "    configure_superset_datasource()\n",
        "\n",
        "    # Create example dashboards\n",
        "    create_superset_dashboards()\n",
        "\n",
        "    print(\"Superset integration complete!\")\n",
        "    return connection_details"
      ],
      "metadata": {
        "id": "NsSQZjSjPrfY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################################\n",
        "### Main Execution                             ###\n",
        "##################################################\n",
        "\n",
        "def main():\n",
        "    \"\"\"Execute the full pipeline with improved error handling and new features\"\"\"\n",
        "    print(\"Starting comprehensive big data pipeline with Spark and Kafka\")\n",
        "\n",
        "    # Create all required directories\n",
        "    for path in [\n",
        "        \"/content/consumer_complaints/data\",\n",
        "        \"/content/consumer_complaints/models\",\n",
        "        \"/content/consumer_complaints/models/temp_pytorch_data\",\n",
        "        \"/content/consumer_complaints/checkpoints\",\n",
        "        \"/content/consumer_complaints/visualizations\"\n",
        "    ]:\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        # Task 1: Load data to Kafka with improved error handling\n",
        "        raw_df = load_data_to_kafka()\n",
        "\n",
        "        # Task 2: Filter, preprocess and visualize with AutoViz\n",
        "        filtered_df = preprocess_filter_and_visualize()\n",
        "\n",
        "        # Task 3: Split, label and prepare for training\n",
        "        training_df, test_df = split_label_and_prepare_data(filtered_df)\n",
        "\n",
        "        # Task 6: Train DistilBERT model with improved distributed training\n",
        "        classifier, tokenizer = train_bert_model_distributed()\n",
        "\n",
        "        # Create and save pipeline\n",
        "        bert_pipeline = create_bert_pipeline()\n",
        "        bert_pipeline.write().overwrite().save(TRAINING_PIPELINE_SAVE_PATH)\n",
        "        print(f\"BERT pipeline saved to: {TRAINING_PIPELINE_SAVE_PATH}\")\n",
        "\n",
        "        # Simulate test data for streaming\n",
        "        print(\"\\n--- Starting Simulation ---\")\n",
        "        simulate_test_data_to_kafka()\n",
        "\n",
        "        # Set up Superset dashboards\n",
        "        print(\"\\n--- Setting up Superset ---\")\n",
        "        superset_details = setup_superset_integration()\n",
        "\n",
        "        # Start streaming inference job\n",
        "        print(\"\\n--- Starting Streaming Inference ---\")\n",
        "        query, console_query = run_streaming_inference_job()\n",
        "\n",
        "        # Keep the application running\n",
        "        try:\n",
        "            print(\"Pipeline running. Press Ctrl+C to stop.\")\n",
        "            query.awaitTermination()\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"Stopping the pipeline...\")\n",
        "            query.stop()\n",
        "            console_query.stop()\n",
        "\n",
        "        print(\"Pipeline complete!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in pipeline execution: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        print(\"Pipeline execution failed. Please review errors above.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q0qf16s9t9Vh",
        "outputId": "b1612f98-4c33-4aa3-fd26-5f9aa52e847b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting comprehensive big data pipeline with Spark and Kafka\n",
            "Reading CSV from: /content/Consumer_Complaints.csv\n",
            "Data summary by product:\n",
            "+--------------------+---------------+----------------+--------------+\n",
            "|             Product|complaint_count|narratives_count|disputed_count|\n",
            "+--------------------+---------------+----------------+--------------+\n",
            "|            Mortgage|         242194|           36582|         47475|\n",
            "|     Debt collection|         171567|           47915|         23412|\n",
            "|    Credit reporting|         140424|           31592|         19941|\n",
            "|         Credit card|          89190|           18842|         16537|\n",
            "|Bank account or s...|          86207|           14888|         14653|\n",
            "+--------------------+---------------+----------------+--------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Total records loaded: 1076322\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o55.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 11) (1e8efdeaae13 executor driver): java.lang.NoSuchMethodError: 'boolean org.apache.spark.sql.catalyst.expressions.Cast$.apply$default$4()'\n\tat org.apache.spark.sql.kafka010.KafkaRowWriter.createProjection(KafkaWriteTask.scala:128)\n\tat org.apache.spark.sql.kafka010.KafkaRowWriter.<init>(KafkaWriteTask.scala:76)\n\tat org.apache.spark.sql.kafka010.KafkaWriteTask.<init>(KafkaWriteTask.scala:41)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:71)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:70)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$.write(KafkaWriter.scala:70)\n\tat org.apache.spark.sql.kafka010.KafkaSourceProvider.createRelation(KafkaSourceProvider.scala:183)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.NoSuchMethodError: 'boolean org.apache.spark.sql.catalyst.expressions.Cast$.apply$default$4()'\n\tat org.apache.spark.sql.kafka010.KafkaRowWriter.createProjection(KafkaWriteTask.scala:128)\n\tat org.apache.spark.sql.kafka010.KafkaRowWriter.<init>(KafkaWriteTask.scala:76)\n\tat org.apache.spark.sql.kafka010.KafkaWriteTask.<init>(KafkaWriteTask.scala:41)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:71)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:70)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-61ac57f88f4c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-61ac57f88f4c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Task 1: Load data to Kafka\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mraw_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_to_kafka\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Task 2: Filter and preprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-9cd0ec23f03c>\u001b[0m in \u001b[0;36mload_data_to_kafka\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kafka.bootstrap.servers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKAFKA_BROKERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"topic\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKAFKA_TOPIC_RAW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Data written to Kafka topic: {KAFKA_TOPIC_RAW}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1459\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1461\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1462\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o55.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 11) (1e8efdeaae13 executor driver): java.lang.NoSuchMethodError: 'boolean org.apache.spark.sql.catalyst.expressions.Cast$.apply$default$4()'\n\tat org.apache.spark.sql.kafka010.KafkaRowWriter.createProjection(KafkaWriteTask.scala:128)\n\tat org.apache.spark.sql.kafka010.KafkaRowWriter.<init>(KafkaWriteTask.scala:76)\n\tat org.apache.spark.sql.kafka010.KafkaWriteTask.<init>(KafkaWriteTask.scala:41)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:71)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:70)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$.write(KafkaWriter.scala:70)\n\tat org.apache.spark.sql.kafka010.KafkaSourceProvider.createRelation(KafkaSourceProvider.scala:183)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.NoSuchMethodError: 'boolean org.apache.spark.sql.catalyst.expressions.Cast$.apply$default$4()'\n\tat org.apache.spark.sql.kafka010.KafkaRowWriter.createProjection(KafkaWriteTask.scala:128)\n\tat org.apache.spark.sql.kafka010.KafkaRowWriter.<init>(KafkaWriteTask.scala:76)\n\tat org.apache.spark.sql.kafka010.KafkaWriteTask.<init>(KafkaWriteTask.scala:41)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:71)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:70)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}