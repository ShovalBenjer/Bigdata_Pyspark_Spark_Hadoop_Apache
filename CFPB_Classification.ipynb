{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShovalBenjer/Bigdata_Pyspark_Spark_Hadoop_Apache/blob/Final_Project/CFPB_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " The setup includes:\n",
        "\n",
        "All necessary imports for PySpark SQL, ML features, classification models, pipelines, evaluation metrics, and tuning components.\n",
        "Configuration variables with the requested naming conventions:\n",
        "\n",
        "File paths for CSV input and all model/pipeline storage locations\n",
        "Kafka broker and topic configurations\n",
        "ML pipeline settings\n",
        "Database connection parameters\n",
        "\n",
        "\n",
        "SparkSession initialization with performance optimizations:\n",
        "\n",
        "Arrow enabled for improved Python-JVM data transfer\n",
        "Memory configurations for driver and executors\n",
        "Parallelism settings\n",
        "Kafka and JDBC connector configurations\n",
        "Checkpointing for streaming applications\n",
        "\n",
        "\n",
        "Optional imports for deep learning models (commented out but available if needed for Task 5)"
      ],
      "metadata": {
        "id": "XJsutje-eYKI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIYdn1woOS1n"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# Consumer Complaints PySpark ML Pipeline\n",
        "\n",
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover, StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.functions import vector_to_array\n",
        "\n",
        "# Advanced ML imports (if using deep learning models)\n",
        "# from pyspark.ml.deepspeed import DeepspeedBertClassifier, DeepspeedBertEmbeddings\n",
        "# import torch\n",
        "# from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Configuration variables\n",
        "# File and storage paths\n",
        "CSV_FILE_PATH = \"/path/to/Consumer_Complaints.csv\"\n",
        "TEST_DATA_PERSISTENCE_PATH = \"/path/to/test_data_source.parquet\"\n",
        "TRAINING_PIPELINE_SAVE_PATH = \"/path/to/training_pipeline\"\n",
        "BEST_MODEL_SAVE_PATH = \"/path/to/best_model\"\n",
        "EMBEDDING_MODEL_SAVE_PATH = \"/path/to/embedding_model\"  # If using Task 5\n",
        "STREAMING_CHECKPOINT_LOCATION = \"/path/to/streaming_checkpoints\"\n",
        "\n",
        "# Kafka configuration\n",
        "KAFKA_BROKERS = \"kafka1:9092,kafka2:9092\"\n",
        "KAFKA_TOPIC_RAW = \"complaints-raw\"\n",
        "KAFKA_TOPIC_TRAINING = \"complaints-training-data\"\n",
        "KAFKA_TOPIC_TESTING_STREAM = \"complaints-testing-stream\"\n",
        "KAFKA_TOPIC_PREDICTIONS = \"complaint-predictions\"\n",
        "\n",
        "# ML pipeline configuration\n",
        "NUM_FOLDS = 5\n",
        "\n",
        "# Database sink configuration\n",
        "DATABASE_SINK_FORMAT = \"jdbc\"\n",
        "DATABASE_CONNECTION_OPTIONS = {\n",
        "    \"url\": \"jdbc:postgresql://dbhost:5432/complaints_db\",\n",
        "    \"dbtable\": \"complaint_predictions\",\n",
        "    \"user\": \"username\",\n",
        "    \"password\": \"password\",\n",
        "    \"driver\": \"org.postgresql.Driver\"\n",
        "}\n",
        "\n",
        "# Initialize Spark Session with appropriate configurations\n",
        "def get_spark_session():\n",
        "    \"\"\"\n",
        "    Initialize and return a SparkSession with appropriate configurations\n",
        "    \"\"\"\n",
        "    return (SparkSession.builder\n",
        "            .appName(\"Consumer Complaints ML Pipeline\")\n",
        "            .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
        "            .config(\"spark.executor.memory\", \"8g\")\n",
        "            .config(\"spark.driver.memory\", \"4g\")\n",
        "            .config(\"spark.executor.cores\", \"4\")\n",
        "            .config(\"spark.default.parallelism\", \"100\")\n",
        "            .config(\"spark.sql.shuffle.partitions\", \"100\")\n",
        "            .config(\"spark.streaming.kafka.maxRatePerPartition\", \"10000\")\n",
        "            # Checkpointing for streaming\n",
        "            .config(\"spark.sql.streaming.checkpointLocation\", STREAMING_CHECKPOINT_LOCATION)\n",
        "            # For Delta Lake if using\n",
        "            # .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.2.1\")\n",
        "            # For Kafka streaming if using\n",
        "            .config(\"spark.jars.packages\",\n",
        "                   \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,org.postgresql:postgresql:42.5.1\")\n",
        "            .getOrCreate())\n",
        "\n",
        "# Create Spark session\n",
        "spark = get_spark_session()\n",
        "\n",
        "# Enable dynamic allocation if available\n",
        "spark.conf.set(\"spark.dynamicAllocation.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.shuffle.service.enabled\", \"true\")\n",
        "\n",
        "# Log the session configuration\n",
        "print(\"Spark Configuration:\")\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"Application ID: {spark.sparkContext.applicationId}\")\n",
        "print(f\"Number of Executors: {spark.sparkContext.getConf().get('spark.executor.instances', 'Not set')}\")\n",
        "\n",
        "print(\"Setup complete. Ready to process consumer complaints data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1: Schema Definition & Initial Kafka Load\n",
        "\n",
        "Schema Definition:\n",
        "\n",
        "Created a complete 18-column StructType schema for the Consumer Complaints dataset\n",
        "All columns are defined as StringType() as requested, including the important \"Consumer complaint narrative\" and \"Consumer consent provided?\" fields\n",
        "The schema properly handles dates as strings per the requirements\n",
        "\n",
        "\n",
        "CSV Loading:\n",
        "\n",
        "Read the CSV file using the defined schema with header=True\n",
        "Implemented basic data inspection with sample display and record count\n",
        "\n",
        "\n",
        "Kafka Writing:\n",
        "\n",
        "Converted the DataFrame to Kafka-compatible format with \"Complaint ID\" as the key\n",
        "Used to_json(struct(*)) to convert all columns to a JSON string as the value\n",
        "Used the configured Kafka brokers and topic name\n",
        "\n",
        "\n",
        "\n",
        "Task 2: Preprocessing & Filtering\n",
        "\n",
        "Reading from Kafka:\n",
        "\n",
        "Read data back from the Kafka raw topic using the earliest offset\n",
        "Parsed the JSON value column using the full schema\n",
        "\n",
        "\n",
        "Filtering Pipeline:\n",
        "\n",
        "Date Parsing: Applied date parsing to the \"Date received\" column using the MM/dd/yyyy format, handling potential errors by allowing nulls\n",
        "Narrative Filter: Removed records with null or empty complaint narratives\n",
        "Date Filter: Kept only records with valid dates on or before 2017-03-31\n",
        "Response Filter: Excluded records where \"Company response to consumer\" is \"In progress\"\n",
        "\n",
        "\n",
        "Monitoring and Reporting:\n",
        "\n",
        "Added record counts at each filtering stage for monitoring purposes\n",
        "Displayed sample data after the complete filtering process"
      ],
      "metadata": {
        "id": "DPQzrw0begvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# Consumer Complaints Schema Definition, Kafka Load & Preprocessing\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Import configuration from the setup file\n",
        "# In practice, you might want to import these from a common module\n",
        "CSV_FILE_PATH = \"/path/to/Consumer_Complaints.csv\"\n",
        "KAFKA_BROKERS = \"kafka1:9092,kafka2:9092\"\n",
        "KAFKA_TOPIC_RAW = \"complaints-raw\"\n",
        "KAFKA_TOPIC_TRAINING = \"complaints-training-data\"\n",
        "\n",
        "# Initialize Spark Session (in case this is run as a standalone script)\n",
        "spark = SparkSession.builder.appName(\"Consumer Complaints Schema & Kafka Load\").getOrCreate()\n",
        "\n",
        "# Task 1: Define Schema & Initial Load to Kafka\n",
        "def define_schema_and_load_to_kafka():\n",
        "    \"\"\"\n",
        "    Define the full 18-column schema for consumer complaints data,\n",
        "    load from CSV, and write to Kafka raw topic\n",
        "    \"\"\"\n",
        "    # Define the full 18-column schema\n",
        "    full_schema = StructType([\n",
        "        StructField(\"Date received\", StringType(), True),\n",
        "        StructField(\"Product\", StringType(), True),\n",
        "        StructField(\"Sub-product\", StringType(), True),\n",
        "        StructField(\"Issue\", StringType(), True),\n",
        "        StructField(\"Sub-issue\", StringType(), True),\n",
        "        StructField(\"Consumer complaint narrative\", StringType(), True),\n",
        "        StructField(\"Company public response\", StringType(), True),\n",
        "        StructField(\"Company\", StringType(), True),\n",
        "        StructField(\"State\", StringType(), True),\n",
        "        StructField(\"ZIP code\", StringType(), True),\n",
        "        StructField(\"Tags\", StringType(), True),\n",
        "        StructField(\"Consumer consent provided?\", StringType(), True),\n",
        "        StructField(\"Submitted via\", StringType(), True),\n",
        "        StructField(\"Date sent to company\", StringType(), True),\n",
        "        StructField(\"Company response to consumer\", StringType(), True),\n",
        "        StructField(\"Timely response?\", StringType(), True),\n",
        "        StructField(\"Consumer disputed?\", StringType(), True),\n",
        "        StructField(\"Complaint ID\", StringType(), True)\n",
        "    ])\n",
        "\n",
        "    # Read the CSV file using the defined schema\n",
        "    print(f\"Reading CSV from: {CSV_FILE_PATH}\")\n",
        "    raw_df = spark.read.format(\"csv\") \\\n",
        "                       .option(\"header\", \"true\") \\\n",
        "                       .schema(full_schema) \\\n",
        "                       .load(CSV_FILE_PATH)\n",
        "\n",
        "    # Show sample data and schema\n",
        "    print(\"Sample data from CSV:\")\n",
        "    raw_df.select(\"Complaint ID\", \"Date received\", \"Product\", \"Consumer complaint narrative\").show(5, truncate=True)\n",
        "    print(\"DataFrame Schema:\")\n",
        "    raw_df.printSchema()\n",
        "\n",
        "    # Count records\n",
        "    count = raw_df.count()\n",
        "    print(f\"Total records loaded: {count}\")\n",
        "\n",
        "    # Write to Kafka topic\n",
        "    print(f\"Writing data to Kafka topic: {KAFKA_TOPIC_RAW}\")\n",
        "    kafka_df = raw_df.selectExpr(\n",
        "        \"Complaint ID AS key\",\n",
        "        \"to_json(struct(*)) AS value\"\n",
        "    )\n",
        "\n",
        "    # Write to Kafka\n",
        "    kafka_df.write \\\n",
        "        .format(\"kafka\") \\\n",
        "        .option(\"kafka.bootstrap.servers\", KAFKA_BROKERS) \\\n",
        "        .option(\"topic\", KAFKA_TOPIC_RAW) \\\n",
        "        .save()\n",
        "\n",
        "    print(f\"Successfully wrote {count} records to Kafka topic: {KAFKA_TOPIC_RAW}\")\n",
        "    return raw_df\n",
        "\n",
        "\n",
        "# Task 2: Preprocessing & Filtering\n",
        "def preprocess_and_filter_from_kafka():\n",
        "    \"\"\"\n",
        "    Read data from Kafka raw topic and apply preprocessing and filtering\n",
        "    \"\"\"\n",
        "    # Read from Kafka topic\n",
        "    print(f\"Reading data from Kafka topic: {KAFKA_TOPIC_RAW}\")\n",
        "    kafka_raw_df = spark.read \\\n",
        "        .format(\"kafka\") \\\n",
        "        .option(\"kafka.bootstrap.servers\", KAFKA_BROKERS) \\\n",
        "        .option(\"subscribe\", KAFKA_TOPIC_RAW) \\\n",
        "        .option(\"startingOffsets\", \"earliest\") \\\n",
        "        .load()\n",
        "\n",
        "    # Parse JSON value\n",
        "    parsed_df = kafka_raw_df.select(\n",
        "        F.col(\"key\").cast(\"string\").alias(\"key\"),\n",
        "        F.from_json(F.col(\"value\").cast(\"string\"), full_schema).alias(\"data\")\n",
        "    ).select(\"data.*\")\n",
        "\n",
        "    # Apply filters sequentially:\n",
        "\n",
        "    # 1. Parse date received - handle potential errors gracefully\n",
        "    DATE_FORMAT = \"MM/dd/yyyy\"\n",
        "    with_parsed_date_df = parsed_df.withColumn(\n",
        "        \"parsed_date_received\",\n",
        "        F.to_date(F.col(\"Date received\"), DATE_FORMAT)\n",
        "    )\n",
        "\n",
        "    # 2. Filter for non-null and non-empty narratives\n",
        "    narrative_filtered_df = with_parsed_date_df.filter(\n",
        "        (F.col(\"Consumer complaint narrative\").isNotNull()) &\n",
        "        (F.length(F.trim(F.col(\"Consumer complaint narrative\"))) > 0)\n",
        "    )\n",
        "\n",
        "    # 3. Filter for valid dates before or on 2017-03-31\n",
        "    date_filtered_df = narrative_filtered_df.filter(\n",
        "        (F.col(\"parsed_date_received\").isNotNull()) &\n",
        "        (F.col(\"parsed_date_received\") <= F.lit(\"2017-03-31\"))\n",
        "    )\n",
        "\n",
        "    # 4. Filter out \"In progress\" company responses\n",
        "    filtered_df = date_filtered_df.filter(\n",
        "        F.col(\"Company response to consumer\") != \"In progress\"\n",
        "    )\n",
        "\n",
        "    # Show sample data after filtering\n",
        "    print(\"Sample data after filtering:\")\n",
        "    filtered_df.select(\"Complaint ID\", \"parsed_date_received\", \"Product\", \"Consumer complaint narrative\").show(5, truncate=True)\n",
        "\n",
        "    # Count records at each filtering stage for monitoring\n",
        "    initial_count = parsed_df.count()\n",
        "    narrative_count = narrative_filtered_df.count()\n",
        "    date_count = date_filtered_df.count()\n",
        "    final_count = filtered_df.count()\n",
        "\n",
        "    print(f\"Initial record count: {initial_count}\")\n",
        "    print(f\"After narrative filter: {narrative_count}\")\n",
        "    print(f\"After date filter: {date_count}\")\n",
        "    print(f\"Final record count: {final_count}\")\n",
        "\n",
        "    return filtered_df\n",
        "\n",
        "\n",
        "# Execute the tasks\n",
        "if __name__ == \"__main__\":\n",
        "    # Define schema and load data to Kafka\n",
        "    raw_df = define_schema_and_load_to_kafka()\n",
        "\n",
        "    # Define the full schema again for the processing function\n",
        "    # (In a real implementation, this would be imported from a common module)\n",
        "    full_schema = StructType([\n",
        "        StructField(\"Date received\", StringType(), True),\n",
        "        StructField(\"Product\", StringType(), True),\n",
        "        StructField(\"Sub-product\", StringType(), True),\n",
        "        StructField(\"Issue\", StringType(), True),\n",
        "        StructField(\"Sub-issue\", StringType(), True),\n",
        "        StructField(\"Consumer complaint narrative\", StringType(), True),\n",
        "        StructField(\"Company public response\", StringType(), True),\n",
        "        StructField(\"Company\", StringType(), True),\n",
        "        StructField(\"State\", StringType(), True),\n",
        "        StructField(\"ZIP code\", StringType(), True),\n",
        "        StructField(\"Tags\", StringType(), True),\n",
        "        StructField(\"Consumer consent provided?\", StringType(), True),\n",
        "        StructField(\"Submitted via\", StringType(), True),\n",
        "        StructField(\"Date sent to company\", StringType(), True),\n",
        "        StructField(\"Company response to consumer\", StringType(), True),\n",
        "        StructField(\"Timely response?\", StringType(), True),\n",
        "        StructField(\"Consumer disputed?\", StringType(), True),\n",
        "        StructField(\"Complaint ID\", StringType(), True)\n",
        "    ])\n",
        "\n",
        "    # Read from Kafka and apply preprocessing\n",
        "    filtered_df = preprocess_and_filter_from_kafka()\n",
        "\n",
        "    print(\"Tasks 1-2 completed successfully\")"
      ],
      "metadata": {
        "id": "Q53jQ5EAeki4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Data Splitting (80/20)\n",
        "\n",
        "Performed a random split on the filtered DataFrame using a fixed seed value (42) for reproducibility\n",
        "Created training_base_df (80%) and test_base_df (20%)\n",
        "Added logging to verify the split proportions\n",
        "\n",
        "2. Training Data Preparation\n",
        "\n",
        "Created the binary target column is_target_complaint using these rules:\n",
        "\n",
        "A complaint is labeled as a target (1) if ALL of these conditions are met:\n",
        "\n",
        "\"Consumer disputed?\" is \"Yes\"\n",
        "\"Timely response?\" is \"No\"\n",
        "\"Company response to consumer\" is one of: \"Closed with explanation\", \"Closed\", \"Closed with monetary relief\", or \"Closed with non-monetary relief\"\n",
        "\n",
        "\n",
        "Otherwise, the complaint is labeled as not a target (0)\n",
        "\n",
        "\n",
        "Added analytics on the target distribution to help with potential class imbalance:\n",
        "\n",
        "Calculated class distribution counts\n",
        "Computed class weights that could be used for model training\n",
        "\n",
        "\n",
        "Selected all 18 original columns plus the new is_target_complaint column\n",
        "Wrote the training data to the Kafka topic KAFKA_TOPIC_TRAINING:\n",
        "\n",
        "Used \"Complaint ID\" as the key\n",
        "Converted all columns to JSON for the value\n",
        "\n",
        "\n",
        "\n",
        "3. Test Data Preparation\n",
        "\n",
        "Selected all 18 original columns from test_base_df\n",
        "Wrote this data to TEST_DATA_PERSISTENCE_PATH using Parquet format with overwrite mode\n",
        "This stored test data will be used by the external simulation script"
      ],
      "metadata": {
        "id": "diC2Fnife8ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# Data Split, Target Labeling & Kafka Preparation\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Import configuration from the setup file\n",
        "# In practice, you would import these from a common module\n",
        "KAFKA_BROKERS = \"kafka1:9092,kafka2:9092\"\n",
        "KAFKA_TOPIC_TRAINING = \"complaints-training-data\"\n",
        "TEST_DATA_PERSISTENCE_PATH = \"/path/to/test_data_source.parquet\"\n",
        "\n",
        "# Set a seed value for reproducibility\n",
        "SEED_VALUE = 42\n",
        "\n",
        "# Initialize Spark Session (in case this is run as a standalone script)\n",
        "spark = SparkSession.builder.appName(\"Consumer Complaints Data Split & Labeling\").getOrCreate()\n",
        "\n",
        "def split_label_and_prepare_data(filtered_df):\n",
        "    \"\"\"\n",
        "    Split the filtered data into training and test sets, label the target,\n",
        "    and prepare data for Kafka and persistence\n",
        "    \"\"\"\n",
        "    print(\"Starting data split, target labeling, and Kafka preparation...\")\n",
        "\n",
        "    # Task 3.1: Perform 80/20 random split\n",
        "    print(f\"Performing 80/20 random split with seed {SEED_VALUE}...\")\n",
        "    training_base_df, test_base_df = filtered_df.randomSplit([0.8, 0.2], seed=SEED_VALUE)\n",
        "\n",
        "    # Log the split results\n",
        "    training_count = training_base_df.count()\n",
        "    test_count = test_base_df.count()\n",
        "    print(f\"Training set size: {training_count} records ({training_count / (training_count + test_count):.2%})\")\n",
        "    print(f\"Test set size: {test_count} records ({test_count / (training_count + test_count):.2%})\")\n",
        "\n",
        "    # Task 3.2: Prepare Training Data with target labeling\n",
        "    print(\"Labeling target complaints in training data...\")\n",
        "\n",
        "    # Create binary target column based on the target group definition\n",
        "    training_labeled_df = training_base_df.withColumn(\n",
        "        \"is_target_complaint\",\n",
        "        F.when(\n",
        "            # A complaint is a target if it meets ALL of these conditions:\n",
        "            (F.col(\"Consumer disputed?\") == \"Yes\") &\n",
        "            (F.col(\"Timely response?\") == \"No\") &\n",
        "            (\n",
        "                # AND it meets ANY of these response conditions:\n",
        "                (F.col(\"Company response to consumer\") == \"Closed with explanation\") |\n",
        "                (F.col(\"Company response to consumer\") == \"Closed\") |\n",
        "                (F.col(\"Company response to consumer\") == \"Closed with monetary relief\") |\n",
        "                (F.col(\"Company response to consumer\") == \"Closed with non-monetary relief\")\n",
        "            ),\n",
        "            1  # True case: it's a target complaint\n",
        "        ).otherwise(0)  # False case: not a target complaint\n",
        "    )\n",
        "\n",
        "    # Show target distribution\n",
        "    target_distribution = training_labeled_df.groupBy(\"is_target_complaint\").count()\n",
        "    print(\"Target distribution in training data:\")\n",
        "    target_distribution.show()\n",
        "\n",
        "    # Calculate class weights for potential use in modeling\n",
        "    total_count = training_labeled_df.count()\n",
        "    class_weights = target_distribution.withColumn(\n",
        "        \"weight\",\n",
        "        F.round(F.lit(total_count) / F.col(\"count\"), 2)\n",
        "    )\n",
        "    print(\"Class weights for potential use in modeling:\")\n",
        "    class_weights.show()\n",
        "\n",
        "    # Select all original columns plus the target column\n",
        "    training_final_df = training_labeled_df.select(\n",
        "        [\"Date received\", \"Product\", \"Sub-product\", \"Issue\", \"Sub-issue\",\n",
        "         \"Consumer complaint narrative\", \"Company public response\", \"Company\",\n",
        "         \"State\", \"ZIP code\", \"Tags\", \"Consumer consent provided?\",\n",
        "         \"Submitted via\", \"Date sent to company\", \"Company response to consumer\",\n",
        "         \"Timely response?\", \"Consumer disputed?\", \"Complaint ID\",\n",
        "         \"is_target_complaint\"]\n",
        "    )\n",
        "\n",
        "    # Write training data to Kafka\n",
        "    print(f\"Writing training data to Kafka topic: {KAFKA_TOPIC_TRAINING}\")\n",
        "    training_kafka_df = training_final_df.selectExpr(\n",
        "        \"Complaint ID AS key\",\n",
        "        \"to_json(struct(*)) AS value\"\n",
        "    )\n",
        "\n",
        "    training_kafka_df.write \\\n",
        "        .format(\"kafka\") \\\n",
        "        .option(\"kafka.bootstrap.servers\", KAFKA_BROKERS) \\\n",
        "        .option(\"topic\", KAFKA_TOPIC_TRAINING) \\\n",
        "        .save()\n",
        "\n",
        "    print(f\"Successfully wrote {training_count} training records to Kafka topic: {KAFKA_TOPIC_TRAINING}\")\n",
        "\n",
        "    # Task 3.3: Prepare Test Data Source\n",
        "    print(\"Preparing test data for persistence...\")\n",
        "\n",
        "    # Select all original columns from test data\n",
        "    test_final_df = test_base_df.select(\n",
        "        [\"Date received\", \"Product\", \"Sub-product\", \"Issue\", \"Sub-issue\",\n",
        "         \"Consumer complaint narrative\", \"Company public response\", \"Company\",\n",
        "         \"State\", \"ZIP code\", \"Tags\", \"Consumer consent provided?\",\n",
        "         \"Submitted via\", \"Date sent to company\", \"Company response to consumer\",\n",
        "         \"Timely response?\", \"Consumer disputed?\", \"Complaint ID\"]\n",
        "    )\n",
        "\n",
        "    # Write test data to persistence path\n",
        "    print(f\"Writing test data to persistence path: {TEST_DATA_PERSISTENCE_PATH}\")\n",
        "    test_final_df.write \\\n",
        "        .format(\"parquet\") \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .save(TEST_DATA_PERSISTENCE_PATH)\n",
        "\n",
        "    print(f\"Successfully wrote {test_count} test records to: {TEST_DATA_PERSISTENCE_PATH}\")\n",
        "\n",
        "    # Return the DataFrames for potential further processing\n",
        "    return training_final_df, test_final_df\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # For demonstration, assume filtered_df exists from previous tasks\n",
        "    # In a real implementation, you would chain these functions together\n",
        "    # or use a workflow orchestration tool\n",
        "\n",
        "    # filtered_df = preprocess_and_filter_from_kafka()\n",
        "    # training_df, test_df = split_label_and_prepare_data(filtered_df)\n",
        "\n",
        "    print(\"Task 3 implementation complete\")"
      ],
      "metadata": {
        "id": "Rtr62ugFe4Pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iOOtWo9me_QO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# Training Feature Engineering Pipeline Definition\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import (\n",
        "    Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer,\n",
        "    OneHotEncoder, VectorAssembler, Imputer, StandardScaler, MinMaxScaler,\n",
        "    RegexTokenizer, CountVectorizer\n",
        ")\n",
        "from pyspark.ml.functions import vector_to_array\n",
        "\n",
        "# Import configuration from the setup file\n",
        "TRAINING_PIPELINE_SAVE_PATH = \"/path/to/training_pipeline\"\n",
        "\n",
        "# Initialize Spark Session (in case this is run as a standalone script)\n",
        "spark = SparkSession.builder.appName(\"Consumer Complaints Feature Engineering\").getOrCreate()\n",
        "\n",
        "def create_training_feature_pipeline():\n",
        "    \"\"\"\n",
        "    Define a comprehensive feature engineering pipeline for the consumer complaints data\n",
        "    \"\"\"\n",
        "    print(\"Creating training feature engineering pipeline...\")\n",
        "\n",
        "    # Define categorical columns to be encoded\n",
        "    categorical_columns = [\n",
        "        \"Product\", \"Sub-product\", \"Issue\", \"Sub-issue\", \"Company\",\n",
        "        \"State\", \"Tags\", \"Consumer consent provided?\", \"Submitted via\",\n",
        "        \"Company response to consumer\", \"Timely response?\", \"Consumer disputed?\"\n",
        "    ]\n",
        "\n",
        "    # Lists to store pipeline stages\n",
        "    stages = []\n",
        "\n",
        "    # ---------- Text Feature Engineering ----------\n",
        "    print(\"Adding text feature engineering stages...\")\n",
        "\n",
        "    # Text cleaning - remove special characters and convert to lowercase\n",
        "    stages.append(\n",
        "        RegexTokenizer(\n",
        "            inputCol=\"Consumer complaint narrative\",\n",
        "            outputCol=\"narrative_tokens\",\n",
        "            pattern=\"\\\\W+\",\n",
        "            toLowercase=True\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Remove stop words\n",
        "    stages.append(\n",
        "        StopWordsRemover(\n",
        "            inputCol=\"narrative_tokens\",\n",
        "            outputCol=\"narrative_filtered\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Generate TF-IDF features\n",
        "    # Using HashingTF for better handling of large vocabularies\n",
        "    stages.append(\n",
        "        HashingTF(\n",
        "            inputCol=\"narrative_filtered\",\n",
        "            outputCol=\"narrative_tf\",\n",
        "            numFeatures=10000  # Adjust based on vocabulary size\n",
        "        )\n",
        "    )\n",
        "\n",
        "    stages.append(\n",
        "        IDF(\n",
        "            inputCol=\"narrative_tf\",\n",
        "            outputCol=\"narrative_features\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # ---------- Categorical Feature Engineering ----------\n",
        "    print(\"Adding categorical feature engineering stages...\")\n",
        "\n",
        "    # Store transformed column names for later use in VectorAssembler\n",
        "    indexed_columns = []\n",
        "    encoded_columns = []\n",
        "\n",
        "    # For each categorical column, create a StringIndexer and OneHotEncoder\n",
        "    for category in categorical_columns:\n",
        "        # Skip columns with too many unique values to avoid explosion\n",
        "        # This is a placeholder - you may need to analyze your data to set thresholds\n",
        "\n",
        "        # Create a StringIndexer for this category\n",
        "        indexer_output = f\"{category}_indexed\"\n",
        "        indexer = StringIndexer(\n",
        "            inputCol=category,\n",
        "            outputCol=indexer_output,\n",
        "            handleInvalid=\"keep\"  # Handle unseen labels as specified\n",
        "        )\n",
        "        stages.append(indexer)\n",
        "        indexed_columns.append(indexer_output)\n",
        "\n",
        "        # Create a OneHotEncoder for this indexed category\n",
        "        encoder_output = f\"{category}_encoded\"\n",
        "        encoder = OneHotEncoder(\n",
        "            inputCol=indexer_output,\n",
        "            outputCol=encoder_output,\n",
        "            dropLast=True  # Drop the last category to avoid collinearity\n",
        "        )\n",
        "        stages.append(encoder)\n",
        "        encoded_columns.append(encoder_output)\n",
        "\n",
        "    # ---------- Numeric Feature Engineering ----------\n",
        "    print(\"Adding numeric feature engineering stages...\")\n",
        "\n",
        "    # Extract ZIP code numeric part (first 5 digits) and convert to numeric\n",
        "    stages.append(\n",
        "        RegexTokenizer(\n",
        "            inputCol=\"ZIP code\",\n",
        "            outputCol=\"zip_numeric_str\",\n",
        "            pattern=\"\\\\D+\",  # Non-digit characters\n",
        "            gaps=True  # Use gaps between tokens\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Convert ZIP numeric string tokens to a single string\n",
        "    stages.append(\n",
        "        Pipeline(stages=[\n",
        "            # Custom UDF to convert array of strings to first element\n",
        "            lambda df: df.withColumn(\n",
        "                \"zip_numeric_str\",\n",
        "                F.when(F.size(F.col(\"zip_numeric_str\")) > 0, F.col(\"zip_numeric_str\")[0])\n",
        "                .otherwise(None)\n",
        "            )\n",
        "        ])\n",
        "    )\n",
        "\n",
        "    # Convert ZIP string to numeric\n",
        "    stages.append(\n",
        "        Pipeline(stages=[\n",
        "            # Custom UDF to convert string to numeric\n",
        "            lambda df: df.withColumn(\n",
        "                \"zip_numeric\",\n",
        "                F.col(\"zip_numeric_str\").cast(IntegerType())\n",
        "            )\n",
        "        ])\n",
        "    )\n",
        "\n",
        "    # Handle missing values in numeric features\n",
        "    numeric_columns = [\"zip_numeric\"]\n",
        "    stages.append(\n",
        "        Imputer(\n",
        "            inputCols=numeric_columns,\n",
        "            outputCols=[f\"{col}_imputed\" for col in numeric_columns],\n",
        "            strategy=\"median\"  # Use median for ZIP codes\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Scale numeric features\n",
        "    for col in numeric_columns:\n",
        "        stages.append(\n",
        "            MinMaxScaler(\n",
        "                inputCol=f\"{col}_imputed\",\n",
        "                outputCol=f\"{col}_scaled\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Get list of scaled numeric columns\n",
        "    scaled_numeric_columns = [f\"{col}_scaled\" for col in numeric_columns]\n",
        "\n",
        "    # ---------- Final Feature Assembly ----------\n",
        "    print(\"Adding final Vector Assembler stage...\")\n",
        "\n",
        "    # Combine all feature columns using VectorAssembler\n",
        "    feature_columns = [\"narrative_features\"] + encoded_columns + scaled_numeric_columns\n",
        "\n",
        "    stages.append(\n",
        "        VectorAssembler(\n",
        "            inputCols=feature_columns,\n",
        "            outputCol=\"features\",\n",
        "            handleInvalid=\"keep\"  # Handle invalid entries\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Create and return the pipeline\n",
        "    training_pipeline = Pipeline(stages=stages)\n",
        "    print(f\"Training pipeline created with {len(stages)} stages\")\n",
        "\n",
        "    return training_pipeline\n",
        "\n",
        "# ---------- Task 5: Advanced Pretraining (Optional - BERT Embeddings) ----------\n",
        "def create_bert_embedding_pipeline():\n",
        "    \"\"\"\n",
        "    Define a pipeline that uses BERT embeddings for text features\n",
        "    This is a placeholder for Task 5 - implementations will vary based on environment\n",
        "    \"\"\"\n",
        "    print(\"Creating BERT embedding pipeline...\")\n",
        "\n",
        "    # Configuration for BERT embedding\n",
        "    TEMP_PYTORCH_DATA_PATH = \"/path/to/temp_pytorch_data\"\n",
        "    EMBEDDING_MODEL_SAVE_PATH = \"/path/to/embedding_model\"\n",
        "    DEEPSPEED_CONFIG_DICT = {\n",
        "        \"train_batch_size\": 32,\n",
        "        \"fp16\": {\"enabled\": True},\n",
        "        \"zero_optimization\": {\"stage\": 2}\n",
        "    }\n",
        "\n",
        "    # Placeholder for the actual BERT embedding implementation\n",
        "    # This would be replaced with actual code if Task 5 is implemented\n",
        "\n",
        "    print(\"\"\"\n",
        "    Task 5 implementation would:\n",
        "    1. Save training data to TEMP_PYTORCH_DATA_PATH\n",
        "    2. Define train_embedding_model() function for fine-tuning BERT\n",
        "    3. Configure DeepspeedTorchDistributor\n",
        "    4. Execute distributed training\n",
        "    5. Define bert_embed_udf for embedding generation\n",
        "    6. Update training_pipeline to use bert_embed_udf\n",
        "    \"\"\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Create the training pipeline\n",
        "    training_pipeline = create_training_feature_pipeline()\n",
        "\n",
        "    # Save the pipeline if needed\n",
        "    training_pipeline.write().overwrite().save(TRAINING_PIPELINE_SAVE_PATH)\n",
        "    print(f\"Training pipeline saved to: {TRAINING_PIPELINE_SAVE_PATH}\")\n",
        "\n",
        "    # Optionally, create BERT embedding pipeline\n",
        "    # Uncomment to implement Task 5\n",
        "    # create_bert_embedding_pipeline()"
      ],
      "metadata": {
        "id": "Gp8D4mpOfADV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# Task 5: Advanced Pretraining with BERT Embeddings\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StopWordsRemover, StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.ml.deepspeed import DeepspeedTorchDistributor\n",
        "\n",
        "# Import necessary libraries for BERT\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Configuration\n",
        "KAFKA_BROKERS = \"kafka1:9092,kafka2:9092\"\n",
        "KAFKA_TOPIC_TRAINING = \"complaints-training-data\"\n",
        "TEMP_PYTORCH_DATA_PATH = \"/path/to/temp_pytorch_data\"\n",
        "EMBEDDING_MODEL_SAVE_PATH = \"/path/to/embedding_model\"\n",
        "TRAINING_PIPELINE_SAVE_PATH = \"/path/to/training_pipeline\"\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"Consumer Complaints BERT Embeddings\").getOrCreate()\n",
        "\n",
        "# Define a PyTorch Dataset for complaint narratives\n",
        "class ComplaintDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "\n",
        "        # Tokenize the text\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten()\n",
        "        }\n",
        "\n",
        "# Function to extract and save narrative data from Kafka for PyTorch\n",
        "def extract_narratives_for_pytorch():\n",
        "    \"\"\"\n",
        "    Read complaint narratives from Kafka and save to format for PyTorch\n",
        "    \"\"\"\n",
        "    print(\"Extracting narratives from Kafka for PyTorch...\")\n",
        "\n",
        "    # Read from Kafka topic\n",
        "    kafka_df = spark.read \\\n",
        "        .format(\"kafka\") \\\n",
        "        .option(\"kafka.bootstrap.servers\", KAFKA_BROKERS) \\\n",
        "        .option(\"subscribe\", KAFKA_TOPIC_TRAINING) \\\n",
        "        .option(\"startingOffsets\", \"earliest\") \\\n",
        "        .load()\n",
        "\n",
        "    # Parse JSON and extract relevant columns\n",
        "    parsed_df = kafka_df.select(\n",
        "        F.col(\"key\").cast(\"string\").alias(\"complaint_id\"),\n",
        "        F.from_json(F.col(\"value\").cast(\"string\"), full_schema).alias(\"data\")\n",
        "    ).select(\n",
        "        \"complaint_id\",\n",
        "        \"data.Consumer complaint narrative\"\n",
        "    )\n",
        "\n",
        "    # Filter out nulls and empty narratives\n",
        "    filtered_df = parsed_df.filter(\n",
        "        (F.col(\"Consumer complaint narrative\").isNotNull()) &\n",
        "        (F.length(F.trim(F.col(\"Consumer complaint narrative\"))) > 0)\n",
        "    )\n",
        "\n",
        "    print(f\"Extracted {filtered_df.count()} narratives for BERT training\")\n",
        "\n",
        "    # Save to parquet for PyTorch processing\n",
        "    filtered_df.write \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .parquet(TEMP_PYTORCH_DATA_PATH)\n",
        "\n",
        "    print(f\"Narratives saved to {TEMP_PYTORCH_DATA_PATH}\")\n",
        "\n",
        "# Define the BERT fine-tuning function\n",
        "def train_embedding_model(data_path, epochs=2, batch_size=32, learning_rate=2e-5):\n",
        "    \"\"\"\n",
        "    Fine-tune a BERT model on complaint narratives\n",
        "    \"\"\"\n",
        "    print(\"Starting BERT model fine-tuning...\")\n",
        "\n",
        "    # Load data from parquet\n",
        "    data_df = spark.read.parquet(data_path)\n",
        "\n",
        "    # Convert to pandas for easier PyTorch integration\n",
        "    pd_df = data_df.toPandas()\n",
        "    texts = pd_df[\"Consumer complaint narrative\"].tolist()\n",
        "\n",
        "    # Initialize tokenizer and model\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    dataset = ComplaintDataset(texts, tokenizer)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        sampler=RandomSampler(dataset),\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # Setup optimizer and scheduler\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n",
        "    total_steps = len(dataloader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Starting epoch {epoch+1}/{epochs}\")\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for step, batch in enumerate(dataloader):\n",
        "            # Get inputs\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            # Clear gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Forward pass - we'll use MLM (Masked Language Modeling) objective\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                output_hidden_states=True\n",
        "            )\n",
        "\n",
        "            # Use the CLS token representation as sentence embedding\n",
        "            # and calculate a simple contrastive loss\n",
        "            hidden_states = outputs.last_hidden_state\n",
        "            cls_embeddings = hidden_states[:, 0, :]\n",
        "\n",
        "            # Simple contrastive loss (maximize similarity within batch)\n",
        "            similarity_matrix = torch.matmul(cls_embeddings, cls_embeddings.T)\n",
        "\n",
        "            # Create targets (identity matrix)\n",
        "            targets = torch.eye(similarity_matrix.size(0)).to(device)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss_fct = torch.nn.MSELoss()\n",
        "            loss = loss_fct(similarity_matrix, targets)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if step % 100 == 0:\n",
        "                print(f\"  Batch {step}/{len(dataloader)} - Loss: {loss.item():.4f}\")\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Save the fine-tuned model and tokenizer\n",
        "    os.makedirs(EMBEDDING_MODEL_SAVE_PATH, exist_ok=True)\n",
        "    model.save_pretrained(EMBEDDING_MODEL_SAVE_PATH)\n",
        "    tokenizer.save_pretrained(EMBEDDING_MODEL_SAVE_PATH)\n",
        "\n",
        "    print(f\"BERT model fine-tuned and saved to {EMBEDDING_MODEL_SAVE_PATH}\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# Define a Spark UDF for BERT embeddings\n",
        "def create_bert_embedding_udf():\n",
        "    \"\"\"\n",
        "    Create a UDF that loads the fine-tuned BERT model and converts text to embeddings\n",
        "    \"\"\"\n",
        "    print(\"Creating BERT embedding UDF...\")\n",
        "\n",
        "    # Load the fine-tuned model and tokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained(EMBEDDING_MODEL_SAVE_PATH)\n",
        "    model = BertModel.from_pretrained(EMBEDDING_MODEL_SAVE_PATH)\n",
        "\n",
        "    # Move to CPU as Spark workers will use the UDF\n",
        "    model.to(\"cpu\")\n",
        "    model.eval()\n",
        "\n",
        "    # Define the UDF function\n",
        "    def bert_embed(text):\n",
        "        if not text or len(text.strip()) == 0:\n",
        "            # Return zeros for empty text\n",
        "            return [0.0] * 768  # BERT base hidden size is 768\n",
        "\n",
        "        # Tokenize and prepare input\n",
        "        inputs = tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "\n",
        "        # Generate embeddings\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        # Get the [CLS] token embedding (first token)\n",
        "        embeddings = outputs.last_hidden_state[:, 0, :].squeeze().tolist()\n",
        "        return embeddings\n",
        "\n",
        "    # Register the UDF\n",
        "    bert_embed_udf = udf(bert_embed, ArrayType(FloatType()))\n",
        "\n",
        "    print(\"BERT embedding UDF created\")\n",
        "    return bert_embed_udf\n",
        "\n",
        "# Main execution for Task 5\n",
        "def execute_bert_embedding_task():\n",
        "    \"\"\"\n",
        "    Execute the BERT embedding task with distributed training\n",
        "    \"\"\"\n",
        "    print(\"Executing Task 5: BERT embedding generation...\")\n",
        "\n",
        "    # Step 1: Extract narratives and save to temporary location\n",
        "    extract_narratives_for_pytorch()\n",
        "\n",
        "    # Step 2: Configure DeepspeedTorchDistributor\n",
        "    distributor = DeepspeedTorchDistributor(\n",
        "        numGpus=4,  # Adjust based on available resources\n",
        "        nnodes=1,\n",
        "        localMode=False,\n",
        "        useGpu=True,\n",
        "        deepspeedConfig={\n",
        "            \"train_batch_size\": 32,\n",
        "            \"fp16\": {\"enabled\": True},\n",
        "            \"zero_optimization\": {\"stage\": 2}\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Step 3: Run distributed training\n",
        "    print(\"Starting distributed BERT fine-tuning...\")\n",
        "    distributor.run(\n",
        "        train_embedding_model,\n",
        "        TEMP_PYTORCH_DATA_PATH,\n",
        "        epochs=3,\n",
        "        batch_size=32,\n",
        "        learning_rate=2e-5\n",
        "    )\n",
        "\n",
        "    # Step 4: Create UDF for embeddings\n",
        "    bert_embed_udf = create_bert_embedding_udf()\n",
        "\n",
        "    # Step 5: Update the training pipeline\n",
        "    # This would be done by modifying the training_pipeline from Task 4\n",
        "    print(\"\"\"\n",
        "    To complete Task 5:\n",
        "    1. Replace the basic text feature stages (tokenizer, stopwords, etc.) with the BERT UDF\n",
        "    2. The UDF would be applied directly to 'Consumer complaint narrative'\n",
        "    3. The output would be aliased as 'narrative_features'\n",
        "    4. The resulting vector would then be used in the VectorAssembler stage\n",
        "    \"\"\")\n",
        "\n",
        "    return bert_embed_udf\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the full schema (needed for parsing Kafka data)\n",
        "    full_schema = StructType([\n",
        "        StructField(\"Date received\", StringType(), True),\n",
        "        StructField(\"Product\", StringType(), True),\n",
        "        StructField(\"Sub-product\", StringType(), True),\n",
        "        StructField(\"Issue\", StringType(), True),\n",
        "        StructField(\"Sub-issue\", StringType(), True),\n",
        "        StructField(\"Consumer complaint narrative\", StringType(), True),\n",
        "        StructField(\"Company public response\", StringType(), True),\n",
        "        StructField(\"Company\", StringType(), True),\n",
        "        StructField(\"State\", StringType(), True),\n",
        "        StructField(\"ZIP code\", StringType(), True),\n",
        "        StructField(\"Tags\", StringType(), True),\n",
        "        StructField(\"Consumer consent provided?\", StringType(), True),\n",
        "        StructField(\"Submitted via\", StringType(), True),\n",
        "        StructField(\"Date sent to company\", StringType(), True),\n",
        "        StructField(\"Company response to consumer\", StringType(), True),\n",
        "        StructField(\"Timely response?\", StringType(), True),\n",
        "        StructField(\"Consumer disputed?\", StringType(), True),\n",
        "        StructField(\"Complaint ID\", StringType(), True)\n",
        "    ])\n",
        "\n",
        "    # Execute the BERT embedding task\n",
        "    bert_embed_udf = execute_bert_embedding_task()\n",
        "\n",
        "    print(\"Task 5 (BERT Embeddings) completed\")"
      ],
      "metadata": {
        "id": "y3w9W6lFe_oH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# Task 6: Classifier Training, Comparison & Cross-Validation\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier, LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "import time\n",
        "import json\n",
        "\n",
        "# Import configuration from the setup file\n",
        "KAFKA_BROKERS = \"kafka1:9092,kafka2:9092\"\n",
        "KAFKA_TOPIC_TRAINING = \"complaints-training-data\"\n",
        "TRAINING_PIPELINE_SAVE_PATH = \"/path/to/training_pipeline\"\n",
        "BEST_MODEL_SAVE_PATH = \"/path/to/best_model\"\n",
        "NUM_FOLDS = 5\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"Consumer Complaints Classifier Training\").getOrCreate()\n",
        "\n",
        "def train_and_evaluate_classifiers(training_pipeline):\n",
        "    \"\"\"\n",
        "    Train and evaluate multiple classifiers using cross-validation\n",
        "    \"\"\"\n",
        "    print(\"Starting classifier training and evaluation...\")\n",
        "\n",
        "    # Step 1: Read training data from Kafka\n",
        "    print(f\"Reading training data from Kafka topic: {KAFKA_TOPIC_TRAINING}\")\n",
        "\n",
        "    # Define the full schema including is_target_complaint\n",
        "    full_schema_with_target = StructType([\n",
        "        StructField(\"Date received\", StringType(), True),\n",
        "        StructField(\"Product\", StringType(), True),\n",
        "        StructField(\"Sub-product\", StringType(), True),\n",
        "        StructField(\"Issue\", StringType(), True),\n",
        "        StructField(\"Sub-issue\", StringType(), True),\n",
        "        StructField(\"Consumer complaint narrative\", StringType(), True),\n",
        "        StructField(\"Company public response\", StringType(), True),\n",
        "        StructField(\"Company\", StringType(), True),\n",
        "        StructField(\"State\", StringType(), True),\n",
        "        StructField(\"ZIP code\", StringType(), True),\n",
        "        StructField(\"Tags\", StringType(), True),\n",
        "        StructField(\"Consumer consent provided?\", StringType(), True),\n",
        "        StructField(\"Submitted via\", StringType(), True),\n",
        "        StructField(\"Date sent to company\", StringType(), True),\n",
        "        StructField(\"Company response to consumer\", StringType(), True),\n",
        "        StructField(\"Timely response?\", StringType(), True),\n",
        "        StructField(\"Consumer disputed?\", StringType(), True),\n",
        "        StructField(\"Complaint ID\", StringType(), True),\n",
        "        StructField(\"is_target_complaint\", IntegerType(), True)\n",
        "    ])\n",
        "\n",
        "    # Read from Kafka and parse JSON values\n",
        "    kafka_df = spark.read \\\n",
        "        .format(\"kafka\") \\\n",
        "        .option(\"kafka.bootstrap.servers\", KAFKA_BROKERS) \\\n",
        "        .option(\"subscribe\", KAFKA_TOPIC_TRAINING) \\\n",
        "        .option(\"startingOffsets\", \"earliest\") \\\n",
        "        .load()\n",
        "\n",
        "    # Parse the JSON values\n",
        "    training_df = kafka_df.select(\n",
        "        F.from_json(F.col(\"value\").cast(\"string\"), full_schema_with_target).alias(\"data\")\n",
        "    ).select(\"data.*\")\n",
        "\n",
        "    # Display training data overview\n",
        "    print(f\"Training data loaded: {training_df.count()} records\")\n",
        "    print(\"Target distribution:\")\n",
        "    training_df.groupBy(\"is_target_complaint\").count().show()\n",
        "\n",
        "    # Step 2: Define classifier instances\n",
        "    print(\"Defining classifier instances and parameter grids...\")\n",
        "\n",
        "    # Random Forest Classifier\n",
        "    rf = RandomForestClassifier(\n",
        "        featuresCol=\"features\",\n",
        "        labelCol=\"is_target_complaint\",\n",
        "        predictionCol=\"prediction\",\n",
        "        probabilityCol=\"probability\"\n",
        "    )\n",
        "\n",
        "    # Gradient-Boosted Trees Classifier\n",
        "    gbt = GBTClassifier(\n",
        "        featuresCol=\"features\",\n",
        "        labelCol=\"is_target_complaint\",\n",
        "        predictionCol=\"prediction\",\n",
        "        maxIter=10  # Limit iterations for GBT to prevent overfitting\n",
        "    )\n",
        "\n",
        "    # Logistic Regression Classifier\n",
        "    lr = LogisticRegression(\n",
        "        featuresCol=\"features\",\n",
        "        labelCol=\"is_target_complaint\",\n",
        "        predictionCol=\"prediction\",\n",
        "        probabilityCol=\"probability\"\n",
        "    )\n",
        "\n",
        "    # Step 3: Define parameter grids for hyperparameter tuning\n",
        "    rf_param_grid = ParamGridBuilder() \\\n",
        "        .addGrid(rf.numTrees, [10, 50, 100]) \\\n",
        "        .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
        "        .addGrid(rf.impurity, [\"gini\", \"entropy\"]) \\\n",
        "        .build()\n",
        "\n",
        "    gbt_param_grid = ParamGridBuilder() \\\n",
        "        .addGrid(gbt.maxDepth, [5, 10]) \\\n",
        "        .addGrid(gbt.stepSize, [0.1, 0.05]) \\\n",
        "        .addGrid(gbt.maxIter, [10, 20]) \\\n",
        "        .build()\n",
        "\n",
        "    lr_param_grid = ParamGridBuilder() \\\n",
        "        .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
        "        .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
        "        .addGrid(lr.maxIter, [10, 50, 100]) \\\n",
        "        .build()\n",
        "\n",
        "    # Store models and their parameter grids in a dictionary\n",
        "    models_and_params = {\n",
        "        \"RandomForest\": (rf, rf_param_grid),\n",
        "        \"GradientBoostedTrees\": (gbt, gbt_param_grid),\n",
        "        \"LogisticRegression\": (lr, lr_param_grid)\n",
        "    }\n",
        "\n",
        "    # Step 4: Define the evaluator\n",
        "    evaluator = BinaryClassificationEvaluator(\n",
        "        rawPredictionCol=\"probability\",\n",
        "        labelCol=\"is_target_complaint\",\n",
        "        metricName=\"areaUnderROC\"\n",
        "    )\n",
        "\n",
        "    # Step 5: Train and evaluate each model\n",
        "    results = {}\n",
        "    best_cv_score = 0.0\n",
        "    best_model_name = None\n",
        "    best_model_params = None\n",
        "    best_cv_model = None\n",
        "\n",
        "    for model_name, (classifier, param_grid) in models_and_params.items():\n",
        "        print(f\"Training and evaluating {model_name}...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Create a pipeline with feature engineering and the classifier\n",
        "        model_pipeline = Pipeline(stages=[training_pipeline, classifier])\n",
        "\n",
        "        # Create a cross-validator\n",
        "        cv = CrossValidator(\n",
        "            estimator=model_pipeline,\n",
        "            estimatorParamMaps=param_grid,\n",
        "            evaluator=evaluator,\n",
        "            numFolds=NUM_FOLDS,\n",
        "            parallelism=4  # Adjust based on cluster capacity\n",
        "        )\n",
        "\n",
        "        # Fit the cross-validator to find the best model\n",
        "        cv_model = cv.fit(training_df)\n",
        "\n",
        "        # Get the average metrics across all folds\n",
        "        avg_metrics = cv_model.avgMetrics\n",
        "        best_metric_idx = avg_metrics.index(max(avg_metrics))\n",
        "        best_metric = avg_metrics[best_metric_idx]\n",
        "        best_params = param_grid[best_metric_idx]\n",
        "\n",
        "        # Record the results\n",
        "        results[model_name] = {\n",
        "            \"best_score\": best_metric,\n",
        "            \"best_params\": str(best_params),\n",
        "            \"training_time\": time.time() - start_time\n",
        "        }\n",
        "\n",
        "        print(f\"{model_name} best CV score: {best_metric:.4f}\")\n",
        "\n",
        "        # Check if this is the best model overall\n",
        "        if best_metric > best_cv_score:\n",
        "            best_cv_score = best_metric\n",
        "            best_model_name = model_name\n",
        "            best_model_params = best_params\n",
        "            best_cv_model = cv_model\n",
        "\n",
        "    # Step 6: Display comparison of models\n",
        "    print(\"\\nModel Comparison Results:\")\n",
        "    for model_name, metrics in results.items():\n",
        "        print(f\"{model_name}:\")\n",
        "        print(f\"  Best AUC-ROC: {metrics['best_score']:.4f}\")\n",
        "        print(f\"  Best Parameters: {metrics['best_params']}\")\n",
        "        print(f\"  Training Time: {metrics['training_time']:.2f} seconds\")\n",
        "\n",
        "    print(f\"\\nOverall Best Model: {best_model_name}\")\n",
        "    print(f\"Best AUC-ROC: {best_cv_score:.4f}\")\n",
        "    print(f\"Best Parameters: {str(best_model_params)}\")\n",
        "\n",
        "    # Step 7: Retrain the best model on the entire training dataset\n",
        "    print(f\"\\nRetraining {best_model_name} with optimal parameters on entire training dataset...\")\n",
        "\n",
        "    # Get the best classifier type and configure with best parameters\n",
        "    best_classifier_base = models_and_params[best_model_name][0]\n",
        "    best_classifier = best_classifier_base.copy(ParamMap(best_model_params))\n",
        "\n",
        "    # Create a pipeline with the training pipeline and the best classifier\n",
        "    final_pipeline = Pipeline(stages=[training_pipeline, best_classifier])\n",
        "\n",
        "    # Fit the pipeline on the entire training dataset\n",
        "    final_model = final_pipeline.fit(training_df)\n",
        "\n",
        "    # Save the fitted training pipeline (feature engineering part)\n",
        "    fitted_training_pipeline = final_model.stages[0]\n",
        "    fitted_training_pipeline.write().overwrite().save(TRAINING_PIPELINE_SAVE_PATH)\n",
        "    print(f\"Fitted training pipeline saved to: {TRAINING_PIPELINE_SAVE_PATH}\")\n",
        "\n",
        "    # Save the trained classifier model\n",
        "    trained_classifier = final_model.stages[1]\n",
        "    trained_classifier.write().overwrite().save(BEST_MODEL_SAVE_PATH)\n",
        "    print(f\"Trained classifier model saved to: {BEST_MODEL_SAVE_PATH}\")\n",
        "\n",
        "    # Save model performance metrics to file\n",
        "    metrics_file = f\"{BEST_MODEL_SAVE_PATH}_metrics.json\"\n",
        "    with open(metrics_file, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    print(f\"Model performance metrics saved to: {metrics_file}\")\n",
        "\n",
        "    return final_model, results\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Load the training pipeline from Task 4 (or Task 5 if BERT embeddings were used)\n",
        "    try:\n",
        "        training_pipeline = Pipeline.load(TRAINING_PIPELINE_SAVE_PATH)\n",
        "        print(f\"Loaded training pipeline from: {TRAINING_PIPELINE_SAVE_PATH}\")\n",
        "    except:\n",
        "        # If not available, import it from the training_pipeline module\n",
        "        from training_pipeline import create_training_feature_pipeline\n",
        "        training_pipeline = create_training_feature_pipeline()\n",
        "        print(\"Created new training pipeline instance\")\n",
        "\n",
        "    # Train and evaluate classifiers\n",
        "    final_model, results = train_and_evaluate_classifiers(training_pipeline)\n",
        "\n",
        "    print(\"Task 6: Classifier Training, Comparison & Cross-Validation completed\")"
      ],
      "metadata": {
        "id": "AVf5at24fSAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# Task 7: Simulation Script for Streaming Test Data to Kafka\n",
        "\n",
        "import json\n",
        "import time\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from kafka import KafkaProducer\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Configuration (these values can be overridden via command-line arguments)\n",
        "TEST_DATA_PERSISTENCE_PATH = \"/path/to/test_data_source.parquet\"\n",
        "KAFKA_BROKERS = \"kafka1:9092,kafka2:9092\"\n",
        "KAFKA_TOPIC_TESTING_STREAM = \"complaints-testing-stream\"\n",
        "MESSAGES_PER_MINUTE = 10000  # Target throughput\n",
        "BATCH_SIZE = 100  # Send messages in batches for efficiency\n",
        "\n",
        "def load_data_with_pandas():\n",
        "    \"\"\"\n",
        "    Load test data using pandas for smaller datasets\n",
        "    Returns a pandas DataFrame with all records\n",
        "    \"\"\"\n",
        "    print(f\"Loading test data from {TEST_DATA_PERSISTENCE_PATH} using pandas...\")\n",
        "\n",
        "    if TEST_DATA_PERSISTENCE_PATH.endswith('.parquet'):\n",
        "        df = pd.read_parquet(TEST_DATA_PERSISTENCE_PATH)\n",
        "    elif TEST_DATA_PERSISTENCE_PATH.endswith('.json'):\n",
        "        df = pd.read_json(TEST_DATA_PERSISTENCE_PATH)\n",
        "    elif TEST_DATA_PERSISTENCE_PATH.endswith('.csv'):\n",
        "        df = pd.read_csv(TEST_DATA_PERSISTENCE_PATH)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file format: {TEST_DATA_PERSISTENCE_PATH}\")\n",
        "\n",
        "    print(f\"Loaded {len(df)} records\")\n",
        "    return df\n",
        "\n",
        "def load_data_with_spark():\n",
        "    \"\"\"\n",
        "    Load test data using Spark for larger datasets\n",
        "    Returns a list of dictionaries with all records\n",
        "    \"\"\"\n",
        "    print(f\"Loading test data from {TEST_DATA_PERSISTENCE_PATH} using Spark...\")\n",
        "\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"Test Data Reader\") \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    # Read the test data\n",
        "    test_df = spark.read.format(\"parquet\").load(TEST_DATA_PERSISTENCE_PATH)\n",
        "\n",
        "    # Convert to pandas to make it easier to iterate\n",
        "    pd_df = test_df.toPandas()\n",
        "\n",
        "    print(f\"Loaded {len(pd_df)} records\")\n",
        "    spark.stop()\n",
        "\n",
        "    return pd_df\n",
        "\n",
        "def prepare_messages(df):\n",
        "    \"\"\"\n",
        "    Prepare messages for Kafka by selecting only the required columns\n",
        "    and converting to JSON\n",
        "    \"\"\"\n",
        "    print(\"Preparing messages for Kafka...\")\n",
        "\n",
        "    # Select only the 7 specified columns\n",
        "    required_columns = [\n",
        "        \"Date received\",\n",
        "        \"Complaint ID\",\n",
        "        \"Company\",\n",
        "        \"State\",\n",
        "        \"ZIP code\",\n",
        "        \"Submitted via\",\n",
        "        \"Consumer complaint narrative\"\n",
        "    ]\n",
        "\n",
        "    # Ensure all required columns are in the DataFrame\n",
        "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "    if missing_columns:\n",
        "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
        "\n",
        "    # Select only the required columns\n",
        "    df_subset = df[required_columns]\n",
        "\n",
        "    # Convert to list of dictionaries for easier processing\n",
        "    messages = df_subset.to_dict('records')\n",
        "\n",
        "    print(f\"Prepared {len(messages)} messages\")\n",
        "    return messages\n",
        "\n",
        "def send_messages_to_kafka(messages, kafka_brokers, topic, messages_per_minute):\n",
        "    \"\"\"\n",
        "    Send messages to Kafka at the specified rate\n",
        "    \"\"\"\n",
        "    print(f\"Connecting to Kafka brokers: {kafka_brokers}\")\n",
        "\n",
        "    # Create Kafka producer\n",
        "    producer = KafkaProducer(\n",
        "        bootstrap_servers=kafka_brokers.split(','),\n",
        "        value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
        "        key_serializer=lambda k: k.encode('utf-8'),\n",
        "        batch_size=16384,  # Adjust based on message size\n",
        "        linger_ms=5,       # Small delay to allow batching\n",
        "        buffer_memory=33554432  # 32MB buffer\n",
        "    )\n",
        "\n",
        "    total_messages = len(messages)\n",
        "    print(f\"Sending {total_messages} messages to topic '{topic}' at {messages_per_minute} messages/minute...\")\n",
        "\n",
        "    # Calculate delay between messages to achieve target rate\n",
        "    # Add some margin to account for processing overhead\n",
        "    delay_between_messages"
      ],
      "metadata": {
        "id": "mD0sENP6faq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# Task 7: Simulation Script for Streaming Test Data to Kafka\n",
        "\n",
        "import json\n",
        "import time\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from kafka import KafkaProducer\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Configuration (these values can be overridden via command-line arguments)\n",
        "TEST_DATA_PERSISTENCE_PATH = \"/path/to/test_data_source.parquet\"\n",
        "KAFKA_BROKERS = \"kafka1:9092,kafka2:9092\"\n",
        "KAFKA_TOPIC_TESTING_STREAM = \"complaints-testing-stream\"\n",
        "MESSAGES_PER_MINUTE = 10000  # Target throughput\n",
        "BATCH_SIZE = 100  # Send messages in batches for efficiency\n",
        "\n",
        "def load_data_with_pandas():\n",
        "    \"\"\"\n",
        "    Load test data using pandas for smaller datasets\n",
        "    Returns a pandas DataFrame with all records\n",
        "    \"\"\"\n",
        "    print(f\"Loading test data from {TEST_DATA_PERSISTENCE_PATH} using pandas...\")\n",
        "\n",
        "    if TEST_DATA_PERSISTENCE_PATH.endswith('.parquet'):\n",
        "        df = pd.read_parquet(TEST_DATA_PERSISTENCE_PATH)\n",
        "    elif TEST_DATA_PERSISTENCE_PATH.endswith('.json'):\n",
        "        df = pd.read_json(TEST_DATA_PERSISTENCE_PATH)\n",
        "    elif TEST_DATA_PERSISTENCE_PATH.endswith('.csv'):\n",
        "        df = pd.read_csv(TEST_DATA_PERSISTENCE_PATH)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file format: {TEST_DATA_PERSISTENCE_PATH}\")\n",
        "\n",
        "    print(f\"Loaded {len(df)} records\")\n",
        "    return df\n",
        "\n",
        "def load_data_with_spark():\n",
        "    \"\"\"\n",
        "    Load test data using Spark for larger datasets\n",
        "    Returns a list of dictionaries with all records\n",
        "    \"\"\"\n",
        "    print(f\"Loading test data from {TEST_DATA_PERSISTENCE_PATH} using Spark...\")\n",
        "\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"Test Data Reader\") \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    # Read the test data\n",
        "    test_df = spark.read.format(\"parquet\").load(TEST_DATA_PERSISTENCE_PATH)\n",
        "\n",
        "    # Convert to pandas to make it easier to iterate\n",
        "    pd_df = test_df.toPandas()\n",
        "\n",
        "    print(f\"Loaded {len(pd_df)} records\")\n",
        "    spark.stop()\n",
        "\n",
        "    return pd_df\n",
        "\n",
        "def prepare_messages(df):\n",
        "    \"\"\"\n",
        "    Prepare messages for Kafka by selecting only the required columns\n",
        "    and converting to JSON\n",
        "    \"\"\"\n",
        "    print(\"Preparing messages for Kafka...\")\n",
        "\n",
        "    # Select only the 7 specified columns\n",
        "    required_columns = [\n",
        "        \"Date received\",\n",
        "        \"Complaint ID\",\n",
        "        \"Company\",\n",
        "        \"State\",\n",
        "        \"ZIP code\",\n",
        "        \"Submitted via\",\n",
        "        \"Consumer complaint narrative\"\n",
        "    ]\n",
        "\n",
        "    # Ensure all required columns are in the DataFrame\n",
        "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "    if missing_columns:\n",
        "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
        "\n",
        "    # Select only the required columns\n",
        "    df_subset = df[required_columns]\n",
        "\n",
        "    # Convert to list of dictionaries for easier processing\n",
        "    messages = df_subset.to_dict('records')\n",
        "\n",
        "    print(f\"Prepared {len(messages)} messages\")\n",
        "    return messages\n",
        "\n",
        "def send_messages_to_kafka(messages, kafka_brokers, topic, messages_per_minute):\n",
        "    \"\"\"\n",
        "    Send messages to Kafka at the specified rate\n",
        "    \"\"\"\n",
        "    print(f\"Connecting to Kafka brokers: {kafka_brokers}\")\n",
        "\n",
        "    # Create Kafka producer\n",
        "    producer = KafkaProducer(\n",
        "        bootstrap_servers=kafka_brokers.split(','),\n",
        "        value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
        "        key_serializer=lambda k: k.encode('utf-8'),\n",
        "        batch_size=16384,  # Adjust based on message size\n",
        "        linger_ms=5,       # Small delay to allow batching\n",
        "        buffer_memory=33554432  # 32MB buffer\n",
        "    )\n",
        "\n",
        "    total_messages = len(messages)\n",
        "    print(f\"Sending {total_messages} messages to topic '{topic}' at {messages_per_minute} messages/minute...\")\n",
        "\n",
        "    # Calculate delay between messages to achieve target rate\n",
        "    # Add some margin to account for processing overhead\n",
        "    delay_between_messages = 60.0 / messages_per_minute\n",
        "\n",
        "    start_time = time.time()\n",
        "    messages_sent = 0\n",
        "    batches_sent = 0\n",
        "\n",
        "    try:\n",
        "        # Process messages in batches for efficiency\n",
        "        for i in range(0, len(messages), BATCH_SIZE):\n",
        "            batch_start_time = time.time()\n",
        "\n",
        "            # Get the current batch of messages\n",
        "            batch = messages[i:i + BATCH_SIZE]\n",
        "            batch_size = len(batch)\n",
        "\n",
        "            # Send each message in the batch\n",
        "            for msg in batch:\n",
        "                # Use Complaint ID as the key\n",
        "                key = msg[\"Complaint ID\"]\n",
        "\n",
        "                # Send the message (async)\n",
        "                producer.send(topic, key=key, value=msg)\n",
        "                messages_sent += 1\n",
        "\n",
        "            # Make sure all messages are sent\n",
        "            producer.flush()\n",
        "            batches_sent += 1\n",
        "\n",
        "            # Calculate time spent sending this batch\n",
        "            batch_elapsed = time.time() - batch_start_time\n",
        "\n",
        "            # Calculate required delay to maintain rate\n",
        "            target_batch_time = batch_size * delay_between_messages\n",
        "            sleep_time = max(0, target_batch_time - batch_elapsed)\n",
        "\n",
        "            # Sleep if needed to maintain rate\n",
        "            if sleep_time > 0:\n",
        "                time.sleep(sleep_time)\n",
        "\n",
        "            # Print progress\n",
        "            if batches_sent % 10 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                rate = messages_sent / elapsed * 60\n",
        "                percent_complete = messages_sent / total_messages * 100\n",
        "                remaining = total_messages - messages_sent\n",
        "                eta = remaining / rate * 60 if rate > 0 else 0\n",
        "\n",
        "                print(f\"Progress: {messages_sent}/{total_messages} messages \"\n",
        "                      f\"({percent_complete:.1f}%) @ {rate:.1f} msgs/min, \"\n",
        "                      f\"ETA: {eta/60:.1f} minutes\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nInterrupted by user. Stopping...\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error sending messages: {e}\")\n",
        "    finally:\n",
        "        # Close the producer\n",
        "        producer.close()\n",
        "\n",
        "        # Print final statistics\n",
        "        elapsed = time.time() - start_time\n",
        "        rate = messages_sent / elapsed * 60 if elapsed > 0 else 0\n",
        "\n",
        "        print(f\"\\nSummary:\")\n",
        "        print(f\"- Total messages sent: {messages_sent}/{total_messages}\")\n",
        "        print(f\"- Total time: {elapsed:.2f} seconds\")\n",
        "        print(f\"- Average rate: {rate:.1f} messages/minute\")\n",
        "        print(f\"- Number of batches: {batches_sent}\")\n",
        "\n",
        "def main():\n",
        "    # Parse command-line arguments\n",
        "    parser = argparse.ArgumentParser(description=\"Stream test data to Kafka\")\n",
        "    parser.add_argument(\"--data-path\", default=TEST_DATA_PERSISTENCE_PATH,\n",
        "                        help=\"Path to test data (parquet, json, or csv)\")\n",
        "    parser.add_argument(\"--brokers\", default=KAFKA_BROKERS,\n",
        "                        help=\"Comma-separated list of Kafka broker addresses\")\n",
        "    parser.add_argument(\"--topic\", default=KAFKA_TOPIC_TESTING_STREAM,\n",
        "                        help=\"Kafka topic to send messages to\")\n",
        "    parser.add_argument(\"--rate\", type=int, default=MESSAGES_PER_MINUTE,\n",
        "                        help=\"Number of messages per minute to send\")\n",
        "    parser.add_argument(\"--batch-size\", type=int, default=BATCH_SIZE,\n",
        "                        help=\"Number of messages to send in each batch\")\n",
        "    parser.add_argument(\"--use-spark\", action=\"store_true\",\n",
        "                        help=\"Use Spark to load data (for very large datasets)\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Update global variables with command-line arguments\n",
        "    global TEST_DATA_PERSISTENCE_PATH, KAFKA_BROKERS, KAFKA_TOPIC_TESTING_STREAM\n",
        "    global MESSAGES_PER_MINUTE, BATCH_SIZE\n",
        "\n",
        "    TEST_DATA_PERSISTENCE_PATH = args.data_path\n",
        "    KAFKA_BROKERS = args.brokers\n",
        "    KAFKA_TOPIC_TESTING_STREAM = args.topic\n",
        "    MESSAGES_PER_MINUTE = args.rate\n",
        "    BATCH_SIZE = args.batch_size\n",
        "\n",
        "    # Load data\n",
        "    if args.use_spark:\n",
        "        df = load_data_with_spark()\n",
        "    else:\n",
        "        df = load_data_with_pandas()\n",
        "\n",
        "    # Prepare messages\n",
        "    messages = prepare_messages(df)\n",
        "\n",
        "    # Send messages to Kafka\n",
        "    send_messages_to_kafka(messages, KAFKA_BROKERS, KAFKA_TOPIC_TESTING_STREAM, MESSAGES_PER_MINUTE)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "xHu8oiZ-fd9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# Task 8: Define Streaming Inference Pipeline\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "from pyspark.ml.feature import (\n",
        "    Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer,\n",
        "    OneHotEncoder, VectorAssembler, Imputer, StandardScaler, MinMaxScaler,\n",
        "    RegexTokenizer, CountVectorizer\n",
        ")\n",
        "from pyspark.ml.classification import RandomForestClassificationModel\n",
        "from pyspark.ml.functions import vector_to_array\n",
        "import os\n",
        "\n",
        "# Import configuration\n",
        "TRAINING_PIPELINE_SAVE_PATH = \"/path/to/training_pipeline\"\n",
        "BEST_MODEL_SAVE_PATH = \"/path/to/best_model\"\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"Consumer Complaints Streaming Inference\").getOrCreate()\n",
        "\n",
        "def create_streaming_inference_pipeline(use_bert_embeddings=False):\n",
        "    \"\"\"\n",
        "    Create a pipeline for streaming inference that processes only the 7 available columns\n",
        "\n",
        "    Args:\n",
        "        use_bert_embeddings: Whether to use BERT embeddings (from Task 5) or basic text features\n",
        "\n",
        "    Returns:\n",
        "        inference_pipeline: A PipelineModel for streaming inference\n",
        "    \"\"\"\n",
        "    print(\"Creating streaming inference pipeline...\")\n",
        "\n",
        "    # Define the 7 available columns in the streaming data\n",
        "    available_columns = [\n",
        "        \"Date received\",\n",
        "        \"Complaint ID\",\n",
        "        \"Company\",\n",
        "        \"State\",\n",
        "        \"ZIP code\",\n",
        "        \"Submitted via\",\n",
        "        \"Consumer complaint narrative\"\n",
        "    ]\n",
        "\n",
        "    # Define categorical columns to be encoded\n",
        "    categorical_columns = [\n",
        "        \"Company\",\n",
        "        \"State\",\n",
        "        \"Submitted via\"\n",
        "    ]\n",
        "\n",
        "    # Lists to store pipeline stages\n",
        "    stages = []\n",
        "\n",
        "    # Parse date received (similar to Task 2)\n",
        "    # This ensures we handle dates consistently\n",
        "    DATE_FORMAT = \"MM/dd/yyyy\"\n",
        "    stages.append(\n",
        "        Pipeline(stages=[\n",
        "            lambda df: df.withColumn(\n",
        "                \"parsed_date_received\",\n",
        "                F.to_date(F.col(\"Date received\"), DATE_FORMAT)\n",
        "            )\n",
        "        ])\n",
        "    )\n",
        "\n",
        "    # ---------- Text Feature Engineering ----------\n",
        "    print(\"Adding text feature engineering stages...\")\n",
        "\n",
        "    if use_bert_embeddings:\n",
        "        # BERT Embedding approach (if Task 5 was implemented)\n",
        "        print(\"Using BERT embeddings for text features\")\n",
        "\n",
        "        # Import the BERT embedding UDF from the bert-embeddings module\n",
        "        # Note: This is a placeholder and would need actual implementation\n",
        "        try:\n",
        "            from bert_embeddings import create_bert_embedding_udf\n",
        "            bert_embed_udf = create_bert_embedding_udf()\n",
        "\n",
        "            # Add a stage to apply the BERT embedding UDF\n",
        "            stages.append(\n",
        "                Pipeline(stages=[\n",
        "                    lambda df: df.withColumn(\n",
        "                        \"narrative_features\",\n",
        "                        bert_embed_udf(F.col(\"Consumer complaint narrative\"))\n",
        "                    )\n",
        "                ])\n",
        "            )\n",
        "        except ImportError:\n",
        "            print(\"BERT embedding module not found, falling back to basic text features\")\n",
        "            use_bert_embeddings = False\n",
        "\n",
        "    if not use_bert_embeddings:\n",
        "        # Basic text feature engineering (TF-IDF)\n",
        "        print(\"Using basic TF-IDF for text features\")\n",
        "\n",
        "        # Text cleaning and tokenization\n",
        "        stages.append(\n",
        "            RegexTokenizer(\n",
        "                inputCol=\"Consumer complaint narrative\",\n",
        "                outputCol=\"narrative_tokens\",\n",
        "                pattern=\"\\\\W+\",\n",
        "                toLowercase=True\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Remove stop words\n",
        "        stages.append(\n",
        "            StopWordsRemover(\n",
        "                inputCol=\"narrative_tokens\",\n",
        "                outputCol=\"narrative_filtered\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Generate TF-IDF features\n",
        "        stages.append(\n",
        "            HashingTF(\n",
        "                inputCol=\"narrative_filtered\",\n",
        "                outputCol=\"narrative_tf\",\n",
        "                numFeatures=10000  # Must match training pipeline\n",
        "            )\n",
        "        )\n",
        "\n",
        "        stages.append(\n",
        "            IDF(\n",
        "                inputCol=\"narrative_tf\",\n",
        "                outputCol=\"narrative_features\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # ---------- Categorical Feature Engineering ----------\n",
        "    print(\"Adding categorical feature engineering stages...\")\n",
        "\n",
        "    # Store transformed column names for later use in VectorAssembler\n",
        "    indexed_columns = []\n",
        "    encoded_columns = []\n",
        "\n",
        "    # For each categorical column, create a StringIndexer and OneHotEncoder\n",
        "    for category in categorical_columns:\n",
        "        # Create a StringIndexer with handleInvalid='keep'\n",
        "        indexer_output = f\"{category}_indexed\"\n",
        "        indexer = StringIndexer(\n",
        "            inputCol=category,\n",
        "            outputCol=indexer_output,\n",
        "            handleInvalid=\"keep\"  # Handle unseen labels as specified\n",
        "        )\n",
        "        stages.append(indexer)\n",
        "        indexed_columns.append(indexer_output)\n",
        "\n",
        "        # Create a OneHotEncoder\n",
        "        encoder_output = f\"{category}_encoded\"\n",
        "        encoder = OneHotEncoder(\n",
        "            inputCol=indexer_output,\n",
        "            outputCol=encoder_output,\n",
        "            dropLast=True  # Drop last category to avoid collinearity\n",
        "        )\n",
        "        stages.append(encoder)\n",
        "        encoded_columns.append(encoder_output)\n",
        "\n",
        "    # ---------- Numeric Feature Engineering ----------\n",
        "    print(\"Adding numeric feature engineering stages...\")\n",
        "\n",
        "    # Extract ZIP code numeric part and convert to numeric\n",
        "    stages.append(\n",
        "        RegexTokenizer(\n",
        "            inputCol=\"ZIP code\",\n",
        "            outputCol=\"zip_numeric_str\",\n",
        "            pattern=\"\\\\D+\",  # Non-digit characters\n",
        "            gaps=True  # Use gaps between tokens\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Convert ZIP numeric string tokens to a single string\n",
        "    stages.append(\n",
        "        Pipeline(stages=[\n",
        "            lambda df: df.withColumn(\n",
        "                \"zip_numeric_str\",\n",
        "                F.when(F.size(F.col(\"zip_numeric_str\")) > 0, F.col(\"zip_numeric_str\")[0])\n",
        "                .otherwise(None)\n",
        "            )\n",
        "        ])\n",
        "    )\n",
        "\n",
        "    # Convert ZIP string to numeric\n",
        "    stages.append(\n",
        "        Pipeline(stages=[\n",
        "            lambda df: df.withColumn(\n",
        "                \"zip_numeric\",\n",
        "                F.col(\"zip_numeric_str\").cast(IntegerType())\n",
        "            )\n",
        "        ])\n",
        "    )\n",
        "\n",
        "    # Handle missing values in numeric features\n",
        "    numeric_columns = [\"zip_numeric\"]\n",
        "    stages.append(\n",
        "        Imputer(\n",
        "            inputCols=numeric_columns,\n",
        "            outputCols=[f\"{col}_imputed\" for col in numeric_columns],\n",
        "            strategy=\"median\"  # Use median for ZIP codes\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Scale numeric features\n",
        "    for col in numeric_columns:\n",
        "        stages.append(\n",
        "            MinMaxScaler(\n",
        "                inputCol=f\"{col}_imputed\",\n",
        "                outputCol=f\"{col}_scaled\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Get list of scaled numeric columns\n",
        "    scaled_numeric_columns = [f\"{col}_scaled\" for col in numeric_columns]\n",
        "\n",
        "    # ---------- Final Feature Assembly ----------\n",
        "    print(\"Adding final Vector Assembler stage...\")\n",
        "\n",
        "    # Combine all feature columns using VectorAssembler\n",
        "    # This must match the subset of features used in the model\n",
        "    feature_columns = [\"narrative_features\"] + encoded_columns + scaled_numeric_columns\n",
        "\n",
        "    stages.append(\n",
        "        VectorAssembler(\n",
        "            inputCols=feature_columns,\n",
        "            outputCol=\"features\",\n",
        "            handleInvalid=\"keep\"  # Handle invalid entries\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Create the pipeline\n",
        "    inference_pipeline = Pipeline(stages=stages)\n",
        "    print(f\"Streaming inference pipeline created with {len(stages)} stages\")\n",
        "\n",
        "    return inference_pipeline\n",
        "\n",
        "def load_saved_training_pipeline():\n",
        "    \"\"\"\n",
        "    Load the fitted training pipeline and extract relevant components for streaming\n",
        "\n",
        "    Returns:\n",
        "        inference_pipeline: A Pipeline with relevant transformations for streaming\n",
        "    \"\"\"\n",
        "    print(f\"Attempting to load saved training pipeline from: {TRAINING_PIPELINE_SAVE_PATH}\")\n",
        "\n",
        "    try:\n",
        "        # Try to load the saved pipeline\n",
        "        saved_pipeline = PipelineModel.load(TRAINING_PIPELINE_SAVE_PATH)\n",
        "        print(\"Successfully loaded saved training pipeline\")\n",
        "\n",
        "        # Extract relevant stages (this would need customization based on your pipeline)\n",
        "        # This is just an example - you would need to analyze your saved pipeline\n",
        "\n",
        "        # For simplicity, we'll create a new streaming pipeline\n",
        "        # that mimics the relevant parts of the saved pipeline\n",
        "        # In practice, you might want to extract and adapt specific stages\n",
        "\n",
        "        return create_streaming_inference_pipeline(use_bert_embeddings=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading saved pipeline: {e}\")\n",
        "        print(\"Creating new streaming inference pipeline from scratch\")\n",
        "\n",
        "        # Create a new pipeline for streaming\n",
        "        return create_streaming_inference_pipeline(use_bert_embeddings=False)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Load saved pipeline or create new one\n",
        "    inference_pipeline = load_saved_training_pipeline()\n",
        "\n",
        "    # Optionally, fit on a sample dataset to initialize stages\n",
        "    # This may be necessary for certain transformers like StringIndexer\n",
        "\n",
        "    print(\"Task 8: Streaming Inference Pipeline created\")"
      ],
      "metadata": {
        "id": "bSE2cbQWfgIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# Task 9: Streaming Inference Job\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml import PipelineModel\n",
        "from pyspark.ml.classification import RandomForestClassificationModel, GBTClassificationModel, LogisticRegressionModel\n",
        "from pyspark.ml.functions import vector_to_array\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Import configuration\n",
        "KAFKA_BROKERS = \"kafka1:9092,kafka2:9092\"\n",
        "KAFKA_TOPIC_TESTING_STREAM = \"complaints-testing-stream\"\n",
        "KAFKA_TOPIC_PREDICTIONS = \"complaint-predictions\"\n",
        "BEST_MODEL_SAVE_PATH = \"/path/to/best_model\"\n",
        "STREAMING_CHECKPOINT_LOCATION = \"/path/to/streaming_checkpoints\"\n",
        "DATABASE_SINK_FORMAT = \"jdbc\"\n",
        "DATABASE_CONNECTION_OPTIONS = {\n",
        "    \"url\": \"jdbc:postgresql://dbhost:5432/complaints_db\",\n",
        "    \"dbtable\": \"complaint_predictions\",\n",
        "    \"user\": \"username\",\n",
        "    \"password\": \"password\",\n",
        "    \"driver\": \"org.postgresql.Driver\"\n",
        "}\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Consumer Complaints Streaming Inference\") \\\n",
        "    .config(\"spark.sql.streaming.checkpointLocation\", STREAMING_CHECKPOINT_LOCATION) \\\n",
        "    .getOrCreate()\n",
        "\n",
        "def run_streaming_inference_job(inference_pipeline, use_kafka_sink=False):\n",
        "    \"\"\"\n",
        "    Execute the streaming inference job:\n",
        "    1. Load the trained classifier model\n",
        "    2. Read streaming data from Kafka\n",
        "    3. Apply the inference pipeline and model\n",
        "    4. Write predictions to database or Kafka\n",
        "\n",
        "    Args:\n",
        "        inference_pipeline: The prepared inference pipeline from Task 8\n",
        "        use_kafka_sink: Whether to write results to Kafka (True) or database (False)\n",
        "    \"\"\"\n",
        "    print(\"Starting streaming inference job...\")\n",
        "\n",
        "    # Step 1: Load the saved best classifier model\n",
        "    print(f\"Loading best model from: {BEST_MODEL_SAVE_PATH}\")\n",
        "    try:\n",
        "        # Try loading as RandomForestClassificationModel first\n",
        "        loaded_model = RandomForestClassificationModel.load(BEST_MODEL_SAVE_PATH)\n",
        "        print(\"Loaded RandomForestClassificationModel\")\n",
        "    except:\n",
        "        try:\n",
        "            # Try loading as GBTClassificationModel\n",
        "            loaded_model = GBTClassificationModel.load(BEST_MODEL_SAVE_PATH)\n",
        "            print(\"Loaded GBTClassificationModel\")\n",
        "        except:\n",
        "            try:\n",
        "                # Try loading as LogisticRegressionModel\n",
        "                loaded_model = LogisticRegressionModel.load(BEST_MODEL_SAVE_PATH)\n",
        "                print(\"Loaded LogisticRegressionModel\")\n",
        "            except Exception as e:\n",
        "                raise Exception(f\"Failed to load model: {e}\")\n",
        "\n",
        "    # Step 2: Define the schema for streaming data\n",
        "    stream_schema = StructType([\n",
        "        StructField(\"Date received\", StringType(), True),\n",
        "        StructField(\"Complaint ID\", StringType(), True),\n",
        "        StructField(\"Company\", StringType(), True),\n",
        "        StructField(\"State\", StringType(), True),\n",
        "        StructField(\"ZIP code\", StringType(), True),\n",
        "        StructField(\"Submitted via\", StringType(), True),\n",
        "        StructField(\"Consumer complaint narrative\", StringType(), True)\n",
        "    ])\n",
        "\n",
        "    # Step 3: Create a streaming DataFrame from Kafka\n",
        "    print(f\"Setting up Kafka streaming source from topic: {KAFKA_TOPIC_TESTING_STREAM}\")\n",
        "    streaming_df = spark.readStream \\\n",
        "        .format(\"kafka\") \\\n",
        "        .option(\"kafka.bootstrap.servers\", KAFKA_BROKERS) \\\n",
        "        .option(\"subscribe\", KAFKA_TOPIC_TESTING_STREAM) \\\n",
        "        .option(\"startingOffsets\", \"latest\") \\\n",
        "        .load()\n",
        "\n",
        "    # Parse the Kafka JSON value\n",
        "    parsed_df = streaming_df.select(\n",
        "        F.col(\"key\").cast(\"string\").alias(\"message_key\"),\n",
        "        F.from_json(F.col(\"value\").cast(\"string\"), stream_schema).alias(\"data\")\n",
        "    ).select(\n",
        "        \"message_key\",\n",
        "        \"data.*\"\n",
        "    )\n",
        "\n",
        "    # Step 4: Apply the inference pipeline\n",
        "    print(\"Applying inference pipeline to streaming data\")\n",
        "    # Fit the pipeline if it's not already fitted\n",
        "    # In a production environment, you might want to fit this on a sample dataset first\n",
        "    try:\n",
        "        # Try to transform directly (if already fitted)\n",
        "        processed_df = inference_pipeline.transform(parsed_df)\n",
        "    except:\n",
        "        # If not fitted, we need to fit it first\n",
        "        # This should ideally be done before starting the stream with a sample dataset\n",
        "        print(\"Pipeline not fitted, fitting on first batch...\")\n",
        "        inference_pipeline_model = inference_pipeline.fit(parsed_df)\n",
        "        processed_df = inference_pipeline_model.transform(parsed_df)\n",
        "\n",
        "    # Step 5: Apply the model to get predictions\n",
        "    print(\"Applying model to get predictions\")\n",
        "    predictions_df = loaded_model.transform(processed_df)\n",
        "\n",
        "    # Step 6: Select and format output columns\n",
        "    output_df = predictions_df.select(\n",
        "        F.col(\"Complaint ID\").alias(\"complaint_id\"),\n",
        "        F.col(\"prediction\").cast(\"double\").alias(\"prediction\"),\n",
        "        # Extract probability of positive class (1)\n",
        "        F.when(\n",
        "            F.size(F.col(\"probability\")) > 1,\n",
        "            F.element_at(vector_to_array(F.col(\"probability\")), 2)\n",
        "        ).otherwise(\n",
        "            F.when(F.col(\"prediction\") > 0.5, 1.0).otherwise(0.0)\n",
        "        ).alias(\"probability_1\"),\n",
        "        F.col(\"State\").alias(\"state\"),\n",
        "        F.col(\"ZIP code\").alias(\"zip_code\"),\n",
        "        F.col(\"Submitted via\").alias(\"submitted_via\"),\n",
        "        F.col(\"parsed_date_received\").alias(\"complaint_date\"),\n",
        "        F.current_timestamp().alias(\"inference_time\")\n",
        "    )\n",
        "\n",
        "    # Add a timestamp string for easier querying\n",
        "    final_df = output_df.withColumn(\n",
        "        \"complaint_date_str\","
      ],
      "metadata": {
        "id": "nsKk8m4bfmdD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}